Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 10,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (Wa_i): Linear(in_features=400, out_features=400, bias=False)
      (Wa_o): Linear(in_features=400, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 365,002
Epoch 0
Loss = 1.9788e-02, PNorm = 35.4618, GNorm = 1.2761, lr_0 = 1.4737e-04
Loss = 2.2986e-02, PNorm = 35.4639, GNorm = 1.9844, lr_0 = 1.9474e-04
Loss = 1.3766e-02, PNorm = 35.4683, GNorm = 2.8045, lr_0 = 2.4211e-04
Loss = 1.8789e-02, PNorm = 35.4749, GNorm = 2.5507, lr_0 = 2.8947e-04
Loss = 1.3797e-02, PNorm = 35.4872, GNorm = 2.6705, lr_0 = 3.3684e-04
Loss = 1.3947e-02, PNorm = 35.5050, GNorm = 1.1424, lr_0 = 3.8421e-04
Loss = 1.0852e-02, PNorm = 35.5244, GNorm = 1.9210, lr_0 = 4.3158e-04
Loss = 1.2445e-02, PNorm = 35.5441, GNorm = 1.6460, lr_0 = 4.7895e-04
Loss = 1.3341e-02, PNorm = 35.5680, GNorm = 2.8039, lr_0 = 5.2632e-04
Validation rmse = 1.641213
Epoch 1
Loss = 1.3265e-02, PNorm = 35.5938, GNorm = 1.2878, lr_0 = 5.7368e-04
Loss = 1.0085e-02, PNorm = 35.6227, GNorm = 2.0571, lr_0 = 6.2105e-04
Loss = 1.2736e-02, PNorm = 35.6495, GNorm = 3.1745, lr_0 = 6.6842e-04
Loss = 1.2049e-02, PNorm = 35.6848, GNorm = 1.4932, lr_0 = 7.1579e-04
Loss = 1.1415e-02, PNorm = 35.7208, GNorm = 1.4055, lr_0 = 7.6316e-04
Loss = 8.0602e-03, PNorm = 35.7568, GNorm = 0.7922, lr_0 = 8.1053e-04
Loss = 9.0040e-03, PNorm = 35.7914, GNorm = 0.8008, lr_0 = 8.5789e-04
Loss = 8.8504e-03, PNorm = 35.8322, GNorm = 2.6567, lr_0 = 9.0526e-04
Loss = 1.1782e-02, PNorm = 35.8769, GNorm = 1.3566, lr_0 = 9.5263e-04
Loss = 1.1166e-02, PNorm = 35.9523, GNorm = 5.0596, lr_0 = 1.0000e-03
Validation rmse = 1.570046
Epoch 2
Loss = 1.1421e-02, PNorm = 36.0159, GNorm = 4.0194, lr_0 = 9.9138e-04
Loss = 8.7775e-03, PNorm = 36.0692, GNorm = 1.8600, lr_0 = 9.8284e-04
Loss = 1.0672e-02, PNorm = 36.1306, GNorm = 2.4093, lr_0 = 9.7437e-04
Loss = 8.9754e-03, PNorm = 36.1814, GNorm = 1.8208, lr_0 = 9.6597e-04
Loss = 1.0843e-02, PNorm = 36.2342, GNorm = 2.7172, lr_0 = 9.5764e-04
Loss = 8.4922e-03, PNorm = 36.2981, GNorm = 1.0735, lr_0 = 9.4939e-04
Loss = 8.7399e-03, PNorm = 36.3460, GNorm = 2.8504, lr_0 = 9.4120e-04
Loss = 8.4723e-03, PNorm = 36.3913, GNorm = 1.4771, lr_0 = 9.3309e-04
Loss = 8.2285e-03, PNorm = 36.4265, GNorm = 0.8487, lr_0 = 9.2505e-04
Validation rmse = 1.321316
Epoch 3
Loss = 8.0159e-03, PNorm = 36.4704, GNorm = 1.1053, lr_0 = 9.1708e-04
Loss = 9.8773e-03, PNorm = 36.5190, GNorm = 1.3962, lr_0 = 9.0917e-04
Loss = 1.1220e-02, PNorm = 36.5697, GNorm = 3.1280, lr_0 = 9.0134e-04
Loss = 6.5716e-03, PNorm = 36.6186, GNorm = 1.0947, lr_0 = 8.9357e-04
Loss = 6.4374e-03, PNorm = 36.6566, GNorm = 0.8054, lr_0 = 8.8587e-04
Loss = 7.2320e-03, PNorm = 36.6930, GNorm = 1.2290, lr_0 = 8.7823e-04
Loss = 8.5382e-03, PNorm = 36.7453, GNorm = 0.9182, lr_0 = 8.7066e-04
Loss = 9.0648e-03, PNorm = 36.8113, GNorm = 1.9817, lr_0 = 8.6316e-04
Loss = 7.4666e-03, PNorm = 36.8766, GNorm = 1.2721, lr_0 = 8.5572e-04
Loss = 8.0999e-03, PNorm = 36.9121, GNorm = 1.9054, lr_0 = 8.4834e-04
Validation rmse = 1.249735
Epoch 4
Loss = 7.2616e-03, PNorm = 36.9452, GNorm = 1.2772, lr_0 = 8.4103e-04
Loss = 5.9847e-03, PNorm = 36.9772, GNorm = 0.7046, lr_0 = 8.3378e-04
Loss = 7.1716e-03, PNorm = 37.0107, GNorm = 0.7479, lr_0 = 8.2660e-04
Loss = 7.3635e-03, PNorm = 37.0445, GNorm = 1.1446, lr_0 = 8.1947e-04
Loss = 7.4949e-03, PNorm = 37.0637, GNorm = 2.2879, lr_0 = 8.1241e-04
Loss = 7.0324e-03, PNorm = 37.0829, GNorm = 2.4201, lr_0 = 8.0541e-04
Loss = 8.6946e-03, PNorm = 37.1180, GNorm = 1.3130, lr_0 = 7.9846e-04
Loss = 7.2430e-03, PNorm = 37.1685, GNorm = 1.0438, lr_0 = 7.9158e-04
Loss = 7.5379e-03, PNorm = 37.2072, GNorm = 0.7041, lr_0 = 7.8476e-04
Validation rmse = 1.254791
Epoch 5
Loss = 6.8458e-03, PNorm = 37.2370, GNorm = 1.0933, lr_0 = 7.7800e-04
Loss = 5.6870e-03, PNorm = 37.2662, GNorm = 1.9638, lr_0 = 7.7129e-04
Loss = 8.3927e-03, PNorm = 37.2984, GNorm = 2.2793, lr_0 = 7.6464e-04
Loss = 6.8441e-03, PNorm = 37.3274, GNorm = 2.6345, lr_0 = 7.5805e-04
Loss = 8.0383e-03, PNorm = 37.3568, GNorm = 1.9345, lr_0 = 7.5152e-04
Loss = 7.1500e-03, PNorm = 37.3834, GNorm = 0.9898, lr_0 = 7.4504e-04
Loss = 6.9274e-03, PNorm = 37.4073, GNorm = 1.9048, lr_0 = 7.3862e-04
Loss = 7.0109e-03, PNorm = 37.4407, GNorm = 0.5414, lr_0 = 7.3225e-04
Loss = 7.8341e-03, PNorm = 37.4789, GNorm = 0.8955, lr_0 = 7.2594e-04
Loss = 6.2373e-03, PNorm = 37.5110, GNorm = 0.7375, lr_0 = 7.1969e-04
Validation rmse = 1.117352
Epoch 6
Loss = 6.3514e-03, PNorm = 37.5385, GNorm = 0.7685, lr_0 = 7.1348e-04
Loss = 6.3954e-03, PNorm = 37.5810, GNorm = 1.5897, lr_0 = 7.0733e-04
Loss = 6.5172e-03, PNorm = 37.6238, GNorm = 1.5461, lr_0 = 7.0124e-04
Loss = 6.6365e-03, PNorm = 37.6434, GNorm = 1.4887, lr_0 = 6.9519e-04
Loss = 7.3325e-03, PNorm = 37.6804, GNorm = 2.7560, lr_0 = 6.8920e-04
Loss = 5.5958e-03, PNorm = 37.7035, GNorm = 1.2456, lr_0 = 6.8326e-04
Loss = 6.2981e-03, PNorm = 37.7137, GNorm = 0.8993, lr_0 = 6.7737e-04
Loss = 6.6219e-03, PNorm = 37.7416, GNorm = 2.0003, lr_0 = 6.7153e-04
Loss = 7.0768e-03, PNorm = 37.7701, GNorm = 0.8281, lr_0 = 6.6575e-04
Validation rmse = 1.229604
Epoch 7
Loss = 5.6641e-03, PNorm = 37.7952, GNorm = 1.8271, lr_0 = 6.6001e-04
Loss = 5.9095e-03, PNorm = 37.8361, GNorm = 1.7442, lr_0 = 6.5432e-04
Loss = 6.5100e-03, PNorm = 37.8684, GNorm = 2.7517, lr_0 = 6.4868e-04
Loss = 6.3186e-03, PNorm = 37.9121, GNorm = 1.3732, lr_0 = 6.4309e-04
Loss = 5.3096e-03, PNorm = 37.9441, GNorm = 1.2470, lr_0 = 6.3755e-04
Loss = 6.5791e-03, PNorm = 37.9604, GNorm = 0.9765, lr_0 = 6.3205e-04
Loss = 8.1373e-03, PNorm = 37.9780, GNorm = 4.5905, lr_0 = 6.2660e-04
Loss = 6.3750e-03, PNorm = 38.0095, GNorm = 1.3622, lr_0 = 6.2120e-04
Loss = 6.0208e-03, PNorm = 38.0399, GNorm = 1.9205, lr_0 = 6.1585e-04
Loss = 5.4966e-03, PNorm = 38.0671, GNorm = 1.0787, lr_0 = 6.1054e-04
Validation rmse = 1.066563
Epoch 8
Loss = 5.7521e-03, PNorm = 38.0867, GNorm = 0.8149, lr_0 = 6.0528e-04
Loss = 5.6101e-03, PNorm = 38.1104, GNorm = 1.3702, lr_0 = 6.0006e-04
Loss = 5.0488e-03, PNorm = 38.1435, GNorm = 1.2541, lr_0 = 5.9489e-04
Loss = 6.5776e-03, PNorm = 38.1589, GNorm = 1.5815, lr_0 = 5.8976e-04
Loss = 6.0120e-03, PNorm = 38.1991, GNorm = 1.0938, lr_0 = 5.8468e-04
Loss = 6.3604e-03, PNorm = 38.2180, GNorm = 1.6049, lr_0 = 5.7964e-04
Loss = 6.2536e-03, PNorm = 38.2373, GNorm = 1.5688, lr_0 = 5.7464e-04
Loss = 6.0937e-03, PNorm = 38.2610, GNorm = 2.1690, lr_0 = 5.6969e-04
Loss = 5.0606e-03, PNorm = 38.2895, GNorm = 0.7635, lr_0 = 5.6478e-04
Validation rmse = 1.067923
Epoch 9
Loss = 4.9418e-03, PNorm = 38.3134, GNorm = 1.1889, lr_0 = 5.5991e-04
Loss = 6.3591e-03, PNorm = 38.3322, GNorm = 0.9236, lr_0 = 5.5509e-04
Loss = 5.8587e-03, PNorm = 38.3543, GNorm = 1.8345, lr_0 = 5.5030e-04
Loss = 6.2347e-03, PNorm = 38.3788, GNorm = 1.2675, lr_0 = 5.4556e-04
Loss = 5.7264e-03, PNorm = 38.3961, GNorm = 1.8858, lr_0 = 5.4086e-04
Loss = 5.2549e-03, PNorm = 38.4182, GNorm = 0.9883, lr_0 = 5.3620e-04
Loss = 6.8665e-03, PNorm = 38.4299, GNorm = 1.7692, lr_0 = 5.3157e-04
Loss = 5.1425e-03, PNorm = 38.4514, GNorm = 0.6482, lr_0 = 5.2699e-04
Loss = 6.3308e-03, PNorm = 38.4643, GNorm = 1.1913, lr_0 = 5.2245e-04
Loss = 6.2233e-03, PNorm = 38.5006, GNorm = 1.2380, lr_0 = 5.1795e-04
Validation rmse = 1.076966
Epoch 10
Loss = 5.7846e-03, PNorm = 38.5246, GNorm = 1.2161, lr_0 = 5.1348e-04
Loss = 6.6497e-03, PNorm = 38.5429, GNorm = 1.0947, lr_0 = 5.0906e-04
Loss = 5.4237e-03, PNorm = 38.5559, GNorm = 2.0612, lr_0 = 5.0467e-04
Loss = 5.7256e-03, PNorm = 38.5704, GNorm = 1.0442, lr_0 = 5.0032e-04
Loss = 5.3314e-03, PNorm = 38.5894, GNorm = 1.1667, lr_0 = 4.9601e-04
Loss = 5.8914e-03, PNorm = 38.6065, GNorm = 1.4313, lr_0 = 4.9173e-04
Loss = 5.7096e-03, PNorm = 38.6226, GNorm = 1.7520, lr_0 = 4.8749e-04
Loss = 6.4715e-03, PNorm = 38.6307, GNorm = 1.7415, lr_0 = 4.8329e-04
Loss = 5.4622e-03, PNorm = 38.6427, GNorm = 1.5975, lr_0 = 4.7913e-04
Validation rmse = 1.060006
Epoch 11
Loss = 5.4289e-03, PNorm = 38.6687, GNorm = 1.5405, lr_0 = 4.7500e-04
Loss = 5.3533e-03, PNorm = 38.6822, GNorm = 0.6390, lr_0 = 4.7090e-04
Loss = 5.2146e-03, PNorm = 38.7003, GNorm = 1.9372, lr_0 = 4.6685e-04
Loss = 5.5117e-03, PNorm = 38.7096, GNorm = 1.5385, lr_0 = 4.6282e-04
Loss = 6.3039e-03, PNorm = 38.7290, GNorm = 0.8140, lr_0 = 4.5883e-04
Loss = 7.5814e-03, PNorm = 38.7427, GNorm = 0.6398, lr_0 = 4.5488e-04
Loss = 6.3567e-03, PNorm = 38.7717, GNorm = 1.6640, lr_0 = 4.5096e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 10,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (Wa_i): Linear(in_features=400, out_features=400, bias=False)
      (Wa_o): Linear(in_features=400, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 365,002
Epoch 0
Loss = 1.9656e-02, PNorm = 35.5505, GNorm = 1.6854, lr_0 = 1.4737e-04
Loss = 2.4100e-02, PNorm = 35.5524, GNorm = 1.3762, lr_0 = 1.9474e-04
Loss = 1.3767e-02, PNorm = 35.5581, GNorm = 1.7840, lr_0 = 2.4211e-04
Loss = 1.7710e-02, PNorm = 35.5716, GNorm = 2.2364, lr_0 = 2.8947e-04
Loss = 1.3770e-02, PNorm = 35.5905, GNorm = 1.0859, lr_0 = 3.3684e-04
Loss = 1.4275e-02, PNorm = 35.6076, GNorm = 2.0129, lr_0 = 3.8421e-04
Loss = 1.0777e-02, PNorm = 35.6241, GNorm = 1.0574, lr_0 = 4.3158e-04
Loss = 1.2094e-02, PNorm = 35.6422, GNorm = 1.8564, lr_0 = 4.7895e-04
Loss = 1.3436e-02, PNorm = 35.6601, GNorm = 2.9686, lr_0 = 5.2632e-04
Validation rmse = 1.627000
Epoch 1
Loss = 1.3238e-02, PNorm = 35.6809, GNorm = 4.9890, lr_0 = 5.7368e-04
Loss = 1.0493e-02, PNorm = 35.7003, GNorm = 3.3851, lr_0 = 6.2105e-04
Loss = 1.3235e-02, PNorm = 35.7185, GNorm = 1.4450, lr_0 = 6.6842e-04
Loss = 1.0573e-02, PNorm = 35.7366, GNorm = 1.2916, lr_0 = 7.1579e-04
Loss = 9.8941e-03, PNorm = 35.7545, GNorm = 1.7756, lr_0 = 7.6316e-04
Loss = 8.2136e-03, PNorm = 35.7738, GNorm = 0.8628, lr_0 = 8.1053e-04
Loss = 8.2455e-03, PNorm = 35.7929, GNorm = 1.5816, lr_0 = 8.5789e-04
Loss = 8.6849e-03, PNorm = 35.8191, GNorm = 1.5406, lr_0 = 9.0526e-04
Loss = 8.4047e-03, PNorm = 35.8527, GNorm = 1.7764, lr_0 = 9.5263e-04
Loss = 1.0558e-02, PNorm = 35.8833, GNorm = 2.9677, lr_0 = 1.0000e-03
Validation rmse = 1.363454
Epoch 2
Loss = 1.0689e-02, PNorm = 35.9119, GNorm = 6.9830, lr_0 = 9.9138e-04
Loss = 9.0587e-03, PNorm = 35.9452, GNorm = 2.6298, lr_0 = 9.8284e-04
Loss = 9.9439e-03, PNorm = 35.9769, GNorm = 3.3117, lr_0 = 9.7437e-04
Loss = 9.1767e-03, PNorm = 36.0095, GNorm = 1.1365, lr_0 = 9.6597e-04
Loss = 8.5503e-03, PNorm = 36.0412, GNorm = 3.7464, lr_0 = 9.5764e-04
Loss = 6.5421e-03, PNorm = 36.0650, GNorm = 0.8126, lr_0 = 9.4939e-04
Loss = 7.4398e-03, PNorm = 36.0852, GNorm = 1.8459, lr_0 = 9.4120e-04
Loss = 7.1762e-03, PNorm = 36.1075, GNorm = 1.9643, lr_0 = 9.3309e-04
Loss = 7.6953e-03, PNorm = 36.1312, GNorm = 2.6327, lr_0 = 9.2505e-04
Validation rmse = 1.233733
Epoch 3
Loss = 7.0208e-03, PNorm = 36.1577, GNorm = 1.1284, lr_0 = 9.1708e-04
Loss = 8.3286e-03, PNorm = 36.1832, GNorm = 1.0037, lr_0 = 9.0917e-04
Loss = 6.8497e-03, PNorm = 36.2035, GNorm = 1.0811, lr_0 = 9.0134e-04
Loss = 6.2769e-03, PNorm = 36.2299, GNorm = 0.6561, lr_0 = 8.9357e-04
Loss = 5.8938e-03, PNorm = 36.2504, GNorm = 0.6207, lr_0 = 8.8587e-04
Loss = 6.6395e-03, PNorm = 36.2718, GNorm = 1.3579, lr_0 = 8.7823e-04
Loss = 8.0835e-03, PNorm = 36.2946, GNorm = 1.2748, lr_0 = 8.7066e-04
Loss = 7.1450e-03, PNorm = 36.3215, GNorm = 1.0376, lr_0 = 8.6316e-04
Loss = 5.8793e-03, PNorm = 36.3419, GNorm = 0.7736, lr_0 = 8.5572e-04
Loss = 7.0115e-03, PNorm = 36.3605, GNorm = 2.2246, lr_0 = 8.4834e-04
Validation rmse = 1.298868
Epoch 4
Loss = 6.4307e-03, PNorm = 36.3798, GNorm = 0.9709, lr_0 = 8.4103e-04
Loss = 5.7224e-03, PNorm = 36.4034, GNorm = 1.3670, lr_0 = 8.3378e-04
Loss = 6.8146e-03, PNorm = 36.4269, GNorm = 1.4016, lr_0 = 8.2660e-04
Loss = 6.9997e-03, PNorm = 36.4493, GNorm = 0.8127, lr_0 = 8.1947e-04
Loss = 5.8760e-03, PNorm = 36.4639, GNorm = 1.4058, lr_0 = 8.1241e-04
Loss = 6.1385e-03, PNorm = 36.4792, GNorm = 1.0174, lr_0 = 8.0541e-04
Loss = 8.1085e-03, PNorm = 36.5024, GNorm = 1.5566, lr_0 = 7.9846e-04
Loss = 5.8608e-03, PNorm = 36.5306, GNorm = 1.2102, lr_0 = 7.9158e-04
Loss = 5.1243e-03, PNorm = 36.5486, GNorm = 0.8936, lr_0 = 7.8476e-04
Validation rmse = 1.187164
Epoch 5
Loss = 5.4978e-03, PNorm = 36.5682, GNorm = 1.2761, lr_0 = 7.7800e-04
Loss = 4.6121e-03, PNorm = 36.5801, GNorm = 1.7081, lr_0 = 7.7129e-04
Loss = 7.4467e-03, PNorm = 36.5996, GNorm = 2.0336, lr_0 = 7.6464e-04
Loss = 6.6921e-03, PNorm = 36.6237, GNorm = 1.8091, lr_0 = 7.5805e-04
Loss = 7.6823e-03, PNorm = 36.6439, GNorm = 1.4513, lr_0 = 7.5152e-04
Loss = 6.5807e-03, PNorm = 36.6574, GNorm = 2.8623, lr_0 = 7.4504e-04
Loss = 5.4901e-03, PNorm = 36.6672, GNorm = 1.9775, lr_0 = 7.3862e-04
Loss = 6.2193e-03, PNorm = 36.6868, GNorm = 1.5277, lr_0 = 7.3225e-04
Loss = 7.3384e-03, PNorm = 36.7039, GNorm = 0.9811, lr_0 = 7.2594e-04
Loss = 5.3787e-03, PNorm = 36.7272, GNorm = 0.8081, lr_0 = 7.1969e-04
Validation rmse = 1.136977
Epoch 6
Loss = 5.4967e-03, PNorm = 36.7447, GNorm = 0.9590, lr_0 = 7.1348e-04
Loss = 6.0777e-03, PNorm = 36.7686, GNorm = 0.9445, lr_0 = 7.0733e-04
Loss = 5.7541e-03, PNorm = 36.7870, GNorm = 1.0522, lr_0 = 7.0124e-04
Loss = 6.1143e-03, PNorm = 36.8019, GNorm = 1.4613, lr_0 = 6.9519e-04
Loss = 7.3159e-03, PNorm = 36.8255, GNorm = 1.4701, lr_0 = 6.8920e-04
Loss = 5.5644e-03, PNorm = 36.8435, GNorm = 1.2433, lr_0 = 6.8326e-04
Loss = 6.1563e-03, PNorm = 36.8560, GNorm = 1.4621, lr_0 = 6.7737e-04
Loss = 5.9951e-03, PNorm = 36.8720, GNorm = 0.7949, lr_0 = 6.7153e-04
Loss = 6.4746e-03, PNorm = 36.8855, GNorm = 1.3141, lr_0 = 6.6575e-04
Validation rmse = 1.171872
Epoch 7
Loss = 4.8534e-03, PNorm = 36.8949, GNorm = 1.3757, lr_0 = 6.6001e-04
Loss = 5.6859e-03, PNorm = 36.9103, GNorm = 0.8199, lr_0 = 6.5432e-04
Loss = 5.8485e-03, PNorm = 36.9297, GNorm = 1.3727, lr_0 = 6.4868e-04
Loss = 6.8764e-03, PNorm = 36.9496, GNorm = 0.9980, lr_0 = 6.4309e-04
Loss = 4.8832e-03, PNorm = 36.9623, GNorm = 0.8094, lr_0 = 6.3755e-04
Loss = 5.2481e-03, PNorm = 36.9683, GNorm = 1.4626, lr_0 = 6.3205e-04
Loss = 7.6708e-03, PNorm = 36.9795, GNorm = 2.7586, lr_0 = 6.2660e-04
Loss = 6.1098e-03, PNorm = 36.9954, GNorm = 1.1036, lr_0 = 6.2120e-04
Loss = 5.6995e-03, PNorm = 37.0071, GNorm = 1.1512, lr_0 = 6.1585e-04
Loss = 4.3603e-03, PNorm = 37.0158, GNorm = 0.8180, lr_0 = 6.1054e-04
Validation rmse = 1.096577
Epoch 8
Loss = 4.6134e-03, PNorm = 37.0308, GNorm = 1.2017, lr_0 = 6.0528e-04
Loss = 5.3902e-03, PNorm = 37.0431, GNorm = 0.8068, lr_0 = 6.0006e-04
Loss = 5.0423e-03, PNorm = 37.0563, GNorm = 1.9177, lr_0 = 5.9489e-04
Loss = 5.1880e-03, PNorm = 37.0708, GNorm = 0.9117, lr_0 = 5.8976e-04
Loss = 5.4613e-03, PNorm = 37.0847, GNorm = 2.0720, lr_0 = 5.8468e-04
Loss = 5.9279e-03, PNorm = 37.0991, GNorm = 3.3981, lr_0 = 5.7964e-04
Loss = 5.4530e-03, PNorm = 37.1127, GNorm = 0.9247, lr_0 = 5.7464e-04
Loss = 6.1620e-03, PNorm = 37.1203, GNorm = 1.8587, lr_0 = 5.6969e-04
Loss = 5.5803e-03, PNorm = 37.1353, GNorm = 1.1215, lr_0 = 5.6478e-04
Validation rmse = 1.078602
Epoch 9
Loss = 5.2839e-03, PNorm = 37.1497, GNorm = 0.7400, lr_0 = 5.5991e-04
Loss = 5.2979e-03, PNorm = 37.1650, GNorm = 1.3583, lr_0 = 5.5509e-04
Loss = 5.1692e-03, PNorm = 37.1809, GNorm = 0.9828, lr_0 = 5.5030e-04
Loss = 5.6499e-03, PNorm = 37.1890, GNorm = 0.8351, lr_0 = 5.4556e-04
Loss = 5.5901e-03, PNorm = 37.1984, GNorm = 1.4675, lr_0 = 5.4086e-04
Loss = 5.0246e-03, PNorm = 37.2099, GNorm = 1.2912, lr_0 = 5.3620e-04
Loss = 5.6256e-03, PNorm = 37.2214, GNorm = 1.2411, lr_0 = 5.3157e-04
Loss = 4.3940e-03, PNorm = 37.2320, GNorm = 0.7230, lr_0 = 5.2699e-04
Loss = 5.1303e-03, PNorm = 37.2416, GNorm = 0.8861, lr_0 = 5.2245e-04
Loss = 6.0182e-03, PNorm = 37.2576, GNorm = 0.8998, lr_0 = 5.1795e-04
Validation rmse = 1.077415
Epoch 10
Loss = 5.7480e-03, PNorm = 37.2680, GNorm = 0.8964, lr_0 = 5.1348e-04
Loss = 5.3432e-03, PNorm = 37.2759, GNorm = 1.2145, lr_0 = 5.0906e-04
Loss = 4.9981e-03, PNorm = 37.2848, GNorm = 0.9041, lr_0 = 5.0467e-04
Loss = 4.7973e-03, PNorm = 37.2934, GNorm = 1.3872, lr_0 = 5.0032e-04
Loss = 4.5492e-03, PNorm = 37.3067, GNorm = 1.1436, lr_0 = 4.9601e-04
Loss = 6.1326e-03, PNorm = 37.3203, GNorm = 1.2350, lr_0 = 4.9173e-04
Loss = 4.9475e-03, PNorm = 37.3304, GNorm = 1.4785, lr_0 = 4.8749e-04
Loss = 5.1027e-03, PNorm = 37.3424, GNorm = 1.3981, lr_0 = 4.8329e-04
Loss = 4.5285e-03, PNorm = 37.3506, GNorm = 1.5193, lr_0 = 4.7913e-04
Validation rmse = 1.097118
Epoch 11
Loss = 4.9407e-03, PNorm = 37.3556, GNorm = 1.3213, lr_0 = 4.7500e-04
Loss = 5.5950e-03, PNorm = 37.3641, GNorm = 0.8067, lr_0 = 4.7090e-04
Loss = 5.2499e-03, PNorm = 37.3717, GNorm = 1.4140, lr_0 = 4.6685e-04
Loss = 4.9849e-03, PNorm = 37.3817, GNorm = 1.2170, lr_0 = 4.6282e-04
Loss = 4.5903e-03, PNorm = 37.3881, GNorm = 0.6227, lr_0 = 4.5883e-04
Loss = 6.1811e-03, PNorm = 37.3952, GNorm = 1.7228, lr_0 = 4.5488e-04
Loss = 5.5313e-03, PNorm = 37.4025, GNorm = 1.1238, lr_0 = 4.5096e-04
Loss = 4.5892e-03, PNorm = 37.4131, GNorm = 0.7134, lr_0 = 4.4707e-04
Loss = 5.3668e-03, PNorm = 37.4268, GNorm = 1.1611, lr_0 = 4.4322e-04
Loss = 5.6705e-03, PNorm = 37.4348, GNorm = 1.4812, lr_0 = 4.3940e-04
Validation rmse = 1.026552
Epoch 12
Loss = 5.0384e-03, PNorm = 37.4443, GNorm = 0.9193, lr_0 = 4.3561e-04
Loss = 5.2741e-03, PNorm = 37.4541, GNorm = 0.9013, lr_0 = 4.3186e-04
Loss = 5.1383e-03, PNorm = 37.4582, GNorm = 0.8183, lr_0 = 4.2813e-04
Loss = 5.0787e-03, PNorm = 37.4676, GNorm = 1.0935, lr_0 = 4.2444e-04
Loss = 5.8771e-03, PNorm = 37.4718, GNorm = 0.9848, lr_0 = 4.2078e-04
Loss = 5.5189e-03, PNorm = 37.4836, GNorm = 3.0269, lr_0 = 4.1716e-04
Loss = 5.6506e-03, PNorm = 37.4942, GNorm = 1.2195, lr_0 = 4.1356e-04
Loss = 5.0535e-03, PNorm = 37.5032, GNorm = 0.8347, lr_0 = 4.1000e-04
Loss = 4.4240e-03, PNorm = 37.5139, GNorm = 0.8605, lr_0 = 4.0646e-04
Validation rmse = 1.047269
Epoch 13
Loss = 5.6573e-03, PNorm = 37.5262, GNorm = 0.6585, lr_0 = 4.0296e-04
Loss = 3.7429e-03, PNorm = 37.5331, GNorm = 0.7559, lr_0 = 3.9949e-04
Loss = 4.8802e-03, PNorm = 37.5369, GNorm = 0.6575, lr_0 = 3.9604e-04
Loss = 5.2306e-03, PNorm = 37.5423, GNorm = 0.8433, lr_0 = 3.9263e-04
Loss = 6.0710e-03, PNorm = 37.5564, GNorm = 1.3431, lr_0 = 3.8925e-04
Loss = 5.1219e-03, PNorm = 37.5668, GNorm = 1.3741, lr_0 = 3.8589e-04
Loss = 4.3641e-03, PNorm = 37.5745, GNorm = 0.9665, lr_0 = 3.8257e-04
Loss = 6.1754e-03, PNorm = 37.5839, GNorm = 0.6672, lr_0 = 3.7927e-04
Loss = 5.0382e-03, PNorm = 37.5924, GNorm = 1.0881, lr_0 = 3.7600e-04
Loss = 4.7992e-03, PNorm = 37.6000, GNorm = 1.4853, lr_0 = 3.7276e-04
Validation rmse = 1.159609
Epoch 14
Loss = 4.9781e-03, PNorm = 37.6087, GNorm = 1.3652, lr_0 = 3.6955e-04
Loss = 6.3468e-03, PNorm = 37.6170, GNorm = 1.1087, lr_0 = 3.6636e-04
Loss = 4.4862e-03, PNorm = 37.6242, GNorm = 1.0901, lr_0 = 3.6320e-04
Loss = 4.5748e-03, PNorm = 37.6296, GNorm = 1.0677, lr_0 = 3.6007e-04
Loss = 4.4654e-03, PNorm = 37.6376, GNorm = 1.0937, lr_0 = 3.5697e-04
Loss = 4.3500e-03, PNorm = 37.6496, GNorm = 0.7787, lr_0 = 3.5389e-04
Loss = 5.5369e-03, PNorm = 37.6596, GNorm = 1.2779, lr_0 = 3.5084e-04
Loss = 5.2450e-03, PNorm = 37.6678, GNorm = 0.9651, lr_0 = 3.4782e-04
Loss = 5.6061e-03, PNorm = 37.6731, GNorm = 0.7702, lr_0 = 3.4482e-04
Validation rmse = 1.088656
Epoch 15
Loss = 5.7543e-03, PNorm = 37.6785, GNorm = 0.9019, lr_0 = 3.4185e-04
Loss = 4.6576e-03, PNorm = 37.6848, GNorm = 1.1428, lr_0 = 3.3890e-04
Loss = 3.7515e-03, PNorm = 37.6904, GNorm = 1.6611, lr_0 = 3.3598e-04
Loss = 4.9840e-03, PNorm = 37.6994, GNorm = 1.0908, lr_0 = 3.3309e-04
Loss = 4.9949e-03, PNorm = 37.7098, GNorm = 0.8889, lr_0 = 3.3022e-04
Loss = 5.0516e-03, PNorm = 37.7187, GNorm = 1.7581, lr_0 = 3.2737e-04
Loss = 3.9225e-03, PNorm = 37.7251, GNorm = 1.0789, lr_0 = 3.2455e-04
Loss = 4.6902e-03, PNorm = 37.7298, GNorm = 0.5593, lr_0 = 3.2175e-04
Loss = 3.8297e-03, PNorm = 37.7338, GNorm = 1.0414, lr_0 = 3.1898e-04
Loss = 4.3353e-03, PNorm = 37.7393, GNorm = 0.9103, lr_0 = 3.1623e-04
Validation rmse = 1.008820
Epoch 16
Loss = 5.2900e-03, PNorm = 37.7465, GNorm = 1.4327, lr_0 = 3.1350e-04
Loss = 3.8677e-03, PNorm = 37.7545, GNorm = 0.8680, lr_0 = 3.1080e-04
Loss = 4.2003e-03, PNorm = 37.7607, GNorm = 0.8965, lr_0 = 3.0812e-04
Loss = 4.9558e-03, PNorm = 37.7656, GNorm = 1.8225, lr_0 = 3.0547e-04
Loss = 5.0218e-03, PNorm = 37.7694, GNorm = 0.6171, lr_0 = 3.0283e-04
Loss = 4.7915e-03, PNorm = 37.7759, GNorm = 0.7291, lr_0 = 3.0022e-04
Loss = 5.6906e-03, PNorm = 37.7820, GNorm = 1.0294, lr_0 = 2.9764e-04
Loss = 3.6838e-03, PNorm = 37.7863, GNorm = 0.9111, lr_0 = 2.9507e-04
Loss = 4.8792e-03, PNorm = 37.7923, GNorm = 1.3254, lr_0 = 2.9253e-04
Validation rmse = 1.066816
Epoch 17
Loss = 4.2211e-03, PNorm = 37.7962, GNorm = 0.9483, lr_0 = 2.9001e-04
Loss = 5.5246e-03, PNorm = 37.8032, GNorm = 0.8515, lr_0 = 2.8751e-04
Loss = 4.1264e-03, PNorm = 37.8075, GNorm = 1.2898, lr_0 = 2.8503e-04
Loss = 4.0403e-03, PNorm = 37.8119, GNorm = 1.6113, lr_0 = 2.8257e-04
Loss = 4.0628e-03, PNorm = 37.8138, GNorm = 1.7166, lr_0 = 2.8014e-04
Loss = 4.3455e-03, PNorm = 37.8168, GNorm = 0.7711, lr_0 = 2.7772e-04
Loss = 5.4955e-03, PNorm = 37.8255, GNorm = 2.2199, lr_0 = 2.7533e-04
Loss = 3.7792e-03, PNorm = 37.8331, GNorm = 1.0719, lr_0 = 2.7295e-04
Loss = 4.3775e-03, PNorm = 37.8388, GNorm = 0.9676, lr_0 = 2.7060e-04
Loss = 4.7501e-03, PNorm = 37.8469, GNorm = 0.8065, lr_0 = 2.6827e-04
Validation rmse = 0.989254
Epoch 18
Loss = 4.0334e-03, PNorm = 37.8504, GNorm = 1.0045, lr_0 = 2.6596e-04
Loss = 3.6904e-03, PNorm = 37.8529, GNorm = 0.8676, lr_0 = 2.6367e-04
Loss = 5.5286e-03, PNorm = 37.8562, GNorm = 1.9270, lr_0 = 2.6139e-04
Loss = 4.3969e-03, PNorm = 37.8638, GNorm = 0.5751, lr_0 = 2.5914e-04
Loss = 4.8369e-03, PNorm = 37.8671, GNorm = 0.6488, lr_0 = 2.5691e-04
Loss = 4.9215e-03, PNorm = 37.8709, GNorm = 0.7214, lr_0 = 2.5469e-04
Loss = 5.4467e-03, PNorm = 37.8753, GNorm = 1.9634, lr_0 = 2.5250e-04
Loss = 5.5444e-03, PNorm = 37.8825, GNorm = 0.7778, lr_0 = 2.5032e-04
Loss = 5.2163e-03, PNorm = 37.8875, GNorm = 1.8764, lr_0 = 2.4816e-04
Validation rmse = 0.995942
Epoch 19
Loss = 5.1746e-03, PNorm = 37.8899, GNorm = 1.7074, lr_0 = 2.4602e-04
Loss = 4.9636e-03, PNorm = 37.8967, GNorm = 0.9446, lr_0 = 2.4390e-04
Loss = 4.2594e-03, PNorm = 37.9004, GNorm = 0.7682, lr_0 = 2.4180e-04
Loss = 4.6792e-03, PNorm = 37.9039, GNorm = 1.2393, lr_0 = 2.3972e-04
Loss = 3.8165e-03, PNorm = 37.9071, GNorm = 0.7551, lr_0 = 2.3765e-04
Loss = 4.6858e-03, PNorm = 37.9100, GNorm = 1.0016, lr_0 = 2.3560e-04
Loss = 4.5541e-03, PNorm = 37.9144, GNorm = 1.7113, lr_0 = 2.3357e-04
Loss = 3.9416e-03, PNorm = 37.9208, GNorm = 0.7801, lr_0 = 2.3156e-04
Loss = 4.2530e-03, PNorm = 37.9262, GNorm = 0.6755, lr_0 = 2.2956e-04
Loss = 5.2712e-03, PNorm = 37.9324, GNorm = 0.9312, lr_0 = 2.2758e-04
Validation rmse = 0.963310
Epoch 20
Loss = 5.0001e-03, PNorm = 37.9394, GNorm = 1.7165, lr_0 = 2.2562e-04
Loss = 4.3103e-03, PNorm = 37.9461, GNorm = 0.6783, lr_0 = 2.2368e-04
Loss = 4.3426e-03, PNorm = 37.9502, GNorm = 0.8834, lr_0 = 2.2175e-04
Loss = 5.3411e-03, PNorm = 37.9582, GNorm = 1.0161, lr_0 = 2.1984e-04
Loss = 5.2728e-03, PNorm = 37.9623, GNorm = 1.3137, lr_0 = 2.1794e-04
Loss = 4.3120e-03, PNorm = 37.9678, GNorm = 1.8327, lr_0 = 2.1607e-04
Loss = 3.5084e-03, PNorm = 37.9706, GNorm = 0.8102, lr_0 = 2.1420e-04
Loss = 4.5824e-03, PNorm = 37.9727, GNorm = 0.9933, lr_0 = 2.1236e-04
Loss = 4.3031e-03, PNorm = 37.9743, GNorm = 0.9543, lr_0 = 2.1053e-04
Validation rmse = 0.982619
Epoch 21
Loss = 4.4947e-03, PNorm = 37.9780, GNorm = 0.7367, lr_0 = 2.0871e-04
Loss = 4.8680e-03, PNorm = 37.9818, GNorm = 0.9016, lr_0 = 2.0691e-04
Loss = 4.7047e-03, PNorm = 37.9860, GNorm = 0.9554, lr_0 = 2.0513e-04
Loss = 3.9874e-03, PNorm = 37.9883, GNorm = 0.8739, lr_0 = 2.0336e-04
Loss = 4.3993e-03, PNorm = 37.9898, GNorm = 1.3051, lr_0 = 2.0161e-04
Loss = 4.5932e-03, PNorm = 37.9915, GNorm = 0.8712, lr_0 = 1.9987e-04
Loss = 3.7034e-03, PNorm = 37.9947, GNorm = 1.6182, lr_0 = 1.9815e-04
Loss = 5.2668e-03, PNorm = 37.9986, GNorm = 1.5924, lr_0 = 1.9644e-04
Loss = 4.5490e-03, PNorm = 38.0025, GNorm = 1.5568, lr_0 = 1.9475e-04
Loss = 4.8967e-03, PNorm = 38.0069, GNorm = 0.7004, lr_0 = 1.9307e-04
Validation rmse = 1.009114
Epoch 22
Loss = 4.1271e-03, PNorm = 38.0114, GNorm = 1.7273, lr_0 = 1.9141e-04
Loss = 4.6890e-03, PNorm = 38.0147, GNorm = 1.5532, lr_0 = 1.8976e-04
Loss = 4.4303e-03, PNorm = 38.0183, GNorm = 0.7969, lr_0 = 1.8812e-04
Loss = 4.2686e-03, PNorm = 38.0220, GNorm = 0.5284, lr_0 = 1.8650e-04
Loss = 4.2867e-03, PNorm = 38.0271, GNorm = 0.7206, lr_0 = 1.8489e-04
Loss = 4.8867e-03, PNorm = 38.0327, GNorm = 1.1586, lr_0 = 1.8330e-04
Loss = 5.3292e-03, PNorm = 38.0382, GNorm = 0.7101, lr_0 = 1.8172e-04
Loss = 3.8029e-03, PNorm = 38.0416, GNorm = 0.8309, lr_0 = 1.8015e-04
Loss = 4.0240e-03, PNorm = 38.0438, GNorm = 0.8752, lr_0 = 1.7860e-04
Validation rmse = 0.993435
Epoch 23
Loss = 5.0277e-03, PNorm = 38.0461, GNorm = 1.6337, lr_0 = 1.7706e-04
Loss = 5.2599e-03, PNorm = 38.0488, GNorm = 1.5584, lr_0 = 1.7553e-04
Loss = 3.8256e-03, PNorm = 38.0523, GNorm = 1.3624, lr_0 = 1.7402e-04
Loss = 4.8109e-03, PNorm = 38.0572, GNorm = 0.8590, lr_0 = 1.7252e-04
Loss = 3.7316e-03, PNorm = 38.0603, GNorm = 1.1946, lr_0 = 1.7103e-04
Loss = 4.3251e-03, PNorm = 38.0614, GNorm = 1.3291, lr_0 = 1.6956e-04
Loss = 4.6403e-03, PNorm = 38.0651, GNorm = 0.9877, lr_0 = 1.6810e-04
Loss = 4.6945e-03, PNorm = 38.0680, GNorm = 0.9217, lr_0 = 1.6665e-04
Loss = 4.1494e-03, PNorm = 38.0707, GNorm = 0.8014, lr_0 = 1.6521e-04
Loss = 5.0120e-03, PNorm = 38.0747, GNorm = 0.9921, lr_0 = 1.6379e-04
Validation rmse = 0.990048
Epoch 24
Loss = 3.8109e-03, PNorm = 38.0788, GNorm = 2.0740, lr_0 = 1.6238e-04
Loss = 4.5169e-03, PNorm = 38.0831, GNorm = 1.3735, lr_0 = 1.6098e-04
Loss = 3.9341e-03, PNorm = 38.0857, GNorm = 0.5793, lr_0 = 1.5959e-04
Loss = 5.2109e-03, PNorm = 38.0881, GNorm = 0.9847, lr_0 = 1.5822e-04
Loss = 4.8843e-03, PNorm = 38.0908, GNorm = 0.8388, lr_0 = 1.5685e-04
Loss = 4.7826e-03, PNorm = 38.0938, GNorm = 1.0239, lr_0 = 1.5550e-04
Loss = 4.2412e-03, PNorm = 38.0974, GNorm = 1.0792, lr_0 = 1.5416e-04
Loss = 4.0866e-03, PNorm = 38.1004, GNorm = 0.8768, lr_0 = 1.5283e-04
Loss = 3.7698e-03, PNorm = 38.1027, GNorm = 0.7591, lr_0 = 1.5151e-04
Validation rmse = 0.982718
Epoch 25
Loss = 4.4878e-03, PNorm = 38.1057, GNorm = 1.2132, lr_0 = 1.5021e-04
Loss = 3.4177e-03, PNorm = 38.1093, GNorm = 1.0642, lr_0 = 1.4891e-04
Loss = 5.5193e-03, PNorm = 38.1142, GNorm = 0.7877, lr_0 = 1.4763e-04
Loss = 3.6669e-03, PNorm = 38.1185, GNorm = 1.1094, lr_0 = 1.4636e-04
Loss = 4.9050e-03, PNorm = 38.1219, GNorm = 0.8980, lr_0 = 1.4510e-04
Loss = 4.9376e-03, PNorm = 38.1236, GNorm = 0.9532, lr_0 = 1.4384e-04
Loss = 3.6554e-03, PNorm = 38.1250, GNorm = 0.7813, lr_0 = 1.4261e-04
Loss = 4.3757e-03, PNorm = 38.1266, GNorm = 0.9836, lr_0 = 1.4138e-04
Loss = 5.1681e-03, PNorm = 38.1307, GNorm = 1.3812, lr_0 = 1.4016e-04
Loss = 4.6808e-03, PNorm = 38.1358, GNorm = 1.1072, lr_0 = 1.3895e-04
Validation rmse = 0.957825
Epoch 26
Loss = 3.5459e-03, PNorm = 38.1399, GNorm = 0.7740, lr_0 = 1.3775e-04
Loss = 4.4529e-03, PNorm = 38.1417, GNorm = 1.0707, lr_0 = 1.3656e-04
Loss = 5.7311e-03, PNorm = 38.1437, GNorm = 1.3086, lr_0 = 1.3539e-04
Loss = 5.0002e-03, PNorm = 38.1467, GNorm = 2.0027, lr_0 = 1.3422e-04
Loss = 4.4097e-03, PNorm = 38.1475, GNorm = 0.7084, lr_0 = 1.3306e-04
Loss = 4.0659e-03, PNorm = 38.1489, GNorm = 0.9940, lr_0 = 1.3192e-04
Loss = 3.7364e-03, PNorm = 38.1494, GNorm = 0.9018, lr_0 = 1.3078e-04
Loss = 3.8709e-03, PNorm = 38.1508, GNorm = 0.8101, lr_0 = 1.2965e-04
Loss = 4.1774e-03, PNorm = 38.1524, GNorm = 1.0000, lr_0 = 1.2854e-04
Validation rmse = 0.981976
Epoch 27
Loss = 5.5150e-03, PNorm = 38.1552, GNorm = 0.7624, lr_0 = 1.2743e-04
Loss = 4.3392e-03, PNorm = 38.1580, GNorm = 0.9783, lr_0 = 1.2633e-04
Loss = 4.1838e-03, PNorm = 38.1608, GNorm = 1.4040, lr_0 = 1.2524e-04
Loss = 4.6140e-03, PNorm = 38.1636, GNorm = 1.0417, lr_0 = 1.2416e-04
Loss = 3.7384e-03, PNorm = 38.1652, GNorm = 1.1355, lr_0 = 1.2309e-04
Loss = 3.6640e-03, PNorm = 38.1675, GNorm = 1.0115, lr_0 = 1.2203e-04
Loss = 3.8436e-03, PNorm = 38.1698, GNorm = 1.4672, lr_0 = 1.2098e-04
Loss = 3.9441e-03, PNorm = 38.1727, GNorm = 0.8199, lr_0 = 1.1994e-04
Loss = 4.1000e-03, PNorm = 38.1754, GNorm = 0.6169, lr_0 = 1.1890e-04
Loss = 5.1767e-03, PNorm = 38.1779, GNorm = 1.7609, lr_0 = 1.1788e-04
Validation rmse = 0.952833
Epoch 28
Loss = 4.9503e-03, PNorm = 38.1793, GNorm = 1.2145, lr_0 = 1.1686e-04
Loss = 3.9824e-03, PNorm = 38.1814, GNorm = 0.9054, lr_0 = 1.1585e-04
Loss = 4.9454e-03, PNorm = 38.1837, GNorm = 0.8133, lr_0 = 1.1486e-04
Loss = 4.7285e-03, PNorm = 38.1868, GNorm = 0.7967, lr_0 = 1.1387e-04
Loss = 4.1934e-03, PNorm = 38.1892, GNorm = 0.9339, lr_0 = 1.1288e-04
Loss = 4.5352e-03, PNorm = 38.1914, GNorm = 1.4617, lr_0 = 1.1191e-04
Loss = 4.2382e-03, PNorm = 38.1929, GNorm = 1.8837, lr_0 = 1.1095e-04
Loss = 3.9326e-03, PNorm = 38.1934, GNorm = 1.0008, lr_0 = 1.0999e-04
Loss = 4.2829e-03, PNorm = 38.1945, GNorm = 1.0796, lr_0 = 1.0904e-04
Validation rmse = 0.955394
Epoch 29
Loss = 3.8203e-03, PNorm = 38.1967, GNorm = 0.7181, lr_0 = 1.0810e-04
Loss = 4.1764e-03, PNorm = 38.1985, GNorm = 2.0885, lr_0 = 1.0717e-04
Loss = 4.6072e-03, PNorm = 38.1996, GNorm = 1.1786, lr_0 = 1.0625e-04
Loss = 3.7018e-03, PNorm = 38.2018, GNorm = 0.7822, lr_0 = 1.0533e-04
Loss = 3.9124e-03, PNorm = 38.2038, GNorm = 0.7182, lr_0 = 1.0442e-04
Loss = 5.0771e-03, PNorm = 38.2070, GNorm = 0.8601, lr_0 = 1.0352e-04
Loss = 4.5764e-03, PNorm = 38.2095, GNorm = 0.9614, lr_0 = 1.0263e-04
Loss = 4.2647e-03, PNorm = 38.2106, GNorm = 1.0281, lr_0 = 1.0175e-04
Loss = 4.9245e-03, PNorm = 38.2126, GNorm = 1.4402, lr_0 = 1.0087e-04
Loss = 3.7448e-03, PNorm = 38.2139, GNorm = 1.0528, lr_0 = 1.0000e-04
Validation rmse = 0.994920
Model 0 best validation rmse = 0.952833 on epoch 27
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.Wa_i.weight".
Loading pretrained parameter "encoder.attention.Wa_o.weight".
Loading pretrained parameter "encoder.attention.Wa_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.104586
Ensemble test rmse = 1.104586
1-fold cross validation
Seed 0 ==> test rmse = 1.104586
Overall test rmse = 1.104586 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 10,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation(
    (encoder_solute): MPNEncoder(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (encoder_solvent): MPNEncoder(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 328,801
Epoch 0
Loss = 1.8707e-02, PNorm = 38.0924, GNorm = 5.4256, lr_0 = 1.4737e-04
Loss = 1.9091e-02, PNorm = 38.0931, GNorm = 5.0195, lr_0 = 1.9474e-04
Loss = 1.0488e-02, PNorm = 38.0944, GNorm = 4.2552, lr_0 = 2.4211e-04
Loss = 1.3876e-02, PNorm = 38.0975, GNorm = 3.7844, lr_0 = 2.8947e-04
Loss = 1.1637e-02, PNorm = 38.1039, GNorm = 2.1776, lr_0 = 3.3684e-04
Loss = 1.1103e-02, PNorm = 38.1138, GNorm = 3.0946, lr_0 = 3.8421e-04
Loss = 8.4046e-03, PNorm = 38.1246, GNorm = 3.1830, lr_0 = 4.3158e-04
Loss = 7.7158e-03, PNorm = 38.1382, GNorm = 7.0633, lr_0 = 4.7895e-04
Loss = 9.7313e-03, PNorm = 38.1525, GNorm = 4.5671, lr_0 = 5.2632e-04
Validation rmse = 1.278842
Epoch 1
Loss = 7.5682e-03, PNorm = 38.1708, GNorm = 2.7918, lr_0 = 5.7368e-04
Loss = 5.7723e-03, PNorm = 38.1886, GNorm = 3.3757, lr_0 = 6.2105e-04
Loss = 7.1778e-03, PNorm = 38.2057, GNorm = 3.4322, lr_0 = 6.6842e-04
Loss = 6.6362e-03, PNorm = 38.2258, GNorm = 1.3172, lr_0 = 7.1579e-04
Loss = 5.5325e-03, PNorm = 38.2406, GNorm = 3.0469, lr_0 = 7.6316e-04
Loss = 4.7364e-03, PNorm = 38.2565, GNorm = 2.7933, lr_0 = 8.1053e-04
Loss = 3.8138e-03, PNorm = 38.2731, GNorm = 3.1906, lr_0 = 8.5789e-04
Loss = 4.5321e-03, PNorm = 38.2957, GNorm = 7.5106, lr_0 = 9.0526e-04
Loss = 6.2782e-03, PNorm = 38.3199, GNorm = 5.3642, lr_0 = 9.5263e-04
Loss = 5.1918e-03, PNorm = 38.3512, GNorm = 1.7599, lr_0 = 1.0000e-03
Validation rmse = 1.112630
Epoch 2
Loss = 6.3283e-03, PNorm = 38.3866, GNorm = 6.7636, lr_0 = 9.9138e-04
Loss = 5.4202e-03, PNorm = 38.4211, GNorm = 22.9953, lr_0 = 9.8284e-04
Loss = 5.1374e-03, PNorm = 38.4528, GNorm = 2.6500, lr_0 = 9.7437e-04
Loss = 3.8904e-03, PNorm = 38.4891, GNorm = 1.9532, lr_0 = 9.6597e-04
Loss = 5.0451e-03, PNorm = 38.5139, GNorm = 5.4572, lr_0 = 9.5764e-04
Loss = 3.2544e-03, PNorm = 38.5399, GNorm = 0.9677, lr_0 = 9.4939e-04
Loss = 3.7806e-03, PNorm = 38.5596, GNorm = 2.5729, lr_0 = 9.4120e-04
Loss = 3.5029e-03, PNorm = 38.5892, GNorm = 1.9110, lr_0 = 9.3309e-04
Loss = 3.4806e-03, PNorm = 38.6143, GNorm = 5.1468, lr_0 = 9.2505e-04
Validation rmse = 0.925287
Epoch 3
Loss = 4.3530e-03, PNorm = 38.6366, GNorm = 2.2648, lr_0 = 9.1708e-04
Loss = 3.6924e-03, PNorm = 38.6643, GNorm = 1.8391, lr_0 = 9.0917e-04
Loss = 4.3059e-03, PNorm = 38.6951, GNorm = 1.9537, lr_0 = 9.0134e-04
Loss = 2.7976e-03, PNorm = 38.7227, GNorm = 0.7319, lr_0 = 8.9357e-04
Loss = 2.3768e-03, PNorm = 38.7426, GNorm = 1.5015, lr_0 = 8.8587e-04
Loss = 2.9711e-03, PNorm = 38.7644, GNorm = 0.6794, lr_0 = 8.7823e-04
Loss = 3.1323e-03, PNorm = 38.7837, GNorm = 2.0241, lr_0 = 8.7066e-04
Loss = 3.5151e-03, PNorm = 38.7962, GNorm = 0.9077, lr_0 = 8.6316e-04
Loss = 2.0033e-03, PNorm = 38.8120, GNorm = 1.4289, lr_0 = 8.5572e-04
Loss = 2.2230e-03, PNorm = 38.8244, GNorm = 0.6731, lr_0 = 8.4834e-04
Validation rmse = 0.717088
Epoch 4
Loss = 2.1554e-03, PNorm = 38.8375, GNorm = 0.7312, lr_0 = 8.4103e-04
Loss = 1.6321e-03, PNorm = 38.8522, GNorm = 0.9006, lr_0 = 8.3378e-04
Loss = 2.1185e-03, PNorm = 38.8640, GNorm = 1.5105, lr_0 = 8.2660e-04
Loss = 2.4150e-03, PNorm = 38.8789, GNorm = 1.9714, lr_0 = 8.1947e-04
Loss = 1.9954e-03, PNorm = 38.8944, GNorm = 2.3572, lr_0 = 8.1241e-04
Loss = 1.9353e-03, PNorm = 38.9102, GNorm = 1.5440, lr_0 = 8.0541e-04
Loss = 2.1905e-03, PNorm = 38.9230, GNorm = 2.0603, lr_0 = 7.9846e-04
Loss = 2.0150e-03, PNorm = 38.9354, GNorm = 1.5993, lr_0 = 7.9158e-04
Loss = 2.2287e-03, PNorm = 38.9497, GNorm = 2.0295, lr_0 = 7.8476e-04
Validation rmse = 0.733025
Epoch 5
Loss = 1.6552e-03, PNorm = 38.9586, GNorm = 1.2857, lr_0 = 7.7800e-04
Loss = 1.3733e-03, PNorm = 38.9684, GNorm = 0.8353, lr_0 = 7.7129e-04
Loss = 2.1154e-03, PNorm = 38.9779, GNorm = 2.3528, lr_0 = 7.6464e-04
Loss = 2.1945e-03, PNorm = 38.9932, GNorm = 2.4018, lr_0 = 7.5805e-04
Loss = 2.1961e-03, PNorm = 39.0080, GNorm = 3.3030, lr_0 = 7.5152e-04
Loss = 2.1233e-03, PNorm = 39.0156, GNorm = 0.8511, lr_0 = 7.4504e-04
Loss = 1.3575e-03, PNorm = 39.0313, GNorm = 0.6888, lr_0 = 7.3862e-04
Loss = 1.4919e-03, PNorm = 39.0373, GNorm = 1.5739, lr_0 = 7.3225e-04
Loss = 2.0337e-03, PNorm = 39.0475, GNorm = 2.6183, lr_0 = 7.2594e-04
Loss = 1.7290e-03, PNorm = 39.0583, GNorm = 2.3031, lr_0 = 7.1969e-04
Validation rmse = 0.898661
Epoch 6
Loss = 2.0050e-03, PNorm = 39.0700, GNorm = 1.3715, lr_0 = 7.1348e-04
Loss = 1.4618e-03, PNorm = 39.0848, GNorm = 2.3367, lr_0 = 7.0733e-04
Loss = 1.5968e-03, PNorm = 39.0941, GNorm = 1.4492, lr_0 = 7.0124e-04
Loss = 1.8591e-03, PNorm = 39.1079, GNorm = 2.1417, lr_0 = 6.9519e-04
Loss = 1.7826e-03, PNorm = 39.1192, GNorm = 2.9729, lr_0 = 6.8920e-04
Loss = 1.1687e-03, PNorm = 39.1270, GNorm = 1.4902, lr_0 = 6.8326e-04
Loss = 1.5108e-03, PNorm = 39.1345, GNorm = 0.9435, lr_0 = 6.7737e-04
Loss = 1.4259e-03, PNorm = 39.1432, GNorm = 0.8924, lr_0 = 6.7153e-04
Loss = 1.7831e-03, PNorm = 39.1557, GNorm = 1.1475, lr_0 = 6.6575e-04
Validation rmse = 0.631920
Epoch 7
Loss = 1.3007e-03, PNorm = 39.1669, GNorm = 1.0258, lr_0 = 6.6001e-04
Loss = 1.4418e-03, PNorm = 39.1759, GNorm = 0.8443, lr_0 = 6.5432e-04
Loss = 1.3037e-03, PNorm = 39.1831, GNorm = 0.7679, lr_0 = 6.4868e-04
Loss = 1.7600e-03, PNorm = 39.1933, GNorm = 0.5409, lr_0 = 6.4309e-04
Loss = 1.2374e-03, PNorm = 39.2006, GNorm = 0.7700, lr_0 = 6.3755e-04
Loss = 1.4380e-03, PNorm = 39.2059, GNorm = 0.5382, lr_0 = 6.3205e-04
Loss = 1.8598e-03, PNorm = 39.2203, GNorm = 4.2536, lr_0 = 6.2660e-04
Loss = 2.0626e-03, PNorm = 39.2292, GNorm = 1.6064, lr_0 = 6.2120e-04
Loss = 2.2145e-03, PNorm = 39.2423, GNorm = 4.6403, lr_0 = 6.1585e-04
Loss = 1.5937e-03, PNorm = 39.2513, GNorm = 1.0642, lr_0 = 6.1054e-04
Validation rmse = 0.631114
Epoch 8
Loss = 1.1808e-03, PNorm = 39.2610, GNorm = 0.6921, lr_0 = 6.0528e-04
Loss = 1.0874e-03, PNorm = 39.2695, GNorm = 0.5091, lr_0 = 6.0006e-04
Loss = 1.4752e-03, PNorm = 39.2742, GNorm = 0.6271, lr_0 = 5.9489e-04
Loss = 1.1932e-03, PNorm = 39.2807, GNorm = 1.2032, lr_0 = 5.8976e-04
Loss = 1.3282e-03, PNorm = 39.2885, GNorm = 0.8855, lr_0 = 5.8468e-04
Loss = 1.4885e-03, PNorm = 39.3013, GNorm = 1.6178, lr_0 = 5.7964e-04
Loss = 1.3256e-03, PNorm = 39.3091, GNorm = 1.1948, lr_0 = 5.7464e-04
Loss = 1.3937e-03, PNorm = 39.3154, GNorm = 2.8843, lr_0 = 5.6969e-04
Loss = 1.4654e-03, PNorm = 39.3196, GNorm = 0.7919, lr_0 = 5.6478e-04
Validation rmse = 0.601469
Epoch 9
Loss = 1.6462e-03, PNorm = 39.3278, GNorm = 1.7170, lr_0 = 5.5991e-04
Loss = 1.3475e-03, PNorm = 39.3375, GNorm = 0.5718, lr_0 = 5.5509e-04
Loss = 1.1589e-03, PNorm = 39.3499, GNorm = 1.1972, lr_0 = 5.5030e-04
Loss = 1.4075e-03, PNorm = 39.3538, GNorm = 1.0233, lr_0 = 5.4556e-04
Loss = 1.4309e-03, PNorm = 39.3600, GNorm = 1.2218, lr_0 = 5.4086e-04
Loss = 1.1156e-03, PNorm = 39.3669, GNorm = 0.5150, lr_0 = 5.3620e-04
Loss = 1.6892e-03, PNorm = 39.3733, GNorm = 0.7841, lr_0 = 5.3157e-04
Loss = 9.8689e-04, PNorm = 39.3765, GNorm = 0.4846, lr_0 = 5.2699e-04
Loss = 1.0656e-03, PNorm = 39.3845, GNorm = 0.6935, lr_0 = 5.2245e-04
Loss = 1.1513e-03, PNorm = 39.3918, GNorm = 0.8963, lr_0 = 5.1795e-04
Validation rmse = 0.651177
Epoch 10
Loss = 1.0044e-03, PNorm = 39.4003, GNorm = 1.4367, lr_0 = 5.1348e-04
Loss = 1.4688e-03, PNorm = 39.4049, GNorm = 1.2843, lr_0 = 5.0906e-04
Loss = 1.0131e-03, PNorm = 39.4115, GNorm = 0.4864, lr_0 = 5.0467e-04
Loss = 1.1454e-03, PNorm = 39.4145, GNorm = 0.4841, lr_0 = 5.0032e-04
Loss = 1.1684e-03, PNorm = 39.4169, GNorm = 0.8263, lr_0 = 4.9601e-04
Loss = 1.2626e-03, PNorm = 39.4243, GNorm = 1.5603, lr_0 = 4.9173e-04
Loss = 9.9424e-04, PNorm = 39.4293, GNorm = 1.0886, lr_0 = 4.8749e-04
Loss = 1.1070e-03, PNorm = 39.4341, GNorm = 1.6721, lr_0 = 4.8329e-04
Loss = 8.7570e-04, PNorm = 39.4388, GNorm = 0.6038, lr_0 = 4.7913e-04
Validation rmse = 0.571754
Epoch 11
Loss = 1.1732e-03, PNorm = 39.4484, GNorm = 2.4196, lr_0 = 4.7500e-04
Loss = 8.7401e-04, PNorm = 39.4499, GNorm = 0.5235, lr_0 = 4.7090e-04
Loss = 1.0440e-03, PNorm = 39.4532, GNorm = 1.7317, lr_0 = 4.6685e-04
Loss = 1.2927e-03, PNorm = 39.4626, GNorm = 1.5898, lr_0 = 4.6282e-04
Loss = 1.2125e-03, PNorm = 39.4676, GNorm = 0.9257, lr_0 = 4.5883e-04
Loss = 1.0606e-03, PNorm = 39.4727, GNorm = 0.6154, lr_0 = 4.5488e-04
Loss = 1.0373e-03, PNorm = 39.4763, GNorm = 1.1788, lr_0 = 4.5096e-04
Loss = 1.1871e-03, PNorm = 39.4799, GNorm = 1.5154, lr_0 = 4.4707e-04
Loss = 1.7584e-03, PNorm = 39.4844, GNorm = 1.0190, lr_0 = 4.4322e-04
Loss = 1.3850e-03, PNorm = 39.4916, GNorm = 1.5618, lr_0 = 4.3940e-04
Validation rmse = 0.568756
Epoch 12
Loss = 1.1113e-03, PNorm = 39.4987, GNorm = 0.8821, lr_0 = 4.3561e-04
Loss = 1.0865e-03, PNorm = 39.5039, GNorm = 1.2025, lr_0 = 4.3186e-04
Loss = 1.5101e-03, PNorm = 39.5101, GNorm = 0.8164, lr_0 = 4.2813e-04
Loss = 1.1724e-03, PNorm = 39.5168, GNorm = 1.0050, lr_0 = 4.2444e-04
Loss = 1.1963e-03, PNorm = 39.5207, GNorm = 0.6362, lr_0 = 4.2078e-04
Loss = 1.0439e-03, PNorm = 39.5271, GNorm = 0.5657, lr_0 = 4.1716e-04
Loss = 1.1668e-03, PNorm = 39.5333, GNorm = 0.4629, lr_0 = 4.1356e-04
Loss = 8.3793e-04, PNorm = 39.5398, GNorm = 1.6663, lr_0 = 4.1000e-04
Loss = 8.8364e-04, PNorm = 39.5414, GNorm = 0.9534, lr_0 = 4.0646e-04
Validation rmse = 0.545515
Epoch 13
Loss = 1.7044e-03, PNorm = 39.5453, GNorm = 1.1329, lr_0 = 4.0296e-04
Loss = 6.6005e-04, PNorm = 39.5501, GNorm = 0.4115, lr_0 = 3.9949e-04
Loss = 8.9697e-04, PNorm = 39.5553, GNorm = 0.4540, lr_0 = 3.9604e-04
Loss = 1.1313e-03, PNorm = 39.5636, GNorm = 0.7870, lr_0 = 3.9263e-04
Loss = 1.2562e-03, PNorm = 39.5696, GNorm = 0.9788, lr_0 = 3.8925e-04
Loss = 9.0983e-04, PNorm = 39.5727, GNorm = 0.7729, lr_0 = 3.8589e-04
Loss = 9.4497e-04, PNorm = 39.5787, GNorm = 1.4171, lr_0 = 3.8257e-04
Loss = 9.8974e-04, PNorm = 39.5845, GNorm = 1.1274, lr_0 = 3.7927e-04
Loss = 1.2740e-03, PNorm = 39.5875, GNorm = 1.1814, lr_0 = 3.7600e-04
Loss = 9.7379e-04, PNorm = 39.5896, GNorm = 0.5129, lr_0 = 3.7276e-04
Validation rmse = 0.597453
Epoch 14
Loss = 8.1718e-04, PNorm = 39.5923, GNorm = 0.5381, lr_0 = 3.6955e-04
Loss = 9.5178e-04, PNorm = 39.5979, GNorm = 2.0261, lr_0 = 3.6636e-04
Loss = 1.0887e-03, PNorm = 39.6014, GNorm = 0.9459, lr_0 = 3.6320e-04
Loss = 9.3719e-04, PNorm = 39.6077, GNorm = 1.8056, lr_0 = 3.6007e-04
Loss = 7.7187e-04, PNorm = 39.6122, GNorm = 0.9888, lr_0 = 3.5697e-04
Loss = 7.8974e-04, PNorm = 39.6153, GNorm = 0.3901, lr_0 = 3.5389e-04
Loss = 1.1181e-03, PNorm = 39.6187, GNorm = 0.6450, lr_0 = 3.5084e-04
Loss = 1.0580e-03, PNorm = 39.6222, GNorm = 0.8633, lr_0 = 3.4782e-04
Loss = 1.1828e-03, PNorm = 39.6286, GNorm = 2.3564, lr_0 = 3.4482e-04
Validation rmse = 0.555838
Epoch 15
Loss = 1.2615e-03, PNorm = 39.6354, GNorm = 2.5784, lr_0 = 3.4185e-04
Loss = 1.1367e-03, PNorm = 39.6361, GNorm = 0.9487, lr_0 = 3.3890e-04
Loss = 7.1814e-04, PNorm = 39.6412, GNorm = 0.5013, lr_0 = 3.3598e-04
Loss = 1.0195e-03, PNorm = 39.6467, GNorm = 0.9553, lr_0 = 3.3309e-04
Loss = 8.4889e-04, PNorm = 39.6498, GNorm = 0.6253, lr_0 = 3.3022e-04
Loss = 8.9137e-04, PNorm = 39.6539, GNorm = 1.3780, lr_0 = 3.2737e-04
Loss = 7.8840e-04, PNorm = 39.6573, GNorm = 0.6130, lr_0 = 3.2455e-04
Loss = 1.1209e-03, PNorm = 39.6606, GNorm = 3.4396, lr_0 = 3.2175e-04
Loss = 9.3966e-04, PNorm = 39.6641, GNorm = 0.8496, lr_0 = 3.1898e-04
Loss = 9.1944e-04, PNorm = 39.6681, GNorm = 0.6849, lr_0 = 3.1623e-04
Validation rmse = 0.517645
Epoch 16
Loss = 8.5109e-04, PNorm = 39.6715, GNorm = 0.5993, lr_0 = 3.1350e-04
Loss = 7.6069e-04, PNorm = 39.6767, GNorm = 0.9423, lr_0 = 3.1080e-04
Loss = 7.6788e-04, PNorm = 39.6791, GNorm = 0.7162, lr_0 = 3.0812e-04
Loss = 1.3292e-03, PNorm = 39.6808, GNorm = 0.7769, lr_0 = 3.0547e-04
Loss = 8.5271e-04, PNorm = 39.6830, GNorm = 0.4383, lr_0 = 3.0283e-04
Loss = 1.0900e-03, PNorm = 39.6845, GNorm = 0.5927, lr_0 = 3.0022e-04
Loss = 1.1985e-03, PNorm = 39.6889, GNorm = 1.0357, lr_0 = 2.9764e-04
Loss = 6.0584e-04, PNorm = 39.6935, GNorm = 0.9925, lr_0 = 2.9507e-04
Loss = 8.2785e-04, PNorm = 39.6959, GNorm = 1.0315, lr_0 = 2.9253e-04
Validation rmse = 0.523909
Epoch 17
Loss = 7.0812e-04, PNorm = 39.6969, GNorm = 0.7064, lr_0 = 2.9001e-04
Loss = 1.0144e-03, PNorm = 39.7021, GNorm = 0.6128, lr_0 = 2.8751e-04
Loss = 6.8398e-04, PNorm = 39.7049, GNorm = 0.7837, lr_0 = 2.8503e-04
Loss = 9.8144e-04, PNorm = 39.7045, GNorm = 1.6064, lr_0 = 2.8257e-04
Loss = 9.3704e-04, PNorm = 39.7074, GNorm = 1.8497, lr_0 = 2.8014e-04
Loss = 8.4978e-04, PNorm = 39.7111, GNorm = 1.0776, lr_0 = 2.7772e-04
Loss = 9.4222e-04, PNorm = 39.7138, GNorm = 0.4547, lr_0 = 2.7533e-04
Loss = 6.5746e-04, PNorm = 39.7168, GNorm = 0.3649, lr_0 = 2.7295e-04
Loss = 8.0007e-04, PNorm = 39.7179, GNorm = 1.6212, lr_0 = 2.7060e-04
Loss = 8.4070e-04, PNorm = 39.7200, GNorm = 0.4090, lr_0 = 2.6827e-04
Validation rmse = 0.522907
Epoch 18
Loss = 7.5668e-04, PNorm = 39.7230, GNorm = 0.3655, lr_0 = 2.6596e-04
Loss = 6.9040e-04, PNorm = 39.7261, GNorm = 0.4577, lr_0 = 2.6367e-04
Loss = 1.1262e-03, PNorm = 39.7288, GNorm = 1.4727, lr_0 = 2.6139e-04
Loss = 6.3972e-04, PNorm = 39.7310, GNorm = 0.7745, lr_0 = 2.5914e-04
Loss = 7.1397e-04, PNorm = 39.7335, GNorm = 0.5673, lr_0 = 2.5691e-04
Loss = 8.5040e-04, PNorm = 39.7362, GNorm = 0.4679, lr_0 = 2.5469e-04
Loss = 1.0118e-03, PNorm = 39.7373, GNorm = 1.1527, lr_0 = 2.5250e-04
Loss = 8.4283e-04, PNorm = 39.7404, GNorm = 0.6104, lr_0 = 2.5032e-04
Loss = 9.6204e-04, PNorm = 39.7427, GNorm = 0.5775, lr_0 = 2.4816e-04
Validation rmse = 0.553477
Epoch 19
Loss = 9.5414e-04, PNorm = 39.7434, GNorm = 0.8961, lr_0 = 2.4602e-04
Loss = 8.5881e-04, PNorm = 39.7460, GNorm = 1.0688, lr_0 = 2.4390e-04
Loss = 9.7688e-04, PNorm = 39.7492, GNorm = 0.4380, lr_0 = 2.4180e-04
Loss = 6.9992e-04, PNorm = 39.7512, GNorm = 2.1237, lr_0 = 2.3972e-04
Loss = 8.5108e-04, PNorm = 39.7542, GNorm = 0.6992, lr_0 = 2.3765e-04
Loss = 6.7681e-04, PNorm = 39.7568, GNorm = 1.0908, lr_0 = 2.3560e-04
Loss = 8.8247e-04, PNorm = 39.7594, GNorm = 1.7450, lr_0 = 2.3357e-04
Loss = 7.9408e-04, PNorm = 39.7635, GNorm = 0.6558, lr_0 = 2.3156e-04
Loss = 8.1273e-04, PNorm = 39.7655, GNorm = 0.6845, lr_0 = 2.2956e-04
Loss = 8.9816e-04, PNorm = 39.7673, GNorm = 0.4829, lr_0 = 2.2758e-04
Validation rmse = 0.557894
Epoch 20
Loss = 1.0817e-03, PNorm = 39.7687, GNorm = 1.3856, lr_0 = 2.2562e-04
Loss = 1.0374e-03, PNorm = 39.7695, GNorm = 0.7298, lr_0 = 2.2368e-04
Loss = 8.9604e-04, PNorm = 39.7743, GNorm = 1.6728, lr_0 = 2.2175e-04
Loss = 9.8532e-04, PNorm = 39.7789, GNorm = 0.7223, lr_0 = 2.1984e-04
Loss = 7.3678e-04, PNorm = 39.7809, GNorm = 0.5536, lr_0 = 2.1794e-04
Loss = 9.5499e-04, PNorm = 39.7833, GNorm = 1.1932, lr_0 = 2.1607e-04
Loss = 6.7248e-04, PNorm = 39.7857, GNorm = 0.8631, lr_0 = 2.1420e-04
Loss = 8.6231e-04, PNorm = 39.7885, GNorm = 2.0776, lr_0 = 2.1236e-04
Loss = 5.7850e-04, PNorm = 39.7905, GNorm = 0.8295, lr_0 = 2.1053e-04
Validation rmse = 0.550291
Epoch 21
Loss = 8.7223e-04, PNorm = 39.7908, GNorm = 0.8141, lr_0 = 2.0871e-04
Loss = 9.0236e-04, PNorm = 39.7924, GNorm = 0.4819, lr_0 = 2.0691e-04
Loss = 7.3055e-04, PNorm = 39.7941, GNorm = 0.9979, lr_0 = 2.0513e-04
Loss = 6.8197e-04, PNorm = 39.7954, GNorm = 0.4627, lr_0 = 2.0336e-04
Loss = 7.1369e-04, PNorm = 39.7982, GNorm = 0.7844, lr_0 = 2.0161e-04
Loss = 8.1234e-04, PNorm = 39.7986, GNorm = 0.5191, lr_0 = 1.9987e-04
Loss = 7.9399e-04, PNorm = 39.7993, GNorm = 0.7631, lr_0 = 1.9815e-04
Loss = 7.6304e-04, PNorm = 39.8030, GNorm = 1.3555, lr_0 = 1.9644e-04
Loss = 8.4659e-04, PNorm = 39.8056, GNorm = 0.9964, lr_0 = 1.9475e-04
Loss = 6.8931e-04, PNorm = 39.8057, GNorm = 0.4713, lr_0 = 1.9307e-04
Validation rmse = 0.540098
Epoch 22
Loss = 6.4836e-04, PNorm = 39.8076, GNorm = 1.3044, lr_0 = 1.9141e-04
Loss = 6.4997e-04, PNorm = 39.8097, GNorm = 1.2356, lr_0 = 1.8976e-04
Loss = 7.2459e-04, PNorm = 39.8115, GNorm = 0.5855, lr_0 = 1.8812e-04
Loss = 6.7356e-04, PNorm = 39.8131, GNorm = 0.8183, lr_0 = 1.8650e-04
Loss = 8.0511e-04, PNorm = 39.8141, GNorm = 1.4275, lr_0 = 1.8489e-04
Loss = 7.2197e-04, PNorm = 39.8160, GNorm = 0.8139, lr_0 = 1.8330e-04
Loss = 7.6575e-04, PNorm = 39.8162, GNorm = 0.7288, lr_0 = 1.8172e-04
Loss = 9.0984e-04, PNorm = 39.8174, GNorm = 0.4163, lr_0 = 1.8015e-04
Loss = 7.6402e-04, PNorm = 39.8191, GNorm = 0.3068, lr_0 = 1.7860e-04
Validation rmse = 0.502196
Epoch 23
Loss = 9.4524e-04, PNorm = 39.8192, GNorm = 2.9617, lr_0 = 1.7706e-04
Loss = 9.4332e-04, PNorm = 39.8209, GNorm = 0.8725, lr_0 = 1.7553e-04
Loss = 7.2047e-04, PNorm = 39.8231, GNorm = 0.6109, lr_0 = 1.7402e-04
Loss = 9.8070e-04, PNorm = 39.8266, GNorm = 0.8812, lr_0 = 1.7252e-04
Loss = 6.3574e-04, PNorm = 39.8294, GNorm = 0.5725, lr_0 = 1.7103e-04
Loss = 5.1686e-04, PNorm = 39.8308, GNorm = 0.4421, lr_0 = 1.6956e-04
Loss = 7.9592e-04, PNorm = 39.8330, GNorm = 0.5978, lr_0 = 1.6810e-04
Loss = 6.8341e-04, PNorm = 39.8351, GNorm = 0.5599, lr_0 = 1.6665e-04
Loss = 6.4048e-04, PNorm = 39.8356, GNorm = 1.1181, lr_0 = 1.6521e-04
Loss = 8.1309e-04, PNorm = 39.8375, GNorm = 0.4864, lr_0 = 1.6379e-04
Validation rmse = 0.508782
Epoch 24
Loss = 6.0196e-04, PNorm = 39.8394, GNorm = 0.5628, lr_0 = 1.6238e-04
Loss = 7.4039e-04, PNorm = 39.8401, GNorm = 0.3764, lr_0 = 1.6098e-04
Loss = 7.1976e-04, PNorm = 39.8392, GNorm = 0.3576, lr_0 = 1.5959e-04
Loss = 7.2571e-04, PNorm = 39.8413, GNorm = 0.3293, lr_0 = 1.5822e-04
Loss = 8.4847e-04, PNorm = 39.8432, GNorm = 0.9616, lr_0 = 1.5685e-04
Loss = 9.3438e-04, PNorm = 39.8446, GNorm = 0.5461, lr_0 = 1.5550e-04
Loss = 7.2248e-04, PNorm = 39.8477, GNorm = 1.2290, lr_0 = 1.5416e-04
Loss = 7.7834e-04, PNorm = 39.8501, GNorm = 0.6271, lr_0 = 1.5283e-04
Loss = 7.5186e-04, PNorm = 39.8497, GNorm = 0.7466, lr_0 = 1.5151e-04
Validation rmse = 0.529969
Epoch 25
Loss = 8.4121e-04, PNorm = 39.8515, GNorm = 2.6190, lr_0 = 1.5021e-04
Loss = 8.8430e-04, PNorm = 39.8529, GNorm = 0.6510, lr_0 = 1.4891e-04
Loss = 6.8515e-04, PNorm = 39.8534, GNorm = 0.5033, lr_0 = 1.4763e-04
Loss = 5.9088e-04, PNorm = 39.8553, GNorm = 0.5255, lr_0 = 1.4636e-04
Loss = 1.1566e-03, PNorm = 39.8569, GNorm = 0.5321, lr_0 = 1.4510e-04
Loss = 6.3811e-04, PNorm = 39.8584, GNorm = 0.3740, lr_0 = 1.4384e-04
Loss = 6.8070e-04, PNorm = 39.8597, GNorm = 0.7197, lr_0 = 1.4261e-04
Loss = 5.9685e-04, PNorm = 39.8616, GNorm = 0.4995, lr_0 = 1.4138e-04
Loss = 7.5887e-04, PNorm = 39.8636, GNorm = 1.1534, lr_0 = 1.4016e-04
Loss = 6.8391e-04, PNorm = 39.8646, GNorm = 0.8060, lr_0 = 1.3895e-04
Validation rmse = 0.507108
Epoch 26
Loss = 5.5957e-04, PNorm = 39.8652, GNorm = 0.3273, lr_0 = 1.3775e-04
Loss = 6.7666e-04, PNorm = 39.8658, GNorm = 1.1758, lr_0 = 1.3656e-04
Loss = 6.0721e-04, PNorm = 39.8678, GNorm = 0.7660, lr_0 = 1.3539e-04
Loss = 6.4154e-04, PNorm = 39.8699, GNorm = 0.7441, lr_0 = 1.3422e-04
Loss = 8.7999e-04, PNorm = 39.8707, GNorm = 0.3708, lr_0 = 1.3306e-04
Loss = 8.0654e-04, PNorm = 39.8707, GNorm = 1.2153, lr_0 = 1.3192e-04
Loss = 6.6465e-04, PNorm = 39.8724, GNorm = 0.4361, lr_0 = 1.3078e-04
Loss = 6.2093e-04, PNorm = 39.8738, GNorm = 0.6086, lr_0 = 1.2965e-04
Loss = 8.0341e-04, PNorm = 39.8753, GNorm = 0.6006, lr_0 = 1.2854e-04
Validation rmse = 0.514343
Epoch 27
Loss = 8.1783e-04, PNorm = 39.8787, GNorm = 0.5488, lr_0 = 1.2743e-04
Loss = 9.5376e-04, PNorm = 39.8778, GNorm = 2.8250, lr_0 = 1.2633e-04
Loss = 8.7545e-04, PNorm = 39.8767, GNorm = 0.7166, lr_0 = 1.2524e-04
Loss = 7.1144e-04, PNorm = 39.8785, GNorm = 0.2963, lr_0 = 1.2416e-04
Loss = 6.5926e-04, PNorm = 39.8805, GNorm = 0.3234, lr_0 = 1.2309e-04
Loss = 5.8502e-04, PNorm = 39.8821, GNorm = 1.0796, lr_0 = 1.2203e-04
Loss = 6.9442e-04, PNorm = 39.8823, GNorm = 1.3154, lr_0 = 1.2098e-04
Loss = 8.1490e-04, PNorm = 39.8845, GNorm = 0.4604, lr_0 = 1.1994e-04
Loss = 6.1596e-04, PNorm = 39.8861, GNorm = 0.3814, lr_0 = 1.1890e-04
Loss = 8.3272e-04, PNorm = 39.8869, GNorm = 2.4584, lr_0 = 1.1788e-04
Validation rmse = 0.513335
Epoch 28
Loss = 8.3617e-04, PNorm = 39.8870, GNorm = 0.5541, lr_0 = 1.1686e-04
Loss = 8.5074e-04, PNorm = 39.8862, GNorm = 0.8146, lr_0 = 1.1585e-04
Loss = 8.6858e-04, PNorm = 39.8890, GNorm = 0.8626, lr_0 = 1.1486e-04
Loss = 5.9317e-04, PNorm = 39.8909, GNorm = 0.7756, lr_0 = 1.1387e-04
Loss = 6.5102e-04, PNorm = 39.8921, GNorm = 0.4762, lr_0 = 1.1288e-04
Loss = 6.1625e-04, PNorm = 39.8929, GNorm = 1.0502, lr_0 = 1.1191e-04
Loss = 8.3222e-04, PNorm = 39.8937, GNorm = 2.0581, lr_0 = 1.1095e-04
Loss = 5.8987e-04, PNorm = 39.8947, GNorm = 0.6087, lr_0 = 1.0999e-04
Loss = 7.7897e-04, PNorm = 39.8960, GNorm = 1.3186, lr_0 = 1.0904e-04
Validation rmse = 0.504047
Epoch 29
Loss = 5.0471e-04, PNorm = 39.8975, GNorm = 0.8389, lr_0 = 1.0810e-04
Loss = 5.8411e-04, PNorm = 39.8984, GNorm = 1.3971, lr_0 = 1.0717e-04
Loss = 6.6917e-04, PNorm = 39.9002, GNorm = 0.9242, lr_0 = 1.0625e-04
Loss = 6.9153e-04, PNorm = 39.9017, GNorm = 0.3532, lr_0 = 1.0533e-04
Loss = 6.8060e-04, PNorm = 39.9027, GNorm = 0.6469, lr_0 = 1.0442e-04
Loss = 6.3541e-04, PNorm = 39.9035, GNorm = 0.6163, lr_0 = 1.0352e-04
Loss = 6.1168e-04, PNorm = 39.9037, GNorm = 1.1718, lr_0 = 1.0263e-04
Loss = 6.6181e-04, PNorm = 39.9049, GNorm = 0.6854, lr_0 = 1.0175e-04
Loss = 8.2571e-04, PNorm = 39.9070, GNorm = 1.2981, lr_0 = 1.0087e-04
Loss = 5.9401e-04, PNorm = 39.9076, GNorm = 0.3914, lr_0 = 1.0000e-04
Validation rmse = 0.493662
Model 0 best validation rmse = 0.493662 on epoch 29
Loading pretrained parameter "encoder.encoder_solute.cached_zero_vector".
Loading pretrained parameter "encoder.encoder_solute.W_i.weight".
Loading pretrained parameter "encoder.encoder_solute.W_h.weight".
Loading pretrained parameter "encoder.encoder_solute.W_o.weight".
Loading pretrained parameter "encoder.encoder_solute.W_o.bias".
Loading pretrained parameter "encoder.encoder_solvent.cached_zero_vector".
Loading pretrained parameter "encoder.encoder_solvent.W_i.weight".
Loading pretrained parameter "encoder.encoder_solvent.W_h.weight".
Loading pretrained parameter "encoder.encoder_solvent.W_o.weight".
Loading pretrained parameter "encoder.encoder_solvent.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 0.641542
Ensemble test rmse = 0.641542
1-fold cross validation
Seed 0 ==> test rmse = 0.641542
Overall test rmse = 0.641542 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att): Sequential(
        (0): Linear(in_features=400, out_features=400, bias=True)
        (1): ReLU()
        (2): Linear(in_features=400, out_features=1, bias=True)
      )
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 365,402
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att): Sequential(
        (0): Linear(in_features=400, out_features=400, bias=True)
        (1): ReLU()
        (2): Linear(in_features=400, out_features=1, bias=True)
      )
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 365,402
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att): Sequential(
        (0): Linear(in_features=400, out_features=400, bias=True)
        (1): ReLU()
        (2): Linear(in_features=400, out_features=1, bias=True)
        (3): Softmax(dim=1)
      )
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 365,402
Epoch 0
Loss = 2.0924e-02, PNorm = 35.5164, GNorm = 6.9225, lr_0 = 1.4737e-04
Loss = 2.3525e-02, PNorm = 35.5176, GNorm = 4.4124, lr_0 = 1.9474e-04
Loss = 1.2452e-02, PNorm = 35.5198, GNorm = 2.1520, lr_0 = 2.4211e-04
Loss = 1.6282e-02, PNorm = 35.5220, GNorm = 2.8388, lr_0 = 2.8947e-04
Loss = 1.2946e-02, PNorm = 35.5247, GNorm = 1.7741, lr_0 = 3.3684e-04
Loss = 1.4239e-02, PNorm = 35.5274, GNorm = 2.1967, lr_0 = 3.8421e-04
Loss = 1.0726e-02, PNorm = 35.5313, GNorm = 1.8794, lr_0 = 4.3158e-04
Loss = 1.2557e-02, PNorm = 35.5363, GNorm = 2.3187, lr_0 = 4.7895e-04
Loss = 1.4752e-02, PNorm = 35.5429, GNorm = 3.1933, lr_0 = 5.2632e-04
Validation rmse = 1.698106
Epoch 1
Loss = 1.4057e-02, PNorm = 35.5525, GNorm = 4.5044, lr_0 = 5.7368e-04
Loss = 1.0903e-02, PNorm = 35.5625, GNorm = 3.5467, lr_0 = 6.2105e-04
Loss = 1.3822e-02, PNorm = 35.5725, GNorm = 4.0774, lr_0 = 6.6842e-04
Loss = 1.1302e-02, PNorm = 35.5844, GNorm = 1.9195, lr_0 = 7.1579e-04
Loss = 1.0833e-02, PNorm = 35.5967, GNorm = 2.3441, lr_0 = 7.6316e-04
Loss = 9.6663e-03, PNorm = 35.6119, GNorm = 2.1054, lr_0 = 8.1053e-04
Loss = 8.7246e-03, PNorm = 35.6268, GNorm = 3.3714, lr_0 = 8.5789e-04
Loss = 9.6618e-03, PNorm = 35.6456, GNorm = 1.7769, lr_0 = 9.0526e-04
Loss = 9.4210e-03, PNorm = 35.6684, GNorm = 2.2889, lr_0 = 9.5263e-04
Loss = 1.3196e-02, PNorm = 35.6926, GNorm = 3.1521, lr_0 = 1.0000e-03
Validation rmse = 1.577854
Epoch 2
Loss = 1.2204e-02, PNorm = 35.7203, GNorm = 6.8454, lr_0 = 9.9138e-04
Loss = 1.0179e-02, PNorm = 35.7477, GNorm = 2.0583, lr_0 = 9.8284e-04
Loss = 1.0456e-02, PNorm = 35.7727, GNorm = 3.4201, lr_0 = 9.7437e-04
Loss = 9.7257e-03, PNorm = 35.7949, GNorm = 1.8134, lr_0 = 9.6597e-04
Loss = 9.4832e-03, PNorm = 35.8172, GNorm = 4.4392, lr_0 = 9.5764e-04
Loss = 7.2448e-03, PNorm = 35.8383, GNorm = 1.8080, lr_0 = 9.4939e-04
Loss = 8.0628e-03, PNorm = 35.8636, GNorm = 2.9256, lr_0 = 9.4120e-04
Loss = 9.2982e-03, PNorm = 35.8851, GNorm = 2.9039, lr_0 = 9.3309e-04
Loss = 8.2878e-03, PNorm = 35.9072, GNorm = 3.8752, lr_0 = 9.2505e-04
Validation rmse = 1.355336
Epoch 3
Loss = 9.3325e-03, PNorm = 35.9335, GNorm = 2.5755, lr_0 = 9.1708e-04
Loss = 1.1475e-02, PNorm = 35.9588, GNorm = 1.9770, lr_0 = 9.0917e-04
Loss = 9.1618e-03, PNorm = 35.9807, GNorm = 1.5023, lr_0 = 9.0134e-04
Loss = 8.2409e-03, PNorm = 36.0023, GNorm = 0.8481, lr_0 = 8.9357e-04
Loss = 6.5179e-03, PNorm = 36.0187, GNorm = 1.3217, lr_0 = 8.8587e-04
Loss = 7.7297e-03, PNorm = 36.0359, GNorm = 1.2119, lr_0 = 8.7823e-04
Loss = 8.2968e-03, PNorm = 36.0555, GNorm = 1.6742, lr_0 = 8.7066e-04
Loss = 9.7787e-03, PNorm = 36.0805, GNorm = 2.9784, lr_0 = 8.6316e-04
Loss = 7.2783e-03, PNorm = 36.1013, GNorm = 1.8738, lr_0 = 8.5572e-04
Loss = 8.5745e-03, PNorm = 36.1216, GNorm = 1.9675, lr_0 = 8.4834e-04
Validation rmse = 1.389780
Epoch 4
Loss = 7.5061e-03, PNorm = 36.1394, GNorm = 1.5206, lr_0 = 8.4103e-04
Loss = 6.2863e-03, PNorm = 36.1569, GNorm = 1.3947, lr_0 = 8.3378e-04
Loss = 8.3283e-03, PNorm = 36.1767, GNorm = 1.1629, lr_0 = 8.2660e-04
Loss = 8.1538e-03, PNorm = 36.2017, GNorm = 1.3410, lr_0 = 8.1947e-04
Loss = 6.2262e-03, PNorm = 36.2189, GNorm = 1.3561, lr_0 = 8.1241e-04
Loss = 7.1562e-03, PNorm = 36.2373, GNorm = 2.1103, lr_0 = 8.0541e-04
Loss = 8.8142e-03, PNorm = 36.2596, GNorm = 1.8378, lr_0 = 7.9846e-04
Loss = 6.8953e-03, PNorm = 36.2796, GNorm = 1.9953, lr_0 = 7.9158e-04
Loss = 6.7084e-03, PNorm = 36.2949, GNorm = 1.2056, lr_0 = 7.8476e-04
Validation rmse = 1.273515
Epoch 5
Loss = 6.9488e-03, PNorm = 36.3124, GNorm = 2.4938, lr_0 = 7.7800e-04
Loss = 6.1307e-03, PNorm = 36.3257, GNorm = 1.2096, lr_0 = 7.7129e-04
Loss = 8.3321e-03, PNorm = 36.3443, GNorm = 1.9407, lr_0 = 7.6464e-04
Loss = 7.5684e-03, PNorm = 36.3659, GNorm = 1.3906, lr_0 = 7.5805e-04
Loss = 8.0093e-03, PNorm = 36.3821, GNorm = 1.9779, lr_0 = 7.5152e-04
Loss = 8.0433e-03, PNorm = 36.3949, GNorm = 2.7293, lr_0 = 7.4504e-04
Loss = 7.8067e-03, PNorm = 36.4085, GNorm = 4.3026, lr_0 = 7.3862e-04
Loss = 7.4751e-03, PNorm = 36.4293, GNorm = 1.2186, lr_0 = 7.3225e-04
Loss = 8.2178e-03, PNorm = 36.4468, GNorm = 1.7706, lr_0 = 7.2594e-04
Loss = 7.0293e-03, PNorm = 36.4640, GNorm = 1.6152, lr_0 = 7.1969e-04
Validation rmse = 1.242800
Epoch 6
Loss = 6.3009e-03, PNorm = 36.4760, GNorm = 1.2197, lr_0 = 7.1348e-04
Loss = 6.7121e-03, PNorm = 36.4930, GNorm = 1.4165, lr_0 = 7.0733e-04
Loss = 7.2760e-03, PNorm = 36.5100, GNorm = 1.7825, lr_0 = 7.0124e-04
Loss = 7.3081e-03, PNorm = 36.5253, GNorm = 2.7838, lr_0 = 6.9519e-04
Loss = 8.4884e-03, PNorm = 36.5438, GNorm = 2.3992, lr_0 = 6.8920e-04
Loss = 6.4654e-03, PNorm = 36.5589, GNorm = 2.7025, lr_0 = 6.8326e-04
Loss = 6.7672e-03, PNorm = 36.5706, GNorm = 1.5879, lr_0 = 6.7737e-04
Loss = 7.7246e-03, PNorm = 36.5842, GNorm = 1.6338, lr_0 = 6.7153e-04
Loss = 6.4348e-03, PNorm = 36.5959, GNorm = 1.9423, lr_0 = 6.6575e-04
Validation rmse = 1.278431
Epoch 7
Loss = 6.3381e-03, PNorm = 36.6036, GNorm = 1.3567, lr_0 = 6.6001e-04
Loss = 6.5529e-03, PNorm = 36.6199, GNorm = 2.2898, lr_0 = 6.5432e-04
Loss = 6.6172e-03, PNorm = 36.6364, GNorm = 3.1614, lr_0 = 6.4868e-04
Loss = 8.4118e-03, PNorm = 36.6501, GNorm = 0.9234, lr_0 = 6.4309e-04
Loss = 5.8507e-03, PNorm = 36.6626, GNorm = 0.9288, lr_0 = 6.3755e-04
Loss = 6.3181e-03, PNorm = 36.6719, GNorm = 1.7705, lr_0 = 6.3205e-04
Loss = 7.5386e-03, PNorm = 36.6841, GNorm = 2.8040, lr_0 = 6.2660e-04
Loss = 7.1379e-03, PNorm = 36.6973, GNorm = 0.9558, lr_0 = 6.2120e-04
Loss = 6.8003e-03, PNorm = 36.7133, GNorm = 2.9683, lr_0 = 6.1585e-04
Loss = 5.1061e-03, PNorm = 36.7231, GNorm = 0.6247, lr_0 = 6.1054e-04
Validation rmse = 1.194134
Epoch 8
Loss = 5.9411e-03, PNorm = 36.7341, GNorm = 1.7907, lr_0 = 6.0528e-04
Loss = 6.6335e-03, PNorm = 36.7468, GNorm = 1.7836, lr_0 = 6.0006e-04
Loss = 6.1803e-03, PNorm = 36.7542, GNorm = 1.5481, lr_0 = 5.9489e-04
Loss = 7.1937e-03, PNorm = 36.7645, GNorm = 1.4795, lr_0 = 5.8976e-04
Loss = 7.5985e-03, PNorm = 36.7805, GNorm = 3.1479, lr_0 = 5.8468e-04
Loss = 7.0719e-03, PNorm = 36.7970, GNorm = 3.6698, lr_0 = 5.7964e-04
Loss = 5.9069e-03, PNorm = 36.8138, GNorm = 1.5212, lr_0 = 5.7464e-04
Loss = 7.4985e-03, PNorm = 36.8306, GNorm = 2.9616, lr_0 = 5.6969e-04
Loss = 5.9219e-03, PNorm = 36.8480, GNorm = 1.4104, lr_0 = 5.6478e-04
Validation rmse = 1.213571
Epoch 9
Loss = 6.6772e-03, PNorm = 36.8624, GNorm = 1.2913, lr_0 = 5.5991e-04
Loss = 6.0901e-03, PNorm = 36.8754, GNorm = 1.0481, lr_0 = 5.5509e-04
Loss = 7.4418e-03, PNorm = 36.8877, GNorm = 1.9865, lr_0 = 5.5030e-04
Loss = 6.6330e-03, PNorm = 36.9003, GNorm = 1.1733, lr_0 = 5.4556e-04
Loss = 6.4327e-03, PNorm = 36.9123, GNorm = 1.6625, lr_0 = 5.4086e-04
Loss = 6.5809e-03, PNorm = 36.9216, GNorm = 2.2049, lr_0 = 5.3620e-04
Loss = 7.7368e-03, PNorm = 36.9326, GNorm = 1.5433, lr_0 = 5.3157e-04
Loss = 5.4895e-03, PNorm = 36.9436, GNorm = 1.1633, lr_0 = 5.2699e-04
Loss = 7.5863e-03, PNorm = 36.9509, GNorm = 2.0275, lr_0 = 5.2245e-04
Loss = 7.7383e-03, PNorm = 36.9678, GNorm = 2.3712, lr_0 = 5.1795e-04
Validation rmse = 1.186917
Epoch 10
Loss = 7.4934e-03, PNorm = 36.9774, GNorm = 1.5323, lr_0 = 5.1348e-04
Loss = 7.0147e-03, PNorm = 36.9878, GNorm = 1.3255, lr_0 = 5.0906e-04
Loss = 6.1685e-03, PNorm = 36.9985, GNorm = 1.2937, lr_0 = 5.0467e-04
Loss = 5.8504e-03, PNorm = 37.0091, GNorm = 2.0609, lr_0 = 5.0032e-04
Loss = 5.7926e-03, PNorm = 37.0193, GNorm = 1.4863, lr_0 = 4.9601e-04
Loss = 6.3773e-03, PNorm = 37.0292, GNorm = 1.9746, lr_0 = 4.9173e-04
Loss = 6.5458e-03, PNorm = 37.0391, GNorm = 2.1336, lr_0 = 4.8749e-04
Loss = 7.3333e-03, PNorm = 37.0492, GNorm = 3.3846, lr_0 = 4.8329e-04
Loss = 5.6702e-03, PNorm = 37.0569, GNorm = 2.1284, lr_0 = 4.7913e-04
Validation rmse = 1.167121
Epoch 11
Loss = 5.7842e-03, PNorm = 37.0636, GNorm = 2.1604, lr_0 = 4.7500e-04
Loss = 6.5938e-03, PNorm = 37.0697, GNorm = 1.4953, lr_0 = 4.7090e-04
Loss = 6.2871e-03, PNorm = 37.0787, GNorm = 1.3688, lr_0 = 4.6685e-04
Loss = 5.8850e-03, PNorm = 37.0860, GNorm = 1.4507, lr_0 = 4.6282e-04
Loss = 6.0601e-03, PNorm = 37.0925, GNorm = 0.9555, lr_0 = 4.5883e-04
Loss = 7.1407e-03, PNorm = 37.1019, GNorm = 2.2131, lr_0 = 4.5488e-04
Loss = 6.3109e-03, PNorm = 37.1124, GNorm = 1.5663, lr_0 = 4.5096e-04
Loss = 6.0330e-03, PNorm = 37.1248, GNorm = 1.2451, lr_0 = 4.4707e-04
Loss = 7.0730e-03, PNorm = 37.1388, GNorm = 1.9872, lr_0 = 4.4322e-04
Loss = 6.8450e-03, PNorm = 37.1464, GNorm = 2.2856, lr_0 = 4.3940e-04
Validation rmse = 1.176433
Epoch 12
Loss = 5.4992e-03, PNorm = 37.1548, GNorm = 1.6419, lr_0 = 4.3561e-04
Loss = 5.5014e-03, PNorm = 37.1632, GNorm = 1.5821, lr_0 = 4.3186e-04
Loss = 5.5735e-03, PNorm = 37.1689, GNorm = 1.1669, lr_0 = 4.2813e-04
Loss = 6.3596e-03, PNorm = 37.1770, GNorm = 1.5803, lr_0 = 4.2444e-04
Loss = 6.0550e-03, PNorm = 37.1834, GNorm = 1.0578, lr_0 = 4.2078e-04
Loss = 6.4344e-03, PNorm = 37.1893, GNorm = 1.7781, lr_0 = 4.1716e-04
Loss = 7.0505e-03, PNorm = 37.1967, GNorm = 2.9071, lr_0 = 4.1356e-04
Loss = 6.6151e-03, PNorm = 37.2058, GNorm = 1.1509, lr_0 = 4.1000e-04
Loss = 5.5038e-03, PNorm = 37.2169, GNorm = 0.7363, lr_0 = 4.0646e-04
Validation rmse = 1.145515
Epoch 13
Loss = 6.0509e-03, PNorm = 37.2261, GNorm = 1.9201, lr_0 = 4.0296e-04
Loss = 4.3131e-03, PNorm = 37.2329, GNorm = 0.9582, lr_0 = 3.9949e-04
Loss = 4.6124e-03, PNorm = 37.2391, GNorm = 1.6060, lr_0 = 3.9604e-04
Loss = 5.8759e-03, PNorm = 37.2473, GNorm = 1.4027, lr_0 = 3.9263e-04
Loss = 7.5525e-03, PNorm = 37.2586, GNorm = 2.2700, lr_0 = 3.8925e-04
Loss = 6.4331e-03, PNorm = 37.2675, GNorm = 1.4080, lr_0 = 3.8589e-04
Loss = 5.2274e-03, PNorm = 37.2744, GNorm = 1.1166, lr_0 = 3.8257e-04
Loss = 6.8102e-03, PNorm = 37.2819, GNorm = 1.0354, lr_0 = 3.7927e-04
Loss = 6.5238e-03, PNorm = 37.2889, GNorm = 1.5384, lr_0 = 3.7600e-04
Loss = 6.2594e-03, PNorm = 37.2958, GNorm = 1.9035, lr_0 = 3.7276e-04
Validation rmse = 1.220351
Epoch 14
Loss = 6.4757e-03, PNorm = 37.3035, GNorm = 1.4624, lr_0 = 3.6955e-04
Loss = 6.8259e-03, PNorm = 37.3086, GNorm = 1.7002, lr_0 = 3.6636e-04
Loss = 5.6375e-03, PNorm = 37.3165, GNorm = 1.5222, lr_0 = 3.6320e-04
Loss = 5.0164e-03, PNorm = 37.3238, GNorm = 2.5081, lr_0 = 3.6007e-04
Loss = 4.8208e-03, PNorm = 37.3319, GNorm = 1.0440, lr_0 = 3.5697e-04
Loss = 5.5295e-03, PNorm = 37.3416, GNorm = 1.6555, lr_0 = 3.5389e-04
Loss = 6.6056e-03, PNorm = 37.3455, GNorm = 2.0979, lr_0 = 3.5084e-04
Loss = 6.1261e-03, PNorm = 37.3498, GNorm = 2.1664, lr_0 = 3.4782e-04
Loss = 6.1557e-03, PNorm = 37.3539, GNorm = 1.4404, lr_0 = 3.4482e-04
Validation rmse = 1.123076
Epoch 15
Loss = 7.3674e-03, PNorm = 37.3609, GNorm = 1.1026, lr_0 = 3.4185e-04
Loss = 5.5632e-03, PNorm = 37.3680, GNorm = 1.5179, lr_0 = 3.3890e-04
Loss = 4.6166e-03, PNorm = 37.3736, GNorm = 1.7673, lr_0 = 3.3598e-04
Loss = 6.6803e-03, PNorm = 37.3807, GNorm = 2.1982, lr_0 = 3.3309e-04
Loss = 7.0264e-03, PNorm = 37.3899, GNorm = 3.3897, lr_0 = 3.3022e-04
Loss = 6.5476e-03, PNorm = 37.3978, GNorm = 2.6305, lr_0 = 3.2737e-04
Loss = 5.4105e-03, PNorm = 37.4034, GNorm = 1.1460, lr_0 = 3.2455e-04
Loss = 5.4990e-03, PNorm = 37.4059, GNorm = 1.3335, lr_0 = 3.2175e-04
Loss = 4.9836e-03, PNorm = 37.4090, GNorm = 1.9645, lr_0 = 3.1898e-04
Loss = 4.4641e-03, PNorm = 37.4124, GNorm = 0.9110, lr_0 = 3.1623e-04
Validation rmse = 1.105072
Epoch 16
Loss = 6.8431e-03, PNorm = 37.4151, GNorm = 2.2514, lr_0 = 3.1350e-04
Loss = 5.1987e-03, PNorm = 37.4227, GNorm = 0.8507, lr_0 = 3.1080e-04
Loss = 6.2054e-03, PNorm = 37.4283, GNorm = 1.3637, lr_0 = 3.0812e-04
Loss = 6.1368e-03, PNorm = 37.4342, GNorm = 3.6212, lr_0 = 3.0547e-04
Loss = 4.9470e-03, PNorm = 37.4397, GNorm = 0.7320, lr_0 = 3.0283e-04
Loss = 5.1576e-03, PNorm = 37.4463, GNorm = 0.8881, lr_0 = 3.0022e-04
Loss = 6.3632e-03, PNorm = 37.4518, GNorm = 1.1073, lr_0 = 2.9764e-04
Loss = 4.7634e-03, PNorm = 37.4558, GNorm = 0.8358, lr_0 = 2.9507e-04
Loss = 5.9176e-03, PNorm = 37.4608, GNorm = 1.1865, lr_0 = 2.9253e-04
Validation rmse = 1.211727
Epoch 17
Loss = 5.3785e-03, PNorm = 37.4649, GNorm = 1.3195, lr_0 = 2.9001e-04
Loss = 6.5925e-03, PNorm = 37.4711, GNorm = 1.2747, lr_0 = 2.8751e-04
Loss = 4.4106e-03, PNorm = 37.4750, GNorm = 1.2084, lr_0 = 2.8503e-04
Loss = 5.2773e-03, PNorm = 37.4810, GNorm = 1.7200, lr_0 = 2.8257e-04
Loss = 4.9396e-03, PNorm = 37.4872, GNorm = 0.9813, lr_0 = 2.8014e-04
Loss = 6.4673e-03, PNorm = 37.4919, GNorm = 1.6330, lr_0 = 2.7772e-04
Loss = 7.1960e-03, PNorm = 37.5012, GNorm = 1.3112, lr_0 = 2.7533e-04
Loss = 4.9444e-03, PNorm = 37.5084, GNorm = 1.2997, lr_0 = 2.7295e-04
Loss = 6.5682e-03, PNorm = 37.5141, GNorm = 1.2634, lr_0 = 2.7060e-04
Loss = 6.1022e-03, PNorm = 37.5186, GNorm = 0.6439, lr_0 = 2.6827e-04
Validation rmse = 1.143772
Epoch 18
Loss = 4.7669e-03, PNorm = 37.5212, GNorm = 2.3481, lr_0 = 2.6596e-04
Loss = 4.5154e-03, PNorm = 37.5237, GNorm = 1.8969, lr_0 = 2.6367e-04
Loss = 6.7378e-03, PNorm = 37.5270, GNorm = 1.9042, lr_0 = 2.6139e-04
Loss = 5.7436e-03, PNorm = 37.5321, GNorm = 1.7979, lr_0 = 2.5914e-04
Loss = 5.6781e-03, PNorm = 37.5370, GNorm = 1.7170, lr_0 = 2.5691e-04
Loss = 6.0767e-03, PNorm = 37.5411, GNorm = 1.5658, lr_0 = 2.5469e-04
Loss = 5.6683e-03, PNorm = 37.5448, GNorm = 3.0851, lr_0 = 2.5250e-04
Loss = 6.3790e-03, PNorm = 37.5507, GNorm = 1.4091, lr_0 = 2.5032e-04
Loss = 6.4872e-03, PNorm = 37.5561, GNorm = 1.0239, lr_0 = 2.4816e-04
Validation rmse = 1.103412
Epoch 19
Loss = 6.4140e-03, PNorm = 37.5589, GNorm = 1.9510, lr_0 = 2.4602e-04
Loss = 5.5089e-03, PNorm = 37.5621, GNorm = 1.1302, lr_0 = 2.4390e-04
Loss = 5.7465e-03, PNorm = 37.5661, GNorm = 1.2602, lr_0 = 2.4180e-04
Loss = 5.6589e-03, PNorm = 37.5699, GNorm = 1.5766, lr_0 = 2.3972e-04
Loss = 5.1278e-03, PNorm = 37.5726, GNorm = 2.0348, lr_0 = 2.3765e-04
Loss = 5.9071e-03, PNorm = 37.5753, GNorm = 1.3542, lr_0 = 2.3560e-04
Loss = 5.3603e-03, PNorm = 37.5792, GNorm = 1.7007, lr_0 = 2.3357e-04
Loss = 6.1297e-03, PNorm = 37.5838, GNorm = 1.0821, lr_0 = 2.3156e-04
Loss = 5.4965e-03, PNorm = 37.5896, GNorm = 1.0563, lr_0 = 2.2956e-04
Loss = 6.8388e-03, PNorm = 37.5966, GNorm = 1.8868, lr_0 = 2.2758e-04
Validation rmse = 1.079951
Epoch 20
Loss = 6.3905e-03, PNorm = 37.6019, GNorm = 2.5300, lr_0 = 2.2562e-04
Loss = 5.5671e-03, PNorm = 37.6062, GNorm = 1.4710, lr_0 = 2.2368e-04
Loss = 5.6049e-03, PNorm = 37.6113, GNorm = 1.8420, lr_0 = 2.2175e-04
Loss = 6.5821e-03, PNorm = 37.6181, GNorm = 1.3196, lr_0 = 2.1984e-04
Loss = 6.0997e-03, PNorm = 37.6207, GNorm = 1.4232, lr_0 = 2.1794e-04
Loss = 5.3079e-03, PNorm = 37.6262, GNorm = 1.7410, lr_0 = 2.1607e-04
Loss = 4.4649e-03, PNorm = 37.6308, GNorm = 1.4234, lr_0 = 2.1420e-04
Loss = 5.5255e-03, PNorm = 37.6342, GNorm = 2.0796, lr_0 = 2.1236e-04
Loss = 5.4537e-03, PNorm = 37.6367, GNorm = 1.2546, lr_0 = 2.1053e-04
Validation rmse = 1.081819
Epoch 21
Loss = 4.9967e-03, PNorm = 37.6391, GNorm = 1.3569, lr_0 = 2.0871e-04
Loss = 5.8622e-03, PNorm = 37.6428, GNorm = 1.2332, lr_0 = 2.0691e-04
Loss = 6.3610e-03, PNorm = 37.6460, GNorm = 2.0531, lr_0 = 2.0513e-04
Loss = 5.0522e-03, PNorm = 37.6483, GNorm = 2.3886, lr_0 = 2.0336e-04
Loss = 5.1948e-03, PNorm = 37.6519, GNorm = 1.1948, lr_0 = 2.0161e-04
Loss = 5.8550e-03, PNorm = 37.6543, GNorm = 1.7389, lr_0 = 1.9987e-04
Loss = 4.3859e-03, PNorm = 37.6565, GNorm = 1.9494, lr_0 = 1.9815e-04
Loss = 6.3727e-03, PNorm = 37.6593, GNorm = 1.3976, lr_0 = 1.9644e-04
Loss = 5.5272e-03, PNorm = 37.6638, GNorm = 2.2687, lr_0 = 1.9475e-04
Loss = 4.4778e-03, PNorm = 37.6673, GNorm = 1.1621, lr_0 = 1.9307e-04
Validation rmse = 1.074363
Epoch 22
Loss = 5.5291e-03, PNorm = 37.6699, GNorm = 5.0138, lr_0 = 1.9141e-04
Loss = 5.2746e-03, PNorm = 37.6728, GNorm = 1.6276, lr_0 = 1.8976e-04
Loss = 5.4982e-03, PNorm = 37.6762, GNorm = 1.1892, lr_0 = 1.8812e-04
Loss = 6.1555e-03, PNorm = 37.6807, GNorm = 0.8313, lr_0 = 1.8650e-04
Loss = 5.5538e-03, PNorm = 37.6845, GNorm = 1.4339, lr_0 = 1.8489e-04
Loss = 5.9312e-03, PNorm = 37.6881, GNorm = 1.9914, lr_0 = 1.8330e-04
Loss = 6.2551e-03, PNorm = 37.6923, GNorm = 1.1176, lr_0 = 1.8172e-04
Loss = 4.3639e-03, PNorm = 37.6959, GNorm = 0.7412, lr_0 = 1.8015e-04
Loss = 5.3237e-03, PNorm = 37.6984, GNorm = 0.8576, lr_0 = 1.7860e-04
Validation rmse = 1.111964
Epoch 23
Loss = 5.8436e-03, PNorm = 37.7001, GNorm = 2.4536, lr_0 = 1.7706e-04
Loss = 6.7849e-03, PNorm = 37.7026, GNorm = 2.2581, lr_0 = 1.7553e-04
Loss = 5.6287e-03, PNorm = 37.7059, GNorm = 2.0946, lr_0 = 1.7402e-04
Loss = 5.8500e-03, PNorm = 37.7096, GNorm = 1.9587, lr_0 = 1.7252e-04
Loss = 5.7988e-03, PNorm = 37.7118, GNorm = 1.6944, lr_0 = 1.7103e-04
Loss = 5.0588e-03, PNorm = 37.7132, GNorm = 1.5456, lr_0 = 1.6956e-04
Loss = 5.2156e-03, PNorm = 37.7154, GNorm = 0.8100, lr_0 = 1.6810e-04
Loss = 5.3646e-03, PNorm = 37.7178, GNorm = 0.9417, lr_0 = 1.6665e-04
Loss = 4.9611e-03, PNorm = 37.7200, GNorm = 1.5629, lr_0 = 1.6521e-04
Loss = 6.0749e-03, PNorm = 37.7239, GNorm = 1.1249, lr_0 = 1.6379e-04
Validation rmse = 1.096814
Epoch 24
Loss = 4.9554e-03, PNorm = 37.7269, GNorm = 2.0788, lr_0 = 1.6238e-04
Loss = 5.6058e-03, PNorm = 37.7300, GNorm = 0.8328, lr_0 = 1.6098e-04
Loss = 4.9438e-03, PNorm = 37.7313, GNorm = 0.8326, lr_0 = 1.5959e-04
Loss = 5.5776e-03, PNorm = 37.7331, GNorm = 0.8804, lr_0 = 1.5822e-04
Loss = 5.2571e-03, PNorm = 37.7363, GNorm = 0.8168, lr_0 = 1.5685e-04
Loss = 5.5494e-03, PNorm = 37.7393, GNorm = 2.0738, lr_0 = 1.5550e-04
Loss = 5.2771e-03, PNorm = 37.7416, GNorm = 1.2264, lr_0 = 1.5416e-04
Loss = 5.3422e-03, PNorm = 37.7436, GNorm = 1.5344, lr_0 = 1.5283e-04
Loss = 5.4834e-03, PNorm = 37.7455, GNorm = 1.1622, lr_0 = 1.5151e-04
Validation rmse = 1.100179
Epoch 25
Loss = 4.5011e-03, PNorm = 37.7482, GNorm = 2.2205, lr_0 = 1.5021e-04
Loss = 4.2400e-03, PNorm = 37.7507, GNorm = 1.3426, lr_0 = 1.4891e-04
Loss = 6.0022e-03, PNorm = 37.7531, GNorm = 2.6009, lr_0 = 1.4763e-04
Loss = 4.4969e-03, PNorm = 37.7552, GNorm = 0.8835, lr_0 = 1.4636e-04
Loss = 6.0222e-03, PNorm = 37.7571, GNorm = 1.0633, lr_0 = 1.4510e-04
Loss = 5.2993e-03, PNorm = 37.7582, GNorm = 0.7091, lr_0 = 1.4384e-04
Loss = 4.6085e-03, PNorm = 37.7593, GNorm = 1.0844, lr_0 = 1.4261e-04
Loss = 5.5747e-03, PNorm = 37.7607, GNorm = 1.3458, lr_0 = 1.4138e-04
Loss = 6.1653e-03, PNorm = 37.7636, GNorm = 1.6059, lr_0 = 1.4016e-04
Loss = 5.5757e-03, PNorm = 37.7672, GNorm = 2.1633, lr_0 = 1.3895e-04
Validation rmse = 1.065646
Epoch 26
Loss = 4.7498e-03, PNorm = 37.7705, GNorm = 1.4103, lr_0 = 1.3775e-04
Loss = 5.2289e-03, PNorm = 37.7726, GNorm = 1.1611, lr_0 = 1.3656e-04
Loss = 6.3318e-03, PNorm = 37.7748, GNorm = 1.7248, lr_0 = 1.3539e-04
Loss = 6.1646e-03, PNorm = 37.7781, GNorm = 1.1957, lr_0 = 1.3422e-04
Loss = 5.2153e-03, PNorm = 37.7805, GNorm = 1.2048, lr_0 = 1.3306e-04
Loss = 4.7730e-03, PNorm = 37.7822, GNorm = 1.7358, lr_0 = 1.3192e-04
Loss = 5.0379e-03, PNorm = 37.7828, GNorm = 2.1117, lr_0 = 1.3078e-04
Loss = 5.0295e-03, PNorm = 37.7845, GNorm = 0.9337, lr_0 = 1.2965e-04
Loss = 5.5767e-03, PNorm = 37.7865, GNorm = 1.1946, lr_0 = 1.2854e-04
Validation rmse = 1.096180
Epoch 27
Loss = 5.5334e-03, PNorm = 37.7897, GNorm = 1.2040, lr_0 = 1.2743e-04
Loss = 5.4616e-03, PNorm = 37.7926, GNorm = 1.1096, lr_0 = 1.2633e-04
Loss = 4.9870e-03, PNorm = 37.7944, GNorm = 1.2224, lr_0 = 1.2524e-04
Loss = 5.4237e-03, PNorm = 37.7959, GNorm = 2.2130, lr_0 = 1.2416e-04
Loss = 4.6731e-03, PNorm = 37.7974, GNorm = 1.5548, lr_0 = 1.2309e-04
Loss = 4.6597e-03, PNorm = 37.7996, GNorm = 1.0639, lr_0 = 1.2203e-04
Loss = 5.1504e-03, PNorm = 37.8017, GNorm = 2.6381, lr_0 = 1.2098e-04
Loss = 5.7925e-03, PNorm = 37.8037, GNorm = 1.7057, lr_0 = 1.1994e-04
Loss = 5.0658e-03, PNorm = 37.8054, GNorm = 0.7323, lr_0 = 1.1890e-04
Loss = 5.3750e-03, PNorm = 37.8076, GNorm = 2.1661, lr_0 = 1.1788e-04
Validation rmse = 1.075770
Epoch 28
Loss = 5.6374e-03, PNorm = 37.8103, GNorm = 0.7662, lr_0 = 1.1686e-04
Loss = 4.4231e-03, PNorm = 37.8132, GNorm = 0.9515, lr_0 = 1.1585e-04
Loss = 5.7133e-03, PNorm = 37.8164, GNorm = 1.0190, lr_0 = 1.1486e-04
Loss = 6.2461e-03, PNorm = 37.8197, GNorm = 1.2090, lr_0 = 1.1387e-04
Loss = 4.4148e-03, PNorm = 37.8223, GNorm = 1.2999, lr_0 = 1.1288e-04
Loss = 5.5059e-03, PNorm = 37.8248, GNorm = 1.0442, lr_0 = 1.1191e-04
Loss = 5.5560e-03, PNorm = 37.8261, GNorm = 1.7025, lr_0 = 1.1095e-04
Loss = 4.4281e-03, PNorm = 37.8267, GNorm = 0.6401, lr_0 = 1.0999e-04
Loss = 5.7218e-03, PNorm = 37.8286, GNorm = 1.1334, lr_0 = 1.0904e-04
Validation rmse = 1.046760
Epoch 29
Loss = 4.3656e-03, PNorm = 37.8306, GNorm = 0.9607, lr_0 = 1.0810e-04
Loss = 4.7922e-03, PNorm = 37.8318, GNorm = 1.5458, lr_0 = 1.0717e-04
Loss = 5.7627e-03, PNorm = 37.8327, GNorm = 2.0391, lr_0 = 1.0625e-04
Loss = 5.0370e-03, PNorm = 37.8341, GNorm = 0.9081, lr_0 = 1.0533e-04
Loss = 4.1255e-03, PNorm = 37.8358, GNorm = 1.5932, lr_0 = 1.0442e-04
Loss = 6.7007e-03, PNorm = 37.8380, GNorm = 1.0050, lr_0 = 1.0352e-04
Loss = 5.6109e-03, PNorm = 37.8407, GNorm = 1.7474, lr_0 = 1.0263e-04
Loss = 5.4621e-03, PNorm = 37.8424, GNorm = 2.2534, lr_0 = 1.0175e-04
Loss = 6.1130e-03, PNorm = 37.8443, GNorm = 1.4742, lr_0 = 1.0087e-04
Loss = 4.2752e-03, PNorm = 37.8459, GNorm = 1.4006, lr_0 = 1.0000e-04
Validation rmse = 1.078203
Model 0 best validation rmse = 1.046760 on epoch 28
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att.0.weight".
Loading pretrained parameter "encoder.attention.att.0.bias".
Loading pretrained parameter "encoder.attention.att.2.weight".
Loading pretrained parameter "encoder.attention.att.2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.220262
Ensemble test rmse = 1.220262
1-fold cross validation
Seed 0 ==> test rmse = 1.220262
Overall test rmse = 1.220262 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att): Sequential(
        (0): Linear(in_features=20, out_features=20, bias=True)
        (1): ReLU()
        (2): Linear(in_features=20, out_features=1, bias=True)
        (3): Softmax(dim=1)
      )
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Validation rmse = 6.924194
Epoch 1
Loss = 7.9679e-01, PNorm = 9.3311, GNorm = 6.9907, lr_0 = 6.0000e-04
Validation rmse = 6.801025
Epoch 2
Loss = 3.6486e-01, PNorm = 9.3303, GNorm = 2.8458, lr_0 = 9.8189e-04
Validation rmse = 6.657527
Epoch 3
Loss = 2.3255e+00, PNorm = 9.3295, GNorm = 14.1446, lr_0 = 8.9615e-04
Validation rmse = 6.547832
Epoch 4
Loss = 7.2022e-01, PNorm = 9.3291, GNorm = 0.8744, lr_0 = 8.1790e-04
Validation rmse = 6.463979
Epoch 5
Loss = 8.6986e-01, PNorm = 9.3290, GNorm = 3.2479, lr_0 = 7.4648e-04
Validation rmse = 6.373676
Epoch 6
Loss = 1.3068e+00, PNorm = 9.3292, GNorm = 3.3345, lr_0 = 6.8129e-04
Validation rmse = 6.314381
Epoch 7
Loss = 1.6646e+00, PNorm = 9.3290, GNorm = 2.5612, lr_0 = 6.2180e-04
Validation rmse = 6.263549
Epoch 8
Loss = 2.0981e+00, PNorm = 9.3294, GNorm = 25.2887, lr_0 = 5.6750e-04
Validation rmse = 6.201753
Epoch 9
Loss = 1.6968e+00, PNorm = 9.3296, GNorm = 3.8332, lr_0 = 5.1795e-04
Validation rmse = 6.137556
Epoch 10
Validation rmse = 6.095889
Epoch 11
Loss = 5.2164e-01, PNorm = 9.3295, GNorm = 5.0463, lr_0 = 4.7272e-04
Validation rmse = 6.052802
Epoch 12
Loss = 5.2271e+00, PNorm = 9.3294, GNorm = 5.7902, lr_0 = 4.3144e-04
Validation rmse = 5.997127
Epoch 13
Loss = 6.6157e-01, PNorm = 9.3293, GNorm = 6.6948, lr_0 = 3.9377e-04
Validation rmse = 5.963912
Epoch 14
Loss = 1.0692e+00, PNorm = 9.3295, GNorm = 6.5146, lr_0 = 3.5938e-04
Validation rmse = 5.938420
Epoch 15
Loss = 1.9057e+00, PNorm = 9.3298, GNorm = 7.9181, lr_0 = 3.2800e-04
Validation rmse = 5.908975
Epoch 16
Loss = 8.8972e-01, PNorm = 9.3301, GNorm = 6.3941, lr_0 = 2.9936e-04
Validation rmse = 5.884566
Epoch 17
Loss = 9.0973e-01, PNorm = 9.3303, GNorm = 5.1006, lr_0 = 2.7322e-04
Validation rmse = 5.859615
Epoch 18
Loss = 1.5465e+00, PNorm = 9.3303, GNorm = 9.2549, lr_0 = 2.4936e-04
Validation rmse = 5.830559
Epoch 19
Loss = 1.6949e+00, PNorm = 9.3303, GNorm = 0.4590, lr_0 = 2.2758e-04
Validation rmse = 5.799688
Epoch 20
Validation rmse = 5.775401
Epoch 21
Loss = 1.3558e+00, PNorm = 9.3303, GNorm = 7.5740, lr_0 = 2.0771e-04
Validation rmse = 5.754147
Epoch 22
Loss = 2.9226e+00, PNorm = 9.3302, GNorm = 11.3672, lr_0 = 1.8957e-04
Validation rmse = 5.742280
Epoch 23
Loss = 4.9129e-01, PNorm = 9.3301, GNorm = 3.7832, lr_0 = 1.7302e-04
Validation rmse = 5.729885
Epoch 24
Loss = 2.9093e+00, PNorm = 9.3301, GNorm = 7.0295, lr_0 = 1.5791e-04
Validation rmse = 5.714362
Epoch 25
Loss = 1.3764e+00, PNorm = 9.3300, GNorm = 9.7999, lr_0 = 1.4412e-04
Validation rmse = 5.706863
Epoch 26
Loss = 9.9234e-01, PNorm = 9.3299, GNorm = 5.1099, lr_0 = 1.3154e-04
Validation rmse = 5.703780
Epoch 27
Loss = 1.3805e+00, PNorm = 9.3299, GNorm = 7.3742, lr_0 = 1.2005e-04
Validation rmse = 5.698703
Epoch 28
Loss = 1.4516e+00, PNorm = 9.3299, GNorm = 6.2802, lr_0 = 1.0957e-04
Validation rmse = 5.689521
Epoch 29
Loss = 1.4390e+00, PNorm = 9.3299, GNorm = 0.8587, lr_0 = 1.0000e-04
Validation rmse = 5.681537
Model 0 best validation rmse = 5.681537 on epoch 29
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att.0.weight".
Loading pretrained parameter "encoder.attention.att.0.bias".
Loading pretrained parameter "encoder.attention.att.2.weight".
Loading pretrained parameter "encoder.attention.att.2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.983779
Ensemble test rmse = 1.983779
1-fold cross validation
Seed 0 ==> test rmse = 1.983779
Overall test rmse = 1.983779 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att): Sequential(
        (0): Linear(in_features=20, out_features=20, bias=True)
        (1): ReLU()
        (2): Linear(in_features=20, out_features=1, bias=True)
        (3): Softmax(dim=1)
      )
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att): Sequential(
        (0): Linear(in_features=20, out_features=20, bias=True)
        (1): ReLU()
        (2): Linear(in_features=20, out_features=1, bias=True)
        (3): Softmax(dim=1)
      )
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
      (att_3): Softmax(dim=1)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Loss = 9.7958e-01, PNorm = 9.2194, GNorm = 7.6432, lr_0 = 1.0095e-04
Loss = 9.2251e-01, PNorm = 9.2194, GNorm = 2.2337, lr_0 = 1.0189e-04
Loss = 6.0294e-01, PNorm = 9.2196, GNorm = 2.2659, lr_0 = 1.0284e-04
Loss = 1.4542e-01, PNorm = 9.2197, GNorm = 0.9058, lr_0 = 1.0379e-04
Loss = 8.7090e-01, PNorm = 9.2197, GNorm = 0.4876, lr_0 = 1.0473e-04
Loss = 4.0852e+00, PNorm = 9.2199, GNorm = 0.4374, lr_0 = 1.0568e-04
Loss = 1.9600e+00, PNorm = 9.2203, GNorm = 1.6190, lr_0 = 1.0663e-04
Loss = 1.6615e-01, PNorm = 9.2205, GNorm = 0.5610, lr_0 = 1.0758e-04
Loss = 7.7766e-01, PNorm = 9.2208, GNorm = 1.0209, lr_0 = 1.0852e-04
Loss = 8.6504e-01, PNorm = 9.2211, GNorm = 1.2782, lr_0 = 1.0947e-04
Loss = 3.9444e-01, PNorm = 9.2213, GNorm = 2.5555, lr_0 = 1.1042e-04
Loss = 6.2068e-01, PNorm = 9.2215, GNorm = 1.5825, lr_0 = 1.1136e-04
Loss = 3.4644e-01, PNorm = 9.2216, GNorm = 0.0975, lr_0 = 1.1231e-04
Loss = 8.4233e-01, PNorm = 9.2216, GNorm = 0.2885, lr_0 = 1.1326e-04
Loss = 7.3333e-01, PNorm = 9.2214, GNorm = 4.9223, lr_0 = 1.1420e-04
Loss = 9.9108e-01, PNorm = 9.2217, GNorm = 0.0780, lr_0 = 1.1515e-04
Loss = 1.5878e+00, PNorm = 9.2217, GNorm = 2.3894, lr_0 = 1.1610e-04
Loss = 1.8359e+00, PNorm = 9.2217, GNorm = 1.8019, lr_0 = 1.1705e-04
Loss = 7.3907e-01, PNorm = 9.2217, GNorm = 4.1461, lr_0 = 1.1799e-04
Loss = 7.3800e-01, PNorm = 9.2217, GNorm = 1.1175, lr_0 = 1.1894e-04
Loss = 4.8240e-01, PNorm = 9.2219, GNorm = 3.6224, lr_0 = 1.1989e-04
Loss = 1.1926e+00, PNorm = 9.2218, GNorm = 3.2496, lr_0 = 1.2083e-04
Loss = 4.0450e-01, PNorm = 9.2219, GNorm = 0.0618, lr_0 = 1.2178e-04
Loss = 3.4686e-01, PNorm = 9.2219, GNorm = 0.1073, lr_0 = 1.2273e-04
Loss = 1.4622e+00, PNorm = 9.2221, GNorm = 1.2227, lr_0 = 1.2367e-04
Loss = 3.1442e-01, PNorm = 9.2224, GNorm = 1.3522, lr_0 = 1.2462e-04
Loss = 5.4277e-01, PNorm = 9.2229, GNorm = 4.2450, lr_0 = 1.2557e-04
Loss = 4.3112e-01, PNorm = 9.2235, GNorm = 1.2955, lr_0 = 1.2652e-04
Loss = 6.0087e-01, PNorm = 9.2238, GNorm = 0.2399, lr_0 = 1.2746e-04
Loss = 6.7521e-01, PNorm = 9.2239, GNorm = 5.3263, lr_0 = 1.2841e-04
Loss = 1.3254e-01, PNorm = 9.2239, GNorm = 1.5437, lr_0 = 1.2936e-04
Loss = 7.6120e-01, PNorm = 9.2242, GNorm = 2.7911, lr_0 = 1.3030e-04
Loss = 6.4237e-01, PNorm = 9.2244, GNorm = 0.9350, lr_0 = 1.3125e-04
Loss = 3.3958e-01, PNorm = 9.2248, GNorm = 0.6190, lr_0 = 1.3220e-04
Loss = 1.1262e+00, PNorm = 9.2255, GNorm = 3.7145, lr_0 = 1.3314e-04
Loss = 1.2847e+00, PNorm = 9.2260, GNorm = 0.0696, lr_0 = 1.3409e-04
Loss = 2.3586e+00, PNorm = 9.2266, GNorm = 1.0567, lr_0 = 1.3504e-04
Loss = 1.8367e-01, PNorm = 9.2268, GNorm = 0.0293, lr_0 = 1.3598e-04
Loss = 8.8139e-01, PNorm = 9.2269, GNorm = 0.0011, lr_0 = 1.3693e-04
Loss = 5.6186e-01, PNorm = 9.2273, GNorm = 0.8509, lr_0 = 1.3788e-04
Loss = 8.5176e-01, PNorm = 9.2280, GNorm = 0.8122, lr_0 = 1.3883e-04
Loss = 6.0819e-01, PNorm = 9.2287, GNorm = 1.9453, lr_0 = 1.3977e-04
Loss = 2.5541e+00, PNorm = 9.2291, GNorm = 2.6506, lr_0 = 1.4072e-04
Loss = 8.2005e-01, PNorm = 9.2298, GNorm = 2.2931, lr_0 = 1.4167e-04
Loss = 8.4028e-01, PNorm = 9.2301, GNorm = 1.7975, lr_0 = 1.4261e-04
Loss = 8.3624e-01, PNorm = 9.2302, GNorm = 0.2107, lr_0 = 1.4356e-04
Loss = 4.6577e+00, PNorm = 9.2302, GNorm = 16.8201, lr_0 = 1.4451e-04
Loss = 6.2785e-01, PNorm = 9.2300, GNorm = 0.7453, lr_0 = 1.4545e-04
Loss = 1.2580e+00, PNorm = 9.2301, GNorm = 5.7006, lr_0 = 1.4640e-04
Loss = 7.1145e-01, PNorm = 9.2303, GNorm = 0.9521, lr_0 = 1.4735e-04
Loss = 2.5802e+00, PNorm = 9.2304, GNorm = 3.7665, lr_0 = 1.4830e-04
Loss = 3.5704e-01, PNorm = 9.2304, GNorm = 0.4541, lr_0 = 1.4924e-04
Loss = 1.0819e+00, PNorm = 9.2306, GNorm = 1.8866, lr_0 = 1.5019e-04
Loss = 5.1847e-01, PNorm = 9.2309, GNorm = 0.0901, lr_0 = 1.5114e-04
Loss = 3.5659e+00, PNorm = 9.2310, GNorm = 13.6775, lr_0 = 1.5208e-04
Loss = 1.6380e+00, PNorm = 9.2310, GNorm = 0.1135, lr_0 = 1.5303e-04
Loss = 4.6733e+00, PNorm = 9.2311, GNorm = 9.7377, lr_0 = 1.5398e-04
Loss = 1.0904e+00, PNorm = 9.2308, GNorm = 2.0717, lr_0 = 1.5492e-04
Loss = 9.0086e-01, PNorm = 9.2309, GNorm = 0.2993, lr_0 = 1.5587e-04
Loss = 6.5944e-01, PNorm = 9.2311, GNorm = 1.3536, lr_0 = 1.5682e-04
Loss = 1.2422e+00, PNorm = 9.2312, GNorm = 0.5985, lr_0 = 1.5777e-04
Loss = 7.8385e-01, PNorm = 9.2311, GNorm = 1.0552, lr_0 = 1.5871e-04
Loss = 7.0374e-01, PNorm = 9.2312, GNorm = 2.2254, lr_0 = 1.5966e-04
Loss = 2.2164e+00, PNorm = 9.2313, GNorm = 1.1251, lr_0 = 1.6061e-04
Loss = 1.5364e+00, PNorm = 9.2315, GNorm = 0.5575, lr_0 = 1.6155e-04
Loss = 2.3294e+00, PNorm = 9.2320, GNorm = 1.8207, lr_0 = 1.6250e-04
Loss = 3.3498e+00, PNorm = 9.2320, GNorm = 1.7649, lr_0 = 1.6345e-04
Loss = 1.3898e+00, PNorm = 9.2321, GNorm = 1.9973, lr_0 = 1.6439e-04
Loss = 4.8670e-01, PNorm = 9.2323, GNorm = 1.5053, lr_0 = 1.6534e-04
Loss = 2.8749e+00, PNorm = 9.2323, GNorm = 0.4131, lr_0 = 1.6629e-04
Loss = 1.2441e+00, PNorm = 9.2321, GNorm = 0.7447, lr_0 = 1.6723e-04
Loss = 8.9788e-01, PNorm = 9.2321, GNorm = 0.4967, lr_0 = 1.6818e-04
Loss = 6.0409e-01, PNorm = 9.2329, GNorm = 1.3425, lr_0 = 1.6913e-04
Loss = 2.1930e+00, PNorm = 9.2333, GNorm = 1.8075, lr_0 = 1.7008e-04
Loss = 4.2822e-01, PNorm = 9.2335, GNorm = 0.6081, lr_0 = 1.7102e-04
Loss = 8.3526e-01, PNorm = 9.2334, GNorm = 1.8486, lr_0 = 1.7197e-04
Loss = 5.3307e-01, PNorm = 9.2334, GNorm = 1.4757, lr_0 = 1.7292e-04
Loss = 7.1913e-01, PNorm = 9.2337, GNorm = 0.1659, lr_0 = 1.7386e-04
Loss = 9.7583e-01, PNorm = 9.2342, GNorm = 4.2164, lr_0 = 1.7481e-04
Loss = 3.1225e-01, PNorm = 9.2349, GNorm = 0.6853, lr_0 = 1.7576e-04
Loss = 1.0994e+00, PNorm = 9.2354, GNorm = 3.2592, lr_0 = 1.7670e-04
Loss = 7.4680e-01, PNorm = 9.2359, GNorm = 1.2596, lr_0 = 1.7765e-04
Loss = 5.9880e-01, PNorm = 9.2362, GNorm = 0.2602, lr_0 = 1.7860e-04
Loss = 2.4476e+00, PNorm = 9.2366, GNorm = 0.3231, lr_0 = 1.7955e-04
Loss = 2.7633e-01, PNorm = 9.2370, GNorm = 0.5557, lr_0 = 1.8049e-04
Loss = 4.9446e-01, PNorm = 9.2371, GNorm = 1.5059, lr_0 = 1.8144e-04
Loss = 3.8322e-01, PNorm = 9.2372, GNorm = 1.5671, lr_0 = 1.8239e-04
Loss = 8.8866e-01, PNorm = 9.2371, GNorm = 2.6145, lr_0 = 1.8333e-04
Loss = 6.4495e-01, PNorm = 9.2370, GNorm = 2.6705, lr_0 = 1.8428e-04
Loss = 1.3761e+00, PNorm = 9.2370, GNorm = 6.9749, lr_0 = 1.8523e-04
Loss = 1.1496e+00, PNorm = 9.2373, GNorm = 0.7539, lr_0 = 1.8617e-04
Loss = 1.7891e+00, PNorm = 9.2374, GNorm = 8.8209, lr_0 = 1.8712e-04
Loss = 8.2011e-01, PNorm = 9.2373, GNorm = 2.9947, lr_0 = 1.8807e-04
Loss = 6.4620e-01, PNorm = 9.2378, GNorm = 2.5832, lr_0 = 1.8902e-04
Loss = 2.2679e-01, PNorm = 9.2382, GNorm = 1.0407, lr_0 = 1.8996e-04
Loss = 1.9590e+00, PNorm = 9.2388, GNorm = 3.7608, lr_0 = 1.9091e-04
Loss = 4.9279e-01, PNorm = 9.2392, GNorm = 2.5805, lr_0 = 1.9186e-04
Loss = 1.1914e+00, PNorm = 9.2392, GNorm = 0.5300, lr_0 = 1.9280e-04
Loss = 4.1071e-01, PNorm = 9.2390, GNorm = 0.5504, lr_0 = 1.9375e-04
Loss = 8.9169e-01, PNorm = 9.2392, GNorm = 2.7689, lr_0 = 1.9470e-04
Loss = 1.6537e-01, PNorm = 9.2393, GNorm = 0.5459, lr_0 = 1.9564e-04
Loss = 7.8362e-01, PNorm = 9.2393, GNorm = 0.4244, lr_0 = 1.9659e-04
Loss = 1.3044e+00, PNorm = 9.2396, GNorm = 0.5795, lr_0 = 1.9754e-04
Loss = 3.7569e-01, PNorm = 9.2401, GNorm = 1.9751, lr_0 = 1.9848e-04
Loss = 1.4048e-01, PNorm = 9.2405, GNorm = 1.6075, lr_0 = 1.9943e-04
Loss = 1.3032e+00, PNorm = 9.2409, GNorm = 0.1909, lr_0 = 2.0038e-04
Loss = 7.5080e-01, PNorm = 9.2412, GNorm = 2.2614, lr_0 = 2.0133e-04
Loss = 6.7504e-01, PNorm = 9.2415, GNorm = 1.8911, lr_0 = 2.0227e-04
Loss = 4.8640e-01, PNorm = 9.2421, GNorm = 0.1158, lr_0 = 2.0322e-04
Loss = 3.2374e-01, PNorm = 9.2429, GNorm = 0.6881, lr_0 = 2.0417e-04
Loss = 4.7367e-01, PNorm = 9.2434, GNorm = 0.5135, lr_0 = 2.0511e-04
Loss = 6.0396e-01, PNorm = 9.2437, GNorm = 0.9054, lr_0 = 2.0606e-04
Loss = 6.2076e-01, PNorm = 9.2439, GNorm = 0.2762, lr_0 = 2.0701e-04
Loss = 4.1080e-01, PNorm = 9.2439, GNorm = 1.5669, lr_0 = 2.0795e-04
Loss = 5.9488e-01, PNorm = 9.2439, GNorm = 0.2546, lr_0 = 2.0890e-04
Loss = 1.0966e+00, PNorm = 9.2443, GNorm = 3.6010, lr_0 = 2.0985e-04
Loss = 9.4934e-01, PNorm = 9.2456, GNorm = 0.4303, lr_0 = 2.1080e-04
Loss = 1.0545e+00, PNorm = 9.2475, GNorm = 0.1349, lr_0 = 2.1174e-04
Loss = 5.1159e-01, PNorm = 9.2490, GNorm = 1.8501, lr_0 = 2.1269e-04
Loss = 3.9759e-01, PNorm = 9.2498, GNorm = 0.6807, lr_0 = 2.1364e-04
Loss = 3.1604e-01, PNorm = 9.2502, GNorm = 2.6145, lr_0 = 2.1458e-04
Loss = 8.7967e-01, PNorm = 9.2503, GNorm = 2.1397, lr_0 = 2.1553e-04
Loss = 2.8993e+00, PNorm = 9.2507, GNorm = 7.2838, lr_0 = 2.1648e-04
Loss = 1.4325e+00, PNorm = 9.2506, GNorm = 4.7765, lr_0 = 2.1742e-04
Loss = 1.0310e+00, PNorm = 9.2506, GNorm = 2.1122, lr_0 = 2.1837e-04
Loss = 2.6688e-01, PNorm = 9.2506, GNorm = 2.8633, lr_0 = 2.1932e-04
Loss = 5.3002e-01, PNorm = 9.2509, GNorm = 0.0982, lr_0 = 2.2027e-04
Loss = 7.0983e-01, PNorm = 9.2512, GNorm = 1.0697, lr_0 = 2.2121e-04
Loss = 6.0590e-01, PNorm = 9.2520, GNorm = 1.0649, lr_0 = 2.2216e-04
Loss = 9.5908e-01, PNorm = 9.2530, GNorm = 0.3795, lr_0 = 2.2311e-04
Loss = 6.7430e-01, PNorm = 9.2534, GNorm = 0.2007, lr_0 = 2.2405e-04
Loss = 1.8452e-01, PNorm = 9.2540, GNorm = 1.1334, lr_0 = 2.2500e-04
Loss = 9.9759e-01, PNorm = 9.2546, GNorm = 3.4244, lr_0 = 2.2595e-04
Loss = 1.2936e+00, PNorm = 9.2548, GNorm = 0.7202, lr_0 = 2.2689e-04
Loss = 2.6724e-01, PNorm = 9.2549, GNorm = 1.5765, lr_0 = 2.2784e-04
Loss = 5.1747e-01, PNorm = 9.2547, GNorm = 0.8601, lr_0 = 2.2879e-04
Loss = 9.5667e-01, PNorm = 9.2546, GNorm = 0.0909, lr_0 = 2.2973e-04
Loss = 1.0975e+00, PNorm = 9.2550, GNorm = 1.6727, lr_0 = 2.3068e-04
Loss = 3.9731e-01, PNorm = 9.2556, GNorm = 0.3943, lr_0 = 2.3163e-04
Loss = 3.1077e-01, PNorm = 9.2562, GNorm = 0.0143, lr_0 = 2.3258e-04
Loss = 5.6168e-01, PNorm = 9.2565, GNorm = 0.6411, lr_0 = 2.3352e-04
Loss = 5.0533e-01, PNorm = 9.2566, GNorm = 3.5591, lr_0 = 2.3447e-04
Loss = 7.8508e-01, PNorm = 9.2576, GNorm = 0.7806, lr_0 = 2.3542e-04
Loss = 1.8188e-01, PNorm = 9.2582, GNorm = 0.6077, lr_0 = 2.3636e-04
Loss = 1.6841e+00, PNorm = 9.2582, GNorm = 0.9882, lr_0 = 2.3731e-04
Loss = 5.2480e-01, PNorm = 9.2597, GNorm = 0.1895, lr_0 = 2.3826e-04
Loss = 8.5312e-01, PNorm = 9.2612, GNorm = 5.3458, lr_0 = 2.3920e-04
Loss = 5.0177e-01, PNorm = 9.2623, GNorm = 1.5916, lr_0 = 2.4015e-04
Loss = 6.4886e-01, PNorm = 9.2637, GNorm = 1.6536, lr_0 = 2.4110e-04
Loss = 6.1783e-01, PNorm = 9.2656, GNorm = 1.1692, lr_0 = 2.4205e-04
Loss = 9.9520e-01, PNorm = 9.2668, GNorm = 0.5947, lr_0 = 2.4299e-04
Loss = 8.8835e-01, PNorm = 9.2673, GNorm = 1.0475, lr_0 = 2.4394e-04
Loss = 6.8873e-01, PNorm = 9.2680, GNorm = 3.1486, lr_0 = 2.4489e-04
Loss = 2.4604e-01, PNorm = 9.2689, GNorm = 1.6010, lr_0 = 2.4583e-04
Loss = 1.0801e+00, PNorm = 9.2692, GNorm = 1.9386, lr_0 = 2.4678e-04
Loss = 1.0702e+00, PNorm = 9.2697, GNorm = 0.6204, lr_0 = 2.4773e-04
Loss = 1.5698e+00, PNorm = 9.2711, GNorm = 6.3367, lr_0 = 2.4867e-04
Loss = 7.0357e-01, PNorm = 9.2716, GNorm = 3.3910, lr_0 = 2.4962e-04
Loss = 1.0229e+00, PNorm = 9.2719, GNorm = 0.5855, lr_0 = 2.5057e-04
Loss = 3.2726e-01, PNorm = 9.2720, GNorm = 3.7116, lr_0 = 2.5152e-04
Loss = 6.6801e-01, PNorm = 9.2719, GNorm = 0.3447, lr_0 = 2.5246e-04
Loss = 8.4742e-01, PNorm = 9.2725, GNorm = 0.4397, lr_0 = 2.5341e-04
Loss = 6.8605e-01, PNorm = 9.2729, GNorm = 3.5028, lr_0 = 2.5436e-04
Loss = 4.3296e-01, PNorm = 9.2734, GNorm = 0.6173, lr_0 = 2.5530e-04
Loss = 2.1496e-01, PNorm = 9.2737, GNorm = 1.7577, lr_0 = 2.5625e-04
Loss = 9.4451e-01, PNorm = 9.2738, GNorm = 2.6978, lr_0 = 2.5720e-04
Loss = 9.3819e-01, PNorm = 9.2761, GNorm = 4.0100, lr_0 = 2.5814e-04
Loss = 8.2898e-01, PNorm = 9.2771, GNorm = 1.1441, lr_0 = 2.5909e-04
Loss = 1.1265e+00, PNorm = 9.2772, GNorm = 2.2837, lr_0 = 2.6004e-04
Loss = 8.9946e-01, PNorm = 9.2783, GNorm = 2.5210, lr_0 = 2.6098e-04
Loss = 1.4413e+00, PNorm = 9.2813, GNorm = 0.4863, lr_0 = 2.6193e-04
Loss = 6.2036e-01, PNorm = 9.2827, GNorm = 0.6435, lr_0 = 2.6288e-04
Loss = 8.1398e-01, PNorm = 9.2836, GNorm = 5.0588, lr_0 = 2.6383e-04
Loss = 4.1851e+00, PNorm = 9.2844, GNorm = 9.9942, lr_0 = 2.6477e-04
Loss = 1.1732e+00, PNorm = 9.2850, GNorm = 5.6682, lr_0 = 2.6572e-04
Loss = 8.8994e-01, PNorm = 9.2847, GNorm = 0.2262, lr_0 = 2.6667e-04
Loss = 6.1314e-01, PNorm = 9.2853, GNorm = 0.9506, lr_0 = 2.6761e-04
Loss = 1.6646e+00, PNorm = 9.2859, GNorm = 0.9364, lr_0 = 2.6856e-04
Loss = 2.5766e+00, PNorm = 9.2862, GNorm = 6.8626, lr_0 = 2.6951e-04
Loss = 9.8005e-01, PNorm = 9.2867, GNorm = 0.1212, lr_0 = 2.7045e-04
Loss = 1.4674e+00, PNorm = 9.2871, GNorm = 4.8188, lr_0 = 2.7140e-04
Loss = 1.2317e+00, PNorm = 9.2878, GNorm = 1.8413, lr_0 = 2.7235e-04
Loss = 5.2794e-01, PNorm = 9.2885, GNorm = 5.8452, lr_0 = 2.7330e-04
Loss = 1.2463e+00, PNorm = 9.2890, GNorm = 0.9172, lr_0 = 2.7424e-04
Loss = 1.2926e+00, PNorm = 9.2901, GNorm = 1.6655, lr_0 = 2.7519e-04
Loss = 1.2418e+00, PNorm = 9.2902, GNorm = 0.0415, lr_0 = 2.7614e-04
Loss = 8.5748e-01, PNorm = 9.2908, GNorm = 0.4997, lr_0 = 2.7708e-04
Loss = 5.2398e-01, PNorm = 9.2915, GNorm = 0.5651, lr_0 = 2.7803e-04
Loss = 2.9773e-01, PNorm = 9.2918, GNorm = 0.4601, lr_0 = 2.7898e-04
Loss = 3.8304e-01, PNorm = 9.2921, GNorm = 0.6189, lr_0 = 2.7992e-04
Loss = 1.3746e+00, PNorm = 9.2922, GNorm = 0.1906, lr_0 = 2.8087e-04
Loss = 6.4150e-01, PNorm = 9.2922, GNorm = 1.8142, lr_0 = 2.8182e-04
Loss = 9.7882e-01, PNorm = 9.2924, GNorm = 1.4787, lr_0 = 2.8277e-04
Loss = 3.0770e-01, PNorm = 9.2923, GNorm = 0.1230, lr_0 = 2.8371e-04
Loss = 7.9130e-01, PNorm = 9.2927, GNorm = 3.3170, lr_0 = 2.8466e-04
Loss = 1.1684e+00, PNorm = 9.2921, GNorm = 2.3045, lr_0 = 2.8561e-04
Loss = 1.8048e+00, PNorm = 9.2923, GNorm = 2.4170, lr_0 = 2.8655e-04
Loss = 3.9872e+00, PNorm = 9.2929, GNorm = 15.5240, lr_0 = 2.8750e-04
Loss = 1.5410e-01, PNorm = 9.2929, GNorm = 0.3597, lr_0 = 2.8845e-04
Loss = 1.0173e+00, PNorm = 9.2935, GNorm = 0.7507, lr_0 = 2.8939e-04
Loss = 5.2289e-01, PNorm = 9.2942, GNorm = 5.2603, lr_0 = 2.9034e-04
Loss = 1.3805e+00, PNorm = 9.2968, GNorm = 2.8645, lr_0 = 2.9129e-04
Loss = 4.4142e-01, PNorm = 9.2987, GNorm = 0.0167, lr_0 = 2.9223e-04
Loss = 2.9985e-01, PNorm = 9.2998, GNorm = 0.1815, lr_0 = 2.9318e-04
Loss = 9.4699e-01, PNorm = 9.2999, GNorm = 6.3641, lr_0 = 2.9413e-04
Loss = 1.2478e+00, PNorm = 9.3018, GNorm = 4.5789, lr_0 = 2.9508e-04
Loss = 7.2667e-01, PNorm = 9.3047, GNorm = 0.0445, lr_0 = 2.9602e-04
Loss = 7.5668e-01, PNorm = 9.3052, GNorm = 0.4367, lr_0 = 2.9697e-04
Loss = 3.5998e-01, PNorm = 9.3060, GNorm = 1.5021, lr_0 = 2.9792e-04
Loss = 3.7939e+00, PNorm = 9.3070, GNorm = 15.3395, lr_0 = 2.9886e-04
Loss = 5.1607e-01, PNorm = 9.3070, GNorm = 2.7093, lr_0 = 2.9981e-04
Loss = 8.2701e-01, PNorm = 9.3075, GNorm = 2.0688, lr_0 = 3.0076e-04
Loss = 7.4013e-01, PNorm = 9.3084, GNorm = 2.6641, lr_0 = 3.0170e-04
Loss = 4.4907e-01, PNorm = 9.3093, GNorm = 0.4159, lr_0 = 3.0265e-04
Loss = 9.1618e-01, PNorm = 9.3101, GNorm = 4.2364, lr_0 = 3.0360e-04
Loss = 9.0579e-01, PNorm = 9.3104, GNorm = 2.4159, lr_0 = 3.0455e-04
Loss = 1.9761e-01, PNorm = 9.3108, GNorm = 2.5062, lr_0 = 3.0549e-04
Loss = 8.1873e-01, PNorm = 9.3128, GNorm = 0.4384, lr_0 = 3.0644e-04
Loss = 2.0441e-01, PNorm = 9.3152, GNorm = 2.3625, lr_0 = 3.0739e-04
Loss = 3.6800e-01, PNorm = 9.3159, GNorm = 2.5250, lr_0 = 3.0833e-04
Loss = 1.6004e-01, PNorm = 9.3163, GNorm = 0.2365, lr_0 = 3.0928e-04
Loss = 1.1654e+00, PNorm = 9.3165, GNorm = 4.4938, lr_0 = 3.1023e-04
Loss = 1.2825e+00, PNorm = 9.3174, GNorm = 4.9330, lr_0 = 3.1117e-04
Loss = 1.2688e+00, PNorm = 9.3182, GNorm = 0.7235, lr_0 = 3.1212e-04
Loss = 1.2007e+00, PNorm = 9.3184, GNorm = 0.4491, lr_0 = 3.1307e-04
Loss = 1.1543e+00, PNorm = 9.3200, GNorm = 4.5291, lr_0 = 3.1402e-04
Loss = 1.3738e-01, PNorm = 9.3211, GNorm = 0.3800, lr_0 = 3.1496e-04
Loss = 2.2669e+00, PNorm = 9.3219, GNorm = 0.2580, lr_0 = 3.1591e-04
Loss = 8.5593e-01, PNorm = 9.3215, GNorm = 2.4744, lr_0 = 3.1686e-04
Loss = 8.7641e-01, PNorm = 9.3212, GNorm = 0.7313, lr_0 = 3.1780e-04
Loss = 8.5970e-01, PNorm = 9.3221, GNorm = 1.0916, lr_0 = 3.1875e-04
Loss = 1.2201e+00, PNorm = 9.3253, GNorm = 0.9023, lr_0 = 3.1970e-04
Loss = 7.7242e-01, PNorm = 9.3274, GNorm = 2.4237, lr_0 = 3.2064e-04
Loss = 8.9725e-01, PNorm = 9.3292, GNorm = 5.7622, lr_0 = 3.2159e-04
Loss = 2.5717e+00, PNorm = 9.3304, GNorm = 0.7112, lr_0 = 3.2254e-04
Loss = 2.6942e-01, PNorm = 9.3310, GNorm = 1.8019, lr_0 = 3.2348e-04
Loss = 1.3704e-01, PNorm = 9.3315, GNorm = 0.2450, lr_0 = 3.2443e-04
Loss = 6.6088e-01, PNorm = 9.3327, GNorm = 3.4236, lr_0 = 3.2538e-04
Loss = 5.4866e-01, PNorm = 9.3348, GNorm = 2.0458, lr_0 = 3.2633e-04
Loss = 7.2431e-01, PNorm = 9.3378, GNorm = 2.4547, lr_0 = 3.2727e-04
Loss = 7.9818e-01, PNorm = 9.3405, GNorm = 0.0015, lr_0 = 3.2822e-04
Loss = 2.6101e-01, PNorm = 9.3417, GNorm = 1.6435, lr_0 = 3.2917e-04
Loss = 3.7107e-01, PNorm = 9.3429, GNorm = 1.2054, lr_0 = 3.3011e-04
Loss = 1.1674e+00, PNorm = 9.3444, GNorm = 0.1978, lr_0 = 3.3106e-04
Loss = 5.9240e-01, PNorm = 9.3446, GNorm = 2.2914, lr_0 = 3.3201e-04
Loss = 5.6231e-01, PNorm = 9.3442, GNorm = 1.4022, lr_0 = 3.3295e-04
Loss = 8.8027e-01, PNorm = 9.3424, GNorm = 5.6384, lr_0 = 3.3390e-04
Loss = 1.0968e+00, PNorm = 9.3416, GNorm = 2.9901, lr_0 = 3.3485e-04
Loss = 9.2061e-01, PNorm = 9.3420, GNorm = 2.0428, lr_0 = 3.3580e-04
Loss = 3.6998e-01, PNorm = 9.3420, GNorm = 0.4891, lr_0 = 3.3674e-04
Loss = 2.9454e-01, PNorm = 9.3419, GNorm = 1.3131, lr_0 = 3.3769e-04
Loss = 5.1889e-01, PNorm = 9.3414, GNorm = 1.1722, lr_0 = 3.3864e-04
Loss = 1.1581e+00, PNorm = 9.3435, GNorm = 0.3151, lr_0 = 3.3958e-04
Loss = 1.2849e+00, PNorm = 9.3449, GNorm = 1.6288, lr_0 = 3.4053e-04
Loss = 1.0749e+00, PNorm = 9.3464, GNorm = 1.1734, lr_0 = 3.4148e-04
Loss = 4.3728e-01, PNorm = 9.3473, GNorm = 1.9703, lr_0 = 3.4242e-04
Loss = 8.4365e-01, PNorm = 9.3493, GNorm = 0.3775, lr_0 = 3.4337e-04
Loss = 7.2652e-01, PNorm = 9.3509, GNorm = 2.2196, lr_0 = 3.4432e-04
Loss = 9.1557e-01, PNorm = 9.3510, GNorm = 1.4603, lr_0 = 3.4527e-04
Loss = 1.0362e+00, PNorm = 9.3541, GNorm = 0.2538, lr_0 = 3.4621e-04
Loss = 6.5439e-01, PNorm = 9.3560, GNorm = 0.6717, lr_0 = 3.4716e-04
Loss = 3.8712e+00, PNorm = 9.3582, GNorm = 19.3890, lr_0 = 3.4811e-04
Loss = 1.5329e+00, PNorm = 9.3591, GNorm = 0.4970, lr_0 = 3.4905e-04
Loss = 1.8438e+00, PNorm = 9.3604, GNorm = 1.9666, lr_0 = 3.5000e-04
Loss = 6.1039e-01, PNorm = 9.3605, GNorm = 1.1819, lr_0 = 3.5095e-04
Loss = 6.3113e-01, PNorm = 9.3611, GNorm = 0.0103, lr_0 = 3.5189e-04
Loss = 7.7910e-01, PNorm = 9.3609, GNorm = 5.3550, lr_0 = 3.5284e-04
Loss = 8.2506e-01, PNorm = 9.3619, GNorm = 7.5225, lr_0 = 3.5379e-04
Loss = 2.6313e-01, PNorm = 9.3620, GNorm = 0.7699, lr_0 = 3.5473e-04
Loss = 1.0186e+00, PNorm = 9.3638, GNorm = 1.3747, lr_0 = 3.5568e-04
Loss = 4.3835e-01, PNorm = 9.3645, GNorm = 0.7606, lr_0 = 3.5663e-04
Loss = 9.7804e-01, PNorm = 9.3654, GNorm = 4.3839, lr_0 = 3.5758e-04
Loss = 3.5561e-01, PNorm = 9.3668, GNorm = 1.6495, lr_0 = 3.5852e-04
Loss = 6.6123e-01, PNorm = 9.3675, GNorm = 5.5667, lr_0 = 3.5947e-04
Loss = 4.1707e-01, PNorm = 9.3684, GNorm = 3.4349, lr_0 = 3.6042e-04
Loss = 9.9556e-01, PNorm = 9.3689, GNorm = 4.4737, lr_0 = 3.6136e-04
Loss = 3.0051e-01, PNorm = 9.3703, GNorm = 0.1816, lr_0 = 3.6231e-04
Loss = 5.6570e-01, PNorm = 9.3713, GNorm = 0.2656, lr_0 = 3.6326e-04
Loss = 1.7604e+00, PNorm = 9.3723, GNorm = 5.1911, lr_0 = 3.6420e-04
Loss = 4.3424e-01, PNorm = 9.3740, GNorm = 3.5148, lr_0 = 3.6515e-04
Loss = 1.7959e-01, PNorm = 9.3750, GNorm = 0.5565, lr_0 = 3.6610e-04
Loss = 3.9342e-01, PNorm = 9.3753, GNorm = 3.5390, lr_0 = 3.6705e-04
Loss = 1.8227e+00, PNorm = 9.3768, GNorm = 0.4513, lr_0 = 3.6799e-04
Loss = 1.6113e+00, PNorm = 9.3774, GNorm = 2.5919, lr_0 = 3.6894e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=400, out_features=400, bias=True)
      (att_2): Linear(in_features=400, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 365,402
Epoch 0
Loss = 1.9885e-02, PNorm = 35.4653, GNorm = 0.5751, lr_0 = 1.4737e-04
Loss = 2.4767e-02, PNorm = 35.4678, GNorm = 0.2708, lr_0 = 1.9474e-04
Loss = 1.4889e-02, PNorm = 35.4761, GNorm = 0.7101, lr_0 = 2.4211e-04
Loss = 2.1361e-02, PNorm = 35.4957, GNorm = 0.2102, lr_0 = 2.8947e-04
Loss = 1.7336e-02, PNorm = 35.5219, GNorm = 0.2711, lr_0 = 3.3684e-04
Loss = 1.8955e-02, PNorm = 35.5603, GNorm = 0.2474, lr_0 = 3.8421e-04
Loss = 1.6788e-02, PNorm = 35.6138, GNorm = 0.4142, lr_0 = 4.3158e-04
Loss = 2.0302e-02, PNorm = 35.6369, GNorm = 0.7098, lr_0 = 4.7895e-04
Loss = 2.1468e-02, PNorm = 35.7004, GNorm = 0.7525, lr_0 = 5.2632e-04
Validation rmse = 2.294665
Epoch 1
Loss = 2.1938e-02, PNorm = 35.8000, GNorm = 0.4109, lr_0 = 5.7368e-04
Loss = 1.9353e-02, PNorm = 35.9509, GNorm = 0.2467, lr_0 = 6.2105e-04
Loss = 2.0394e-02, PNorm = 36.1043, GNorm = 0.6633, lr_0 = 6.6842e-04
Loss = 1.8209e-02, PNorm = 36.3322, GNorm = 0.2780, lr_0 = 7.1579e-04
Loss = 1.7241e-02, PNorm = 36.5458, GNorm = 0.2745, lr_0 = 7.6316e-04
Loss = 1.6494e-02, PNorm = 36.7021, GNorm = 0.6743, lr_0 = 8.1053e-04
Loss = 1.4878e-02, PNorm = 36.9026, GNorm = 0.1980, lr_0 = 8.5789e-04
Loss = 1.9021e-02, PNorm = 37.2119, GNorm = 0.3053, lr_0 = 9.0526e-04
Loss = 1.8601e-02, PNorm = 37.6160, GNorm = 0.2429, lr_0 = 9.5263e-04
Loss = 2.0425e-02, PNorm = 38.1300, GNorm = 0.7624, lr_0 = 1.0000e-03
Validation rmse = 2.259691
Epoch 2
Loss = 1.8988e-02, PNorm = 38.4437, GNorm = 1.1807, lr_0 = 9.9138e-04
Loss = 1.6872e-02, PNorm = 38.8342, GNorm = 0.9057, lr_0 = 9.8284e-04
Loss = 1.9257e-02, PNorm = 39.4207, GNorm = 0.8997, lr_0 = 9.7437e-04
Loss = 1.7507e-02, PNorm = 39.9645, GNorm = 1.0968, lr_0 = 9.6597e-04
Loss = 1.6202e-02, PNorm = 40.4940, GNorm = 0.9732, lr_0 = 9.5764e-04
Loss = 1.4825e-02, PNorm = 40.8632, GNorm = 0.8667, lr_0 = 9.4939e-04
Loss = 1.4818e-02, PNorm = 41.2605, GNorm = 1.5956, lr_0 = 9.4120e-04
Loss = 1.9314e-02, PNorm = 41.4951, GNorm = 0.3114, lr_0 = 9.3309e-04
Loss = 1.5779e-02, PNorm = 41.8897, GNorm = 0.2340, lr_0 = 9.2505e-04
Validation rmse = 2.144271
Epoch 3
Loss = 1.7367e-02, PNorm = 42.3194, GNorm = 0.5770, lr_0 = 9.1708e-04
Loss = 1.8915e-02, PNorm = 42.7967, GNorm = 1.1057, lr_0 = 9.0917e-04
Loss = 1.5542e-02, PNorm = 43.3425, GNorm = 1.2854, lr_0 = 9.0134e-04
Loss = 1.4364e-02, PNorm = 43.9864, GNorm = 0.3914, lr_0 = 8.9357e-04
Loss = 1.3390e-02, PNorm = 44.4513, GNorm = 0.4863, lr_0 = 8.8587e-04
Loss = 1.4671e-02, PNorm = 44.8984, GNorm = 0.3126, lr_0 = 8.7823e-04
Loss = 1.8626e-02, PNorm = 45.4231, GNorm = 1.3615, lr_0 = 8.7066e-04
Loss = 1.7165e-02, PNorm = 46.0561, GNorm = 1.1833, lr_0 = 8.6316e-04
Loss = 1.7712e-02, PNorm = 46.6229, GNorm = 3.3434, lr_0 = 8.5572e-04
Loss = 1.5962e-02, PNorm = 47.2733, GNorm = 0.8904, lr_0 = 8.4834e-04
Validation rmse = 2.118616
Epoch 4
Loss = 1.4400e-02, PNorm = 47.8504, GNorm = 0.1919, lr_0 = 8.4103e-04
Loss = 1.4034e-02, PNorm = 48.3903, GNorm = 0.6996, lr_0 = 8.3378e-04
Loss = 1.8064e-02, PNorm = 49.0604, GNorm = 1.4295, lr_0 = 8.2660e-04
Loss = 1.7034e-02, PNorm = 49.8356, GNorm = 1.0642, lr_0 = 8.1947e-04
Loss = 1.4240e-02, PNorm = 50.4439, GNorm = 0.3138, lr_0 = 8.1241e-04
Loss = 1.4575e-02, PNorm = 51.0198, GNorm = 0.8654, lr_0 = 8.0541e-04
Loss = 1.6683e-02, PNorm = 51.5879, GNorm = 0.6762, lr_0 = 7.9846e-04
Loss = 1.3268e-02, PNorm = 52.2031, GNorm = 0.6028, lr_0 = 7.9158e-04
Loss = 1.5830e-02, PNorm = 52.7664, GNorm = 0.4946, lr_0 = 7.8476e-04
Validation rmse = 1.994601
Epoch 5
Loss = 1.3698e-02, PNorm = 53.4366, GNorm = 0.5051, lr_0 = 7.7800e-04
Loss = 1.1660e-02, PNorm = 53.9743, GNorm = 0.9387, lr_0 = 7.7129e-04
Loss = 1.5926e-02, PNorm = 54.5887, GNorm = 1.0224, lr_0 = 7.6464e-04
Loss = 1.5671e-02, PNorm = 55.3476, GNorm = 0.8705, lr_0 = 7.5805e-04
Loss = 1.7134e-02, PNorm = 55.9115, GNorm = 1.8263, lr_0 = 7.5152e-04
Loss = 1.4448e-02, PNorm = 56.6780, GNorm = 1.1128, lr_0 = 7.4504e-04
Loss = 1.1480e-02, PNorm = 57.3384, GNorm = 1.9085, lr_0 = 7.3862e-04
Loss = 1.3473e-02, PNorm = 57.8963, GNorm = 0.6158, lr_0 = 7.3225e-04
Loss = 1.4235e-02, PNorm = 58.3282, GNorm = 0.4896, lr_0 = 7.2594e-04
Loss = 1.1443e-02, PNorm = 58.7322, GNorm = 0.5263, lr_0 = 7.1969e-04
Validation rmse = 1.902011
Epoch 6
Loss = 1.1339e-02, PNorm = 59.1576, GNorm = 0.6817, lr_0 = 7.1348e-04
Loss = 1.2390e-02, PNorm = 59.7772, GNorm = 1.0888, lr_0 = 7.0733e-04
Loss = 1.3754e-02, PNorm = 60.2525, GNorm = 1.2221, lr_0 = 7.0124e-04
Loss = 1.3038e-02, PNorm = 60.6814, GNorm = 1.3128, lr_0 = 6.9519e-04
Loss = 1.5400e-02, PNorm = 61.4881, GNorm = 1.9597, lr_0 = 6.8920e-04
Loss = 1.2689e-02, PNorm = 62.3776, GNorm = 0.6594, lr_0 = 6.8326e-04
Loss = 1.2821e-02, PNorm = 63.0386, GNorm = 0.8581, lr_0 = 6.7737e-04
Loss = 1.2831e-02, PNorm = 63.5643, GNorm = 0.7058, lr_0 = 6.7153e-04
Loss = 1.4030e-02, PNorm = 64.2388, GNorm = 0.3096, lr_0 = 6.6575e-04
Validation rmse = 1.844658
Epoch 7
Loss = 1.2548e-02, PNorm = 64.9149, GNorm = 2.4353, lr_0 = 6.6001e-04
Loss = 1.3139e-02, PNorm = 65.6111, GNorm = 0.5780, lr_0 = 6.5432e-04
Loss = 1.2905e-02, PNorm = 66.2682, GNorm = 2.5763, lr_0 = 6.4868e-04
Loss = 1.4939e-02, PNorm = 66.9828, GNorm = 0.6102, lr_0 = 6.4309e-04
Loss = 1.0709e-02, PNorm = 67.3963, GNorm = 2.5998, lr_0 = 6.3755e-04
Loss = 1.0991e-02, PNorm = 67.6938, GNorm = 1.8471, lr_0 = 6.3205e-04
Loss = 1.4570e-02, PNorm = 68.0411, GNorm = 1.9122, lr_0 = 6.2660e-04
Loss = 1.1908e-02, PNorm = 68.5150, GNorm = 2.1290, lr_0 = 6.2120e-04
Loss = 1.1297e-02, PNorm = 69.0535, GNorm = 1.4177, lr_0 = 6.1585e-04
Loss = 1.0598e-02, PNorm = 69.3487, GNorm = 1.3108, lr_0 = 6.1054e-04
Validation rmse = 1.795148
Epoch 8
Loss = 1.1398e-02, PNorm = 69.6107, GNorm = 0.9401, lr_0 = 6.0528e-04
Loss = 1.2227e-02, PNorm = 69.9400, GNorm = 1.9945, lr_0 = 6.0006e-04
Loss = 1.0016e-02, PNorm = 70.4260, GNorm = 1.2521, lr_0 = 5.9489e-04
Loss = 1.3405e-02, PNorm = 71.0672, GNorm = 0.6304, lr_0 = 5.8976e-04
Loss = 1.2161e-02, PNorm = 71.3131, GNorm = 0.8811, lr_0 = 5.8468e-04
Loss = 1.2991e-02, PNorm = 71.6608, GNorm = 3.1059, lr_0 = 5.7964e-04
Loss = 1.0323e-02, PNorm = 72.2000, GNorm = 0.8406, lr_0 = 5.7464e-04
Loss = 1.3028e-02, PNorm = 72.6651, GNorm = 2.4402, lr_0 = 5.6969e-04
Loss = 9.6721e-03, PNorm = 73.1541, GNorm = 1.1543, lr_0 = 5.6478e-04
Validation rmse = 1.740813
Epoch 9
Loss = 1.0919e-02, PNorm = 73.5322, GNorm = 0.7264, lr_0 = 5.5991e-04
Loss = 1.0903e-02, PNorm = 73.9326, GNorm = 1.7010, lr_0 = 5.5509e-04
Loss = 1.1640e-02, PNorm = 74.3073, GNorm = 1.6155, lr_0 = 5.5030e-04
Loss = 1.1317e-02, PNorm = 74.6477, GNorm = 0.5907, lr_0 = 5.4556e-04
Loss = 1.2216e-02, PNorm = 74.9464, GNorm = 4.0286, lr_0 = 5.4086e-04
Loss = 1.0310e-02, PNorm = 75.5650, GNorm = 0.6354, lr_0 = 5.3620e-04
Loss = 1.2025e-02, PNorm = 75.9807, GNorm = 1.9428, lr_0 = 5.3157e-04
Loss = 1.0892e-02, PNorm = 76.1261, GNorm = 1.1740, lr_0 = 5.2699e-04
Loss = 1.0747e-02, PNorm = 76.2529, GNorm = 0.5159, lr_0 = 5.2245e-04
Loss = 1.3772e-02, PNorm = 76.6800, GNorm = 0.7486, lr_0 = 5.1795e-04
Validation rmse = 1.712939
Epoch 10
Loss = 1.0521e-02, PNorm = 77.0101, GNorm = 0.5918, lr_0 = 5.1348e-04
Loss = 1.1704e-02, PNorm = 77.4714, GNorm = 1.0979, lr_0 = 5.0906e-04
Loss = 1.0586e-02, PNorm = 77.7673, GNorm = 0.4500, lr_0 = 5.0467e-04
Loss = 1.0003e-02, PNorm = 77.8715, GNorm = 0.4839, lr_0 = 5.0032e-04
Loss = 1.1467e-02, PNorm = 78.1532, GNorm = 1.6605, lr_0 = 4.9601e-04
Loss = 1.1795e-02, PNorm = 78.6215, GNorm = 1.1307, lr_0 = 4.9173e-04
Loss = 1.0855e-02, PNorm = 79.1173, GNorm = 1.4535, lr_0 = 4.8749e-04
Loss = 1.1992e-02, PNorm = 79.5042, GNorm = 1.0139, lr_0 = 4.8329e-04
Loss = 9.9197e-03, PNorm = 79.7391, GNorm = 1.9025, lr_0 = 4.7913e-04
Validation rmse = 1.690064
Epoch 11
Loss = 9.5819e-03, PNorm = 79.9472, GNorm = 1.2252, lr_0 = 4.7500e-04
Loss = 1.1469e-02, PNorm = 80.2146, GNorm = 1.7619, lr_0 = 4.7090e-04
Loss = 9.1407e-03, PNorm = 80.4599, GNorm = 0.9895, lr_0 = 4.6685e-04
Loss = 1.0717e-02, PNorm = 80.9225, GNorm = 0.8476, lr_0 = 4.6282e-04
Loss = 9.6763e-03, PNorm = 81.0995, GNorm = 0.6746, lr_0 = 4.5883e-04
Loss = 1.1921e-02, PNorm = 81.3642, GNorm = 0.7396, lr_0 = 4.5488e-04
Loss = 1.2585e-02, PNorm = 81.7582, GNorm = 1.0178, lr_0 = 4.5096e-04
Loss = 9.0415e-03, PNorm = 82.0818, GNorm = 0.6325, lr_0 = 4.4707e-04
Loss = 1.1493e-02, PNorm = 82.3224, GNorm = 0.8020, lr_0 = 4.4322e-04
Loss = 1.0856e-02, PNorm = 82.5731, GNorm = 1.5468, lr_0 = 4.3940e-04
Validation rmse = 1.643664
Epoch 12
Loss = 1.2146e-02, PNorm = 82.8481, GNorm = 0.9004, lr_0 = 4.3561e-04
Loss = 9.7259e-03, PNorm = 83.1139, GNorm = 0.4957, lr_0 = 4.3186e-04
Loss = 9.5252e-03, PNorm = 83.3663, GNorm = 1.4996, lr_0 = 4.2813e-04
Loss = 1.1717e-02, PNorm = 83.6269, GNorm = 0.7090, lr_0 = 4.2444e-04
Loss = 1.0480e-02, PNorm = 83.7383, GNorm = 1.1898, lr_0 = 4.2078e-04
Loss = 1.0724e-02, PNorm = 83.9271, GNorm = 0.6091, lr_0 = 4.1716e-04
Loss = 9.5161e-03, PNorm = 84.1790, GNorm = 0.3742, lr_0 = 4.1356e-04
Loss = 1.1122e-02, PNorm = 84.3915, GNorm = 0.6567, lr_0 = 4.1000e-04
Loss = 9.5548e-03, PNorm = 84.5282, GNorm = 0.9264, lr_0 = 4.0646e-04
Validation rmse = 1.635584
Epoch 13
Loss = 9.6657e-03, PNorm = 84.7410, GNorm = 1.5861, lr_0 = 4.0296e-04
Loss = 7.9151e-03, PNorm = 84.9167, GNorm = 0.6334, lr_0 = 3.9949e-04
Loss = 8.9883e-03, PNorm = 85.0237, GNorm = 0.3749, lr_0 = 3.9604e-04
Loss = 1.0697e-02, PNorm = 85.2041, GNorm = 1.4461, lr_0 = 3.9263e-04
Loss = 1.2182e-02, PNorm = 85.5989, GNorm = 1.4515, lr_0 = 3.8925e-04
Loss = 1.0860e-02, PNorm = 85.8576, GNorm = 1.3476, lr_0 = 3.8589e-04
Loss = 9.7122e-03, PNorm = 86.0030, GNorm = 1.5341, lr_0 = 3.8257e-04
Loss = 9.6293e-03, PNorm = 86.2951, GNorm = 1.8710, lr_0 = 3.7927e-04
Loss = 1.0362e-02, PNorm = 86.5795, GNorm = 0.6665, lr_0 = 3.7600e-04
Loss = 1.1453e-02, PNorm = 86.6665, GNorm = 4.0429, lr_0 = 3.7276e-04
Validation rmse = 1.637130
Epoch 14
Loss = 9.8468e-03, PNorm = 86.6973, GNorm = 2.0751, lr_0 = 3.6955e-04
Loss = 1.1566e-02, PNorm = 86.9011, GNorm = 3.5233, lr_0 = 3.6636e-04
Loss = 1.0077e-02, PNorm = 87.0512, GNorm = 0.7554, lr_0 = 3.6320e-04
Loss = 9.0492e-03, PNorm = 87.2356, GNorm = 3.5736, lr_0 = 3.6007e-04
Loss = 9.1556e-03, PNorm = 87.2875, GNorm = 1.5643, lr_0 = 3.5697e-04
Loss = 1.0165e-02, PNorm = 87.4475, GNorm = 1.2038, lr_0 = 3.5389e-04
Loss = 9.1623e-03, PNorm = 87.6733, GNorm = 1.7752, lr_0 = 3.5084e-04
Loss = 1.2873e-02, PNorm = 87.9809, GNorm = 1.7656, lr_0 = 3.4782e-04
Loss = 1.0693e-02, PNorm = 88.1973, GNorm = 1.0137, lr_0 = 3.4482e-04
Validation rmse = 1.592241
Epoch 15
Loss = 1.1395e-02, PNorm = 88.2764, GNorm = 0.8782, lr_0 = 3.4185e-04
Loss = 1.0301e-02, PNorm = 88.4278, GNorm = 0.2718, lr_0 = 3.3890e-04
Loss = 8.4356e-03, PNorm = 88.5030, GNorm = 0.6397, lr_0 = 3.3598e-04
Loss = 1.1421e-02, PNorm = 88.6772, GNorm = 1.8474, lr_0 = 3.3309e-04
Loss = 1.1464e-02, PNorm = 88.9377, GNorm = 1.9260, lr_0 = 3.3022e-04
Loss = 1.2206e-02, PNorm = 89.0979, GNorm = 1.7327, lr_0 = 3.2737e-04
Loss = 9.7352e-03, PNorm = 89.2337, GNorm = 1.2214, lr_0 = 3.2455e-04
Loss = 8.4679e-03, PNorm = 89.3751, GNorm = 1.2854, lr_0 = 3.2175e-04
Loss = 9.4406e-03, PNorm = 89.5678, GNorm = 0.8798, lr_0 = 3.1898e-04
Loss = 8.0029e-03, PNorm = 89.6794, GNorm = 2.1930, lr_0 = 3.1623e-04
Validation rmse = 1.581677
Epoch 16
Loss = 1.1573e-02, PNorm = 89.7630, GNorm = 2.1987, lr_0 = 3.1350e-04
Loss = 1.0600e-02, PNorm = 89.9133, GNorm = 1.0165, lr_0 = 3.1080e-04
Loss = 1.0055e-02, PNorm = 90.0979, GNorm = 0.7216, lr_0 = 3.0812e-04
Loss = 9.7276e-03, PNorm = 90.2917, GNorm = 1.0344, lr_0 = 3.0547e-04
Loss = 1.0015e-02, PNorm = 90.4156, GNorm = 1.5098, lr_0 = 3.0283e-04
Loss = 9.7388e-03, PNorm = 90.5640, GNorm = 4.0530, lr_0 = 3.0022e-04
Loss = 1.0224e-02, PNorm = 90.7659, GNorm = 1.7476, lr_0 = 2.9764e-04
Loss = 7.7308e-03, PNorm = 90.8989, GNorm = 0.4517, lr_0 = 2.9507e-04
Loss = 1.0918e-02, PNorm = 90.9142, GNorm = 1.0142, lr_0 = 2.9253e-04
Validation rmse = 1.591236
Epoch 17
Loss = 9.0401e-03, PNorm = 90.9439, GNorm = 1.4746, lr_0 = 2.9001e-04
Loss = 1.1331e-02, PNorm = 91.0901, GNorm = 0.9949, lr_0 = 2.8751e-04
Loss = 9.0103e-03, PNorm = 91.1941, GNorm = 1.3360, lr_0 = 2.8503e-04
Loss = 9.7032e-03, PNorm = 91.3360, GNorm = 1.5757, lr_0 = 2.8257e-04
Loss = 8.2806e-03, PNorm = 91.4288, GNorm = 0.5108, lr_0 = 2.8014e-04
Loss = 9.3550e-03, PNorm = 91.5641, GNorm = 3.5266, lr_0 = 2.7772e-04
Loss = 1.2490e-02, PNorm = 91.7090, GNorm = 1.8731, lr_0 = 2.7533e-04
Loss = 7.7847e-03, PNorm = 91.8301, GNorm = 1.0740, lr_0 = 2.7295e-04
Loss = 9.7398e-03, PNorm = 91.9534, GNorm = 1.6507, lr_0 = 2.7060e-04
Loss = 1.1342e-02, PNorm = 92.1665, GNorm = 1.0666, lr_0 = 2.6827e-04
Validation rmse = 1.552627
Epoch 18
Loss = 8.3280e-03, PNorm = 92.1853, GNorm = 0.8364, lr_0 = 2.6596e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Validation rmse = 5.249261
Epoch 1
Loss = 3.0211e-01, PNorm = 9.3222, GNorm = 1.6928, lr_0 = 6.0000e-04
Validation rmse = 5.242540
Epoch 2
Loss = 1.5564e-01, PNorm = 9.3209, GNorm = 0.2325, lr_0 = 9.8189e-04
Validation rmse = 5.220659
Epoch 3
Loss = 8.1551e-01, PNorm = 9.3201, GNorm = 2.9144, lr_0 = 8.9615e-04
Validation rmse = 5.190946
Epoch 4
Loss = 1.2581e+00, PNorm = 9.3198, GNorm = 0.2341, lr_0 = 8.1790e-04
Validation rmse = 5.175968
Epoch 5
Loss = 1.3169e+00, PNorm = 9.3195, GNorm = 3.1216, lr_0 = 7.4648e-04
Validation rmse = 5.163504
Epoch 6
Loss = 1.4019e+00, PNorm = 9.3198, GNorm = 2.8454, lr_0 = 6.8129e-04
Validation rmse = 5.155612
Epoch 7
Loss = 1.1972e+00, PNorm = 9.3204, GNorm = 0.6120, lr_0 = 6.2180e-04
Validation rmse = 5.139258
Epoch 8
Loss = 1.1412e+00, PNorm = 9.3209, GNorm = 2.9583, lr_0 = 5.6750e-04
Validation rmse = 5.125258
Epoch 9
Loss = 1.0187e+00, PNorm = 9.3217, GNorm = 4.0575, lr_0 = 5.1795e-04
Validation rmse = 5.119585
Epoch 10
Validation rmse = 5.116346
Epoch 11
Loss = 2.4696e+00, PNorm = 9.3224, GNorm = 4.0509, lr_0 = 4.7272e-04
Validation rmse = 5.109031
Epoch 12
Loss = 1.6113e+00, PNorm = 9.3234, GNorm = 3.4969, lr_0 = 4.3144e-04
Validation rmse = 5.095712
Epoch 13
Loss = 5.3235e-01, PNorm = 9.3242, GNorm = 3.2074, lr_0 = 3.9377e-04
Validation rmse = 5.090361
Epoch 14
Loss = 1.3631e+00, PNorm = 9.3253, GNorm = 2.9428, lr_0 = 3.5938e-04
Validation rmse = 5.083143
Epoch 15
Loss = 1.3116e+00, PNorm = 9.3263, GNorm = 4.0427, lr_0 = 3.2800e-04
Validation rmse = 5.079672
Epoch 16
Loss = 1.0137e+00, PNorm = 9.3272, GNorm = 4.0400, lr_0 = 2.9936e-04
Validation rmse = 5.076671
Epoch 17
Loss = 9.9540e-01, PNorm = 9.3281, GNorm = 4.0376, lr_0 = 2.7322e-04
Validation rmse = 5.073075
Epoch 18
Loss = 9.4440e-01, PNorm = 9.3291, GNorm = 0.2066, lr_0 = 2.4936e-04
Validation rmse = 5.071414
Epoch 19
Loss = 1.0084e+00, PNorm = 9.3301, GNorm = 0.6096, lr_0 = 2.2758e-04
Validation rmse = 5.065305
Epoch 20
Validation rmse = 5.058723
Epoch 21
Loss = 7.3847e-01, PNorm = 9.3308, GNorm = 2.7026, lr_0 = 2.0771e-04
Validation rmse = 5.053812
Epoch 22
Loss = 1.9987e+00, PNorm = 9.3317, GNorm = 3.1795, lr_0 = 1.8957e-04
Validation rmse = 5.053587
Epoch 23
Loss = 3.8536e-01, PNorm = 9.3325, GNorm = 1.8387, lr_0 = 1.7302e-04
Validation rmse = 5.048470
Epoch 24
Loss = 1.1863e+00, PNorm = 9.3331, GNorm = 3.1771, lr_0 = 1.5791e-04
Validation rmse = 5.047170
Epoch 25
Loss = 1.4442e+00, PNorm = 9.3338, GNorm = 4.0299, lr_0 = 1.4412e-04
Validation rmse = 5.042914
Epoch 26
Loss = 1.3245e+00, PNorm = 9.3344, GNorm = 2.6858, lr_0 = 1.3154e-04
Validation rmse = 5.040409
Epoch 27
Loss = 1.1868e+00, PNorm = 9.3350, GNorm = 4.0686, lr_0 = 1.2005e-04
Validation rmse = 5.037411
Epoch 28
Loss = 9.4037e-01, PNorm = 9.3354, GNorm = 4.0692, lr_0 = 1.0957e-04
Validation rmse = 5.034017
Epoch 29
Loss = 1.0039e+00, PNorm = 9.3360, GNorm = 0.6096, lr_0 = 1.0000e-04
Validation rmse = 5.033074
Model 0 best validation rmse = 5.033074 on epoch 29
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.942924
Ensemble test rmse = 1.942924
1-fold cross validation
Seed 0 ==> test rmse = 1.942924
Overall test rmse = 1.942924 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Validation rmse = 4.941927
Epoch 1
Loss = 4.4555e-01, PNorm = 9.0325, GNorm = 2.0404, lr_0 = 6.0000e-04
Validation rmse = 4.948381
Epoch 2
Loss = 2.2863e-01, PNorm = 9.0320, GNorm = 0.2043, lr_0 = 9.8189e-04
Validation rmse = 4.948417
Epoch 3
Loss = 7.0298e-01, PNorm = 9.0321, GNorm = 2.3684, lr_0 = 8.9615e-04
Validation rmse = 4.940724
Epoch 4
Loss = 1.2309e+00, PNorm = 9.0339, GNorm = 0.2641, lr_0 = 8.1790e-04
Validation rmse = 4.946078
Epoch 5
Loss = 1.2682e+00, PNorm = 9.0371, GNorm = 3.7808, lr_0 = 7.4648e-04
Validation rmse = 4.947253
Epoch 6
Loss = 1.3638e+00, PNorm = 9.0410, GNorm = 2.3347, lr_0 = 6.8129e-04
Validation rmse = 4.949060
Epoch 7
Loss = 1.1782e+00, PNorm = 9.0452, GNorm = 0.6589, lr_0 = 6.2180e-04
Validation rmse = 4.944122
Epoch 8
Loss = 1.1001e+00, PNorm = 9.0494, GNorm = 3.4896, lr_0 = 5.6750e-04
Validation rmse = 4.940785
Epoch 9
Loss = 9.8297e-01, PNorm = 9.0533, GNorm = 4.6583, lr_0 = 5.1795e-04
Validation rmse = 4.940641
Epoch 10
Validation rmse = 4.944216
Epoch 11
Loss = 2.4686e+00, PNorm = 9.0585, GNorm = 4.6553, lr_0 = 4.7272e-04
Validation rmse = 4.944667
Epoch 12
Loss = 1.5869e+00, PNorm = 9.0630, GNorm = 3.6722, lr_0 = 4.3144e-04
Validation rmse = 4.939687
Epoch 13
Loss = 4.8946e-01, PNorm = 9.0671, GNorm = 3.3276, lr_0 = 3.9377e-04
Validation rmse = 4.939935
Epoch 14
Loss = 1.2968e+00, PNorm = 9.0716, GNorm = 3.5054, lr_0 = 3.5938e-04
Validation rmse = 4.938460
Epoch 15
Loss = 1.2707e+00, PNorm = 9.0750, GNorm = 4.6768, lr_0 = 3.2800e-04
Validation rmse = 4.938753
Epoch 16
Loss = 9.9294e-01, PNorm = 9.0789, GNorm = 4.6764, lr_0 = 2.9936e-04
Validation rmse = 4.940359
Epoch 17
Loss = 9.7125e-01, PNorm = 9.0825, GNorm = 4.6782, lr_0 = 2.7322e-04
Validation rmse = 4.940837
Epoch 18
Loss = 9.2422e-01, PNorm = 9.0859, GNorm = 0.2724, lr_0 = 2.4936e-04
Validation rmse = 4.942459
Epoch 19
Loss = 9.7692e-01, PNorm = 9.0889, GNorm = 0.6595, lr_0 = 2.2758e-04
Validation rmse = 4.941989
Epoch 20
Validation rmse = 4.940785
Epoch 21
Loss = 5.8858e-01, PNorm = 9.0914, GNorm = 2.2943, lr_0 = 2.0771e-04
Validation rmse = 4.939849
Epoch 22
Loss = 1.9319e+00, PNorm = 9.0940, GNorm = 3.3439, lr_0 = 1.8957e-04
Validation rmse = 4.941796
Epoch 23
Loss = 3.8064e-01, PNorm = 9.0965, GNorm = 2.3102, lr_0 = 1.7302e-04
Validation rmse = 4.941427
Epoch 24
Loss = 1.1433e+00, PNorm = 9.0988, GNorm = 3.3476, lr_0 = 1.5791e-04
Validation rmse = 4.941842
Epoch 25
Loss = 1.4066e+00, PNorm = 9.1009, GNorm = 4.6927, lr_0 = 1.4412e-04
Validation rmse = 4.941140
Epoch 26
Loss = 1.2693e+00, PNorm = 9.1031, GNorm = 2.2914, lr_0 = 1.3154e-04
Validation rmse = 4.941716
Epoch 27
Loss = 1.1685e+00, PNorm = 9.1046, GNorm = 3.6982, lr_0 = 1.2005e-04
Validation rmse = 4.941128
Epoch 28
Loss = 9.2130e-01, PNorm = 9.1061, GNorm = 3.6989, lr_0 = 1.0957e-04
Validation rmse = 4.940701
Epoch 29
Loss = 9.7434e-01, PNorm = 9.1076, GNorm = 0.6607, lr_0 = 1.0000e-04
Validation rmse = 4.941446
Model 0 best validation rmse = 4.938460 on epoch 14
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.915385
Ensemble test rmse = 1.915385
1-fold cross validation
Seed 0 ==> test rmse = 1.915385
Overall test rmse = 1.915385 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Validation rmse = 4.691850
Epoch 1
Loss = 4.9659e-01, PNorm = 9.1941, GNorm = 1.7984, lr_0 = 6.0000e-04
Validation rmse = 4.709302
Epoch 2
Loss = 2.4322e-01, PNorm = 9.1961, GNorm = 0.0730, lr_0 = 9.8189e-04
Validation rmse = 4.714264
Epoch 3
Loss = 6.4358e-01, PNorm = 9.1989, GNorm = 2.5179, lr_0 = 8.9615e-04
Validation rmse = 4.712648
Epoch 4
Loss = 1.0873e+00, PNorm = 9.2020, GNorm = 0.0586, lr_0 = 8.1790e-04
Validation rmse = 4.721918
Epoch 5
Loss = 1.1109e+00, PNorm = 9.2054, GNorm = 3.4267, lr_0 = 7.4648e-04
Validation rmse = 4.726695
Epoch 6
Loss = 1.1883e+00, PNorm = 9.2099, GNorm = 2.5435, lr_0 = 6.8129e-04
Validation rmse = 4.734007
Epoch 7
Loss = 1.0007e+00, PNorm = 9.2140, GNorm = 0.4309, lr_0 = 6.2180e-04
Validation rmse = 4.734361
Epoch 8
Loss = 9.4847e-01, PNorm = 9.2174, GNorm = 3.3656, lr_0 = 5.6750e-04
Validation rmse = 4.732001
Epoch 9
Loss = 8.3725e-01, PNorm = 9.2210, GNorm = 4.4108, lr_0 = 5.1795e-04
Validation rmse = 4.734088
Epoch 10
Validation rmse = 4.745223
Epoch 11
Loss = 2.2138e+00, PNorm = 9.2249, GNorm = 4.0748, lr_0 = 4.7272e-04
Validation rmse = 4.748661
Epoch 12
Loss = 1.1457e+00, PNorm = 9.2286, GNorm = 3.7945, lr_0 = 4.3144e-04
Validation rmse = 4.746058
Epoch 13
Loss = 3.6690e-01, PNorm = 9.2318, GNorm = 3.1961, lr_0 = 3.9377e-04
Validation rmse = 4.747887
Epoch 14
Loss = 9.9321e-01, PNorm = 9.2354, GNorm = 3.3381, lr_0 = 3.5938e-04
Validation rmse = 4.747801
Epoch 15
Loss = 1.0720e+00, PNorm = 9.2389, GNorm = 4.4680, lr_0 = 3.2800e-04
Validation rmse = 4.750893
Epoch 16
Loss = 8.4242e-01, PNorm = 9.2421, GNorm = 4.0865, lr_0 = 2.9936e-04
Validation rmse = 4.754319
Epoch 17
Loss = 7.7368e-01, PNorm = 9.2450, GNorm = 4.0874, lr_0 = 2.7322e-04
Validation rmse = 4.755738
Epoch 18
Loss = 7.5396e-01, PNorm = 9.2478, GNorm = 0.5546, lr_0 = 2.4936e-04
Validation rmse = 4.759610
Epoch 19
Loss = 7.8081e-01, PNorm = 9.2503, GNorm = 0.1373, lr_0 = 2.2758e-04
Validation rmse = 4.759984
Epoch 20
Validation rmse = 4.760471
Epoch 21
Loss = 6.3651e-01, PNorm = 9.2526, GNorm = 2.5526, lr_0 = 2.0771e-04
Validation rmse = 4.760503
Epoch 22
Loss = 1.5695e+00, PNorm = 9.2549, GNorm = 3.1262, lr_0 = 1.8957e-04
Validation rmse = 4.763640
Epoch 23
Loss = 3.4841e-01, PNorm = 9.2570, GNorm = 1.6749, lr_0 = 1.7302e-04
Validation rmse = 4.757903
Epoch 24
Loss = 7.6574e-01, PNorm = 9.2589, GNorm = 3.1142, lr_0 = 1.5791e-04
Validation rmse = 4.753326
Epoch 25
Loss = 1.0337e+00, PNorm = 9.2608, GNorm = 4.4831, lr_0 = 1.4412e-04
Validation rmse = 4.748415
Epoch 26
Loss = 9.6330e-01, PNorm = 9.2625, GNorm = 2.5573, lr_0 = 1.3154e-04
Validation rmse = 4.746153
Epoch 27
Loss = 8.8195e-01, PNorm = 9.2639, GNorm = 4.8531, lr_0 = 1.2005e-04
Validation rmse = 4.742174
Epoch 28
Loss = 7.2912e-01, PNorm = 9.2652, GNorm = 4.8533, lr_0 = 1.0957e-04
Validation rmse = 4.738909
Epoch 29
Loss = 7.5384e-01, PNorm = 9.2666, GNorm = 0.0377, lr_0 = 1.0000e-04
Validation rmse = 4.738975
Model 0 best validation rmse = 4.691850 on epoch 0
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.894765
Ensemble test rmse = 1.894765
1-fold cross validation
Seed 0 ==> test rmse = 1.894765
Overall test rmse = 1.894765 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Loss = 1.9973e-02, PNorm = 26.0245, GNorm = 0.5498, lr_0 = 1.4737e-04
Loss = 2.4820e-02, PNorm = 26.0250, GNorm = 0.3473, lr_0 = 1.9474e-04
Loss = 1.4829e-02, PNorm = 26.0264, GNorm = 0.7512, lr_0 = 2.4211e-04
Loss = 2.1266e-02, PNorm = 26.0280, GNorm = 0.4389, lr_0 = 2.8947e-04
Loss = 1.7239e-02, PNorm = 26.0323, GNorm = 0.2609, lr_0 = 3.3684e-04
Loss = 1.8798e-02, PNorm = 26.0450, GNorm = 0.2425, lr_0 = 3.8421e-04
Loss = 1.7370e-02, PNorm = 26.0626, GNorm = 0.5198, lr_0 = 4.3158e-04
Loss = 1.8559e-02, PNorm = 26.0874, GNorm = 0.6305, lr_0 = 4.7895e-04
Loss = 1.9600e-02, PNorm = 26.1313, GNorm = 0.9057, lr_0 = 5.2632e-04
Validation rmse = 2.097751
Epoch 1
Loss = 1.7688e-02, PNorm = 26.1965, GNorm = 0.9558, lr_0 = 5.7368e-04
Loss = 1.5503e-02, PNorm = 26.2793, GNorm = 0.5152, lr_0 = 6.2105e-04
Loss = 1.5748e-02, PNorm = 26.3335, GNorm = 1.1949, lr_0 = 6.6842e-04
Loss = 1.5130e-02, PNorm = 26.3915, GNorm = 0.4046, lr_0 = 7.1579e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Loss = 1.9814e-02, PNorm = 25.9372, GNorm = 0.5870, lr_0 = 1.4737e-04
Loss = 2.4461e-02, PNorm = 25.9399, GNorm = 0.3983, lr_0 = 1.9474e-04
Loss = 1.4577e-02, PNorm = 25.9464, GNorm = 0.8535, lr_0 = 2.4211e-04
Loss = 2.0451e-02, PNorm = 25.9608, GNorm = 0.4120, lr_0 = 2.8947e-04
Loss = 1.6462e-02, PNorm = 25.9832, GNorm = 0.2991, lr_0 = 3.3684e-04
Loss = 1.6787e-02, PNorm = 26.0143, GNorm = 0.2066, lr_0 = 3.8421e-04
Loss = 1.4205e-02, PNorm = 26.0457, GNorm = 0.6126, lr_0 = 4.3158e-04
Loss = 1.8011e-02, PNorm = 26.0635, GNorm = 0.6597, lr_0 = 4.7895e-04
Loss = 1.8194e-02, PNorm = 26.0831, GNorm = 0.9493, lr_0 = 5.2632e-04
Validation rmse = 2.062715
Epoch 1
Loss = 1.6930e-02, PNorm = 26.1132, GNorm = 0.8020, lr_0 = 5.7368e-04
Loss = 1.5192e-02, PNorm = 26.1554, GNorm = 0.6690, lr_0 = 6.2105e-04
Loss = 1.6147e-02, PNorm = 26.1797, GNorm = 0.7788, lr_0 = 6.6842e-04
Loss = 1.5181e-02, PNorm = 26.2213, GNorm = 0.2743, lr_0 = 7.1579e-04
Loss = 1.5075e-02, PNorm = 26.2462, GNorm = 0.8987, lr_0 = 7.6316e-04
Loss = 1.2123e-02, PNorm = 26.2967, GNorm = 0.3009, lr_0 = 8.1053e-04
Loss = 1.1829e-02, PNorm = 26.3769, GNorm = 0.3367, lr_0 = 8.5789e-04
Loss = 1.2387e-02, PNorm = 26.4628, GNorm = 1.1226, lr_0 = 9.0526e-04
Loss = 1.3225e-02, PNorm = 26.5967, GNorm = 0.3127, lr_0 = 9.5263e-04
Loss = 1.4832e-02, PNorm = 26.7879, GNorm = 0.8721, lr_0 = 1.0000e-03
Validation rmse = 1.830112
Epoch 2
Loss = 1.3193e-02, PNorm = 26.9348, GNorm = 1.6477, lr_0 = 9.9138e-04
Loss = 1.2134e-02, PNorm = 27.0575, GNorm = 0.6675, lr_0 = 9.8284e-04
Loss = 1.1661e-02, PNorm = 27.1718, GNorm = 0.6414, lr_0 = 9.7437e-04
Loss = 1.1025e-02, PNorm = 27.2695, GNorm = 1.1218, lr_0 = 9.6597e-04
Loss = 1.0889e-02, PNorm = 27.3739, GNorm = 1.3938, lr_0 = 9.5764e-04
Loss = 1.0515e-02, PNorm = 27.4050, GNorm = 1.0337, lr_0 = 9.4939e-04
Loss = 9.9765e-03, PNorm = 27.4788, GNorm = 1.9748, lr_0 = 9.4120e-04
Loss = 9.9095e-03, PNorm = 27.6711, GNorm = 0.4921, lr_0 = 9.3309e-04
Loss = 9.8918e-03, PNorm = 27.8337, GNorm = 0.3958, lr_0 = 9.2505e-04
Validation rmse = 1.585058
Epoch 3
Loss = 1.0003e-02, PNorm = 27.9704, GNorm = 1.0524, lr_0 = 9.1708e-04
Loss = 1.2123e-02, PNorm = 28.0642, GNorm = 1.2285, lr_0 = 9.0917e-04
Loss = 1.0422e-02, PNorm = 28.0911, GNorm = 0.9760, lr_0 = 9.0134e-04
Loss = 8.3217e-03, PNorm = 28.1713, GNorm = 0.4535, lr_0 = 8.9357e-04
Loss = 7.9329e-03, PNorm = 28.2284, GNorm = 0.9307, lr_0 = 8.8587e-04
Loss = 8.8861e-03, PNorm = 28.2991, GNorm = 0.4882, lr_0 = 8.7823e-04
Loss = 1.0734e-02, PNorm = 28.3893, GNorm = 1.1190, lr_0 = 8.7066e-04
Loss = 1.0994e-02, PNorm = 28.5114, GNorm = 1.0340, lr_0 = 8.6316e-04
Loss = 9.1352e-03, PNorm = 28.6055, GNorm = 0.8934, lr_0 = 8.5572e-04
Loss = 1.0934e-02, PNorm = 28.7093, GNorm = 1.2405, lr_0 = 8.4834e-04
Validation rmse = 1.600762
Epoch 4
Loss = 9.3864e-03, PNorm = 28.7991, GNorm = 0.6288, lr_0 = 8.4103e-04
Loss = 8.0977e-03, PNorm = 28.8550, GNorm = 0.4431, lr_0 = 8.3378e-04
Loss = 8.8774e-03, PNorm = 28.9265, GNorm = 0.7877, lr_0 = 8.2660e-04
Loss = 9.6858e-03, PNorm = 29.0897, GNorm = 0.9075, lr_0 = 8.1947e-04
Loss = 8.4713e-03, PNorm = 29.1695, GNorm = 0.4099, lr_0 = 8.1241e-04
Loss = 9.2590e-03, PNorm = 29.2271, GNorm = 0.5368, lr_0 = 8.0541e-04
Loss = 1.0972e-02, PNorm = 29.2462, GNorm = 1.3215, lr_0 = 7.9846e-04
Loss = 8.4910e-03, PNorm = 29.2924, GNorm = 0.8585, lr_0 = 7.9158e-04
Loss = 9.1544e-03, PNorm = 29.3187, GNorm = 0.7355, lr_0 = 7.8476e-04
Validation rmse = 1.488137
Epoch 5
Loss = 8.5536e-03, PNorm = 29.4029, GNorm = 0.5278, lr_0 = 7.7800e-04
Loss = 6.9570e-03, PNorm = 29.4430, GNorm = 0.5943, lr_0 = 7.7129e-04
Loss = 1.0042e-02, PNorm = 29.5024, GNorm = 1.5150, lr_0 = 7.6464e-04
Loss = 8.7301e-03, PNorm = 29.6138, GNorm = 6.8857, lr_0 = 7.5805e-04
Loss = 9.2434e-03, PNorm = 29.6609, GNorm = 1.7372, lr_0 = 7.5152e-04
Loss = 9.0702e-03, PNorm = 29.7217, GNorm = 1.0760, lr_0 = 7.4504e-04
Loss = 8.5544e-03, PNorm = 29.7547, GNorm = 1.4243, lr_0 = 7.3862e-04
Loss = 9.7054e-03, PNorm = 29.8487, GNorm = 1.1484, lr_0 = 7.3225e-04
Loss = 8.4596e-03, PNorm = 29.9240, GNorm = 0.9430, lr_0 = 7.2594e-04
Loss = 8.7431e-03, PNorm = 29.9054, GNorm = 0.4756, lr_0 = 7.1969e-04
Validation rmse = 1.460406
Epoch 6
Loss = 7.5713e-03, PNorm = 29.8968, GNorm = 0.5915, lr_0 = 7.1348e-04
Loss = 8.5194e-03, PNorm = 29.9672, GNorm = 3.0942, lr_0 = 7.0733e-04
Loss = 8.2981e-03, PNorm = 30.0462, GNorm = 1.4995, lr_0 = 7.0124e-04
Loss = 9.1918e-03, PNorm = 30.0705, GNorm = 1.1415, lr_0 = 6.9519e-04
Loss = 1.0084e-02, PNorm = 30.1241, GNorm = 1.4269, lr_0 = 6.8920e-04
Loss = 9.2459e-03, PNorm = 30.1837, GNorm = 0.6236, lr_0 = 6.8326e-04
Loss = 7.2900e-03, PNorm = 30.2305, GNorm = 0.7266, lr_0 = 6.7737e-04
Loss = 9.3008e-03, PNorm = 30.2508, GNorm = 0.8172, lr_0 = 6.7153e-04
Loss = 9.1011e-03, PNorm = 30.2617, GNorm = 0.6995, lr_0 = 6.6575e-04
Validation rmse = 1.451211
Epoch 7
Loss = 8.5593e-03, PNorm = 30.3046, GNorm = 1.3737, lr_0 = 6.6001e-04
Loss = 8.8186e-03, PNorm = 30.3591, GNorm = 0.8001, lr_0 = 6.5432e-04
Loss = 8.2732e-03, PNorm = 30.3940, GNorm = 1.2912, lr_0 = 6.4868e-04
Loss = 9.4098e-03, PNorm = 30.4801, GNorm = 0.4842, lr_0 = 6.4309e-04
Loss = 7.7905e-03, PNorm = 30.5410, GNorm = 0.9319, lr_0 = 6.3755e-04
Loss = 8.3821e-03, PNorm = 30.5512, GNorm = 1.1300, lr_0 = 6.3205e-04
Loss = 9.1590e-03, PNorm = 30.5735, GNorm = 1.4525, lr_0 = 6.2660e-04
Loss = 7.8260e-03, PNorm = 30.6232, GNorm = 0.8342, lr_0 = 6.2120e-04
Loss = 8.3402e-03, PNorm = 30.6900, GNorm = 0.5896, lr_0 = 6.1585e-04
Loss = 7.7756e-03, PNorm = 30.7095, GNorm = 1.0041, lr_0 = 6.1054e-04
Validation rmse = 1.442376
Epoch 8
Loss = 7.9028e-03, PNorm = 30.7099, GNorm = 0.4219, lr_0 = 6.0528e-04
Loss = 7.9028e-03, PNorm = 30.7125, GNorm = 1.3521, lr_0 = 6.0006e-04
Loss = 7.0347e-03, PNorm = 30.7523, GNorm = 0.7921, lr_0 = 5.9489e-04
Loss = 9.4024e-03, PNorm = 30.8278, GNorm = 1.0055, lr_0 = 5.8976e-04
Loss = 9.1559e-03, PNorm = 30.8748, GNorm = 0.7014, lr_0 = 5.8468e-04
Loss = 9.0123e-03, PNorm = 30.8872, GNorm = 1.1674, lr_0 = 5.7964e-04
Loss = 6.9770e-03, PNorm = 30.9021, GNorm = 0.6545, lr_0 = 5.7464e-04
Loss = 9.5327e-03, PNorm = 30.9254, GNorm = 2.1829, lr_0 = 5.6969e-04
Loss = 7.1193e-03, PNorm = 30.9899, GNorm = 0.4465, lr_0 = 5.6478e-04
Validation rmse = 1.416410
Epoch 9
Loss = 8.3547e-03, PNorm = 31.0399, GNorm = 0.5970, lr_0 = 5.5991e-04
Loss = 8.1312e-03, PNorm = 31.0498, GNorm = 1.0761, lr_0 = 5.5509e-04
Loss = 8.0359e-03, PNorm = 31.0395, GNorm = 0.8453, lr_0 = 5.5030e-04
Loss = 7.8808e-03, PNorm = 31.0693, GNorm = 0.5929, lr_0 = 5.4556e-04
Loss = 8.4075e-03, PNorm = 31.0944, GNorm = 1.9565, lr_0 = 5.4086e-04
Loss = 7.3369e-03, PNorm = 31.1589, GNorm = 0.5774, lr_0 = 5.3620e-04
Loss = 8.8945e-03, PNorm = 31.2130, GNorm = 0.9777, lr_0 = 5.3157e-04
Loss = 7.5459e-03, PNorm = 31.2084, GNorm = 0.7435, lr_0 = 5.2699e-04
Loss = 8.2153e-03, PNorm = 31.1845, GNorm = 0.8047, lr_0 = 5.2245e-04
Loss = 8.6297e-03, PNorm = 31.2264, GNorm = 0.7084, lr_0 = 5.1795e-04
Validation rmse = 1.404230
Epoch 10
Loss = 8.0525e-03, PNorm = 31.2681, GNorm = 0.6878, lr_0 = 5.1348e-04
Loss = 8.1570e-03, PNorm = 31.3311, GNorm = 0.9526, lr_0 = 5.0906e-04
Loss = 8.2290e-03, PNorm = 31.3771, GNorm = 0.6963, lr_0 = 5.0467e-04
Loss = 6.7379e-03, PNorm = 31.3884, GNorm = 0.4092, lr_0 = 5.0032e-04
Loss = 9.3732e-03, PNorm = 31.3859, GNorm = 0.9474, lr_0 = 4.9601e-04
Loss = 8.2850e-03, PNorm = 31.4333, GNorm = 0.7812, lr_0 = 4.9173e-04
Loss = 7.6367e-03, PNorm = 31.4795, GNorm = 1.3702, lr_0 = 4.8749e-04
Loss = 7.7068e-03, PNorm = 31.5316, GNorm = 1.1051, lr_0 = 4.8329e-04
Loss = 7.1272e-03, PNorm = 31.5530, GNorm = 0.8186, lr_0 = 4.7913e-04
Validation rmse = 1.385747
Epoch 11
Loss = 5.8269e-03, PNorm = 31.5575, GNorm = 1.3784, lr_0 = 4.7500e-04
Loss = 8.1516e-03, PNorm = 31.5742, GNorm = 1.1500, lr_0 = 4.7090e-04
Loss = 7.5527e-03, PNorm = 31.5744, GNorm = 1.4130, lr_0 = 4.6685e-04
Loss = 8.2792e-03, PNorm = 31.6083, GNorm = 2.8950, lr_0 = 4.6282e-04
Loss = 7.5353e-03, PNorm = 31.6244, GNorm = 0.5464, lr_0 = 4.5883e-04
Loss = 8.4951e-03, PNorm = 31.6668, GNorm = 0.3307, lr_0 = 4.5488e-04
Loss = 8.8939e-03, PNorm = 31.7050, GNorm = 1.4834, lr_0 = 4.5096e-04
Loss = 7.4542e-03, PNorm = 31.7289, GNorm = 0.4048, lr_0 = 4.4707e-04
Loss = 9.5201e-03, PNorm = 31.7506, GNorm = 1.6571, lr_0 = 4.4322e-04
Loss = 7.0924e-03, PNorm = 31.7602, GNorm = 0.8435, lr_0 = 4.3940e-04
Validation rmse = 1.371585
Epoch 12
Loss = 7.3906e-03, PNorm = 31.7856, GNorm = 0.4489, lr_0 = 4.3561e-04
Loss = 7.3923e-03, PNorm = 31.8113, GNorm = 0.7916, lr_0 = 4.3186e-04
Loss = 7.9270e-03, PNorm = 31.8592, GNorm = 0.8793, lr_0 = 4.2813e-04
Loss = 7.9212e-03, PNorm = 31.8958, GNorm = 0.6387, lr_0 = 4.2444e-04
Loss = 8.0795e-03, PNorm = 31.8997, GNorm = 0.5958, lr_0 = 4.2078e-04
Loss = 7.9268e-03, PNorm = 31.8974, GNorm = 0.5420, lr_0 = 4.1716e-04
Loss = 7.2427e-03, PNorm = 31.9121, GNorm = 0.5450, lr_0 = 4.1356e-04
Loss = 7.6150e-03, PNorm = 31.9437, GNorm = 0.5701, lr_0 = 4.1000e-04
Loss = 7.9227e-03, PNorm = 31.9648, GNorm = 0.4866, lr_0 = 4.0646e-04
Validation rmse = 1.387041
Epoch 13
Loss = 7.4240e-03, PNorm = 31.9944, GNorm = 0.9728, lr_0 = 4.0296e-04
Loss = 6.3621e-03, PNorm = 32.0031, GNorm = 0.5958, lr_0 = 3.9949e-04
Loss = 6.9667e-03, PNorm = 31.9977, GNorm = 0.4662, lr_0 = 3.9604e-04
Loss = 7.8235e-03, PNorm = 32.0032, GNorm = 2.0925, lr_0 = 3.9263e-04
Loss = 8.3670e-03, PNorm = 32.0714, GNorm = 1.0832, lr_0 = 3.8925e-04
Loss = 8.3495e-03, PNorm = 32.1147, GNorm = 0.6941, lr_0 = 3.8589e-04
Loss = 7.8894e-03, PNorm = 32.1337, GNorm = 0.6419, lr_0 = 3.8257e-04
Loss = 8.1203e-03, PNorm = 32.1641, GNorm = 0.4644, lr_0 = 3.7927e-04
Loss = 8.3205e-03, PNorm = 32.1911, GNorm = 0.4368, lr_0 = 3.7600e-04
Loss = 8.4238e-03, PNorm = 32.1886, GNorm = 1.6171, lr_0 = 3.7276e-04
Validation rmse = 1.371327
Epoch 14
Loss = 6.3948e-03, PNorm = 32.1763, GNorm = 0.4679, lr_0 = 3.6955e-04
Loss = 9.1962e-03, PNorm = 32.1948, GNorm = 1.7531, lr_0 = 3.6636e-04
Loss = 7.5714e-03, PNorm = 32.2035, GNorm = 0.9050, lr_0 = 3.6320e-04
Loss = 7.3749e-03, PNorm = 32.2310, GNorm = 1.4627, lr_0 = 3.6007e-04
Loss = 7.6700e-03, PNorm = 32.2441, GNorm = 0.6194, lr_0 = 3.5697e-04
Loss = 7.6739e-03, PNorm = 32.2629, GNorm = 0.5385, lr_0 = 3.5389e-04
Loss = 8.1116e-03, PNorm = 32.2965, GNorm = 2.1821, lr_0 = 3.5084e-04
Loss = 1.0436e-02, PNorm = 32.3025, GNorm = 1.5841, lr_0 = 3.4782e-04
Loss = 9.0159e-03, PNorm = 32.3154, GNorm = 1.4684, lr_0 = 3.4482e-04
Validation rmse = 1.390970
Epoch 15
Loss = 1.1087e-02, PNorm = 32.3355, GNorm = 0.5313, lr_0 = 3.4185e-04
Loss = 7.3299e-03, PNorm = 32.3557, GNorm = 0.4827, lr_0 = 3.3890e-04
Loss = 7.8843e-03, PNorm = 32.3592, GNorm = 0.4608, lr_0 = 3.3598e-04
Loss = 8.4079e-03, PNorm = 32.3724, GNorm = 1.3660, lr_0 = 3.3309e-04
Loss = 8.1912e-03, PNorm = 32.3915, GNorm = 0.7379, lr_0 = 3.3022e-04
Loss = 7.8165e-03, PNorm = 32.4059, GNorm = 1.9902, lr_0 = 3.2737e-04
Loss = 6.8621e-03, PNorm = 32.4245, GNorm = 1.0565, lr_0 = 3.2455e-04
Loss = 7.3497e-03, PNorm = 32.4466, GNorm = 0.8695, lr_0 = 3.2175e-04
Loss = 7.1465e-03, PNorm = 32.4753, GNorm = 0.8355, lr_0 = 3.1898e-04
Loss = 6.0294e-03, PNorm = 32.4910, GNorm = 0.7139, lr_0 = 3.1623e-04
Validation rmse = 1.358123
Epoch 16
Loss = 8.1848e-03, PNorm = 32.5121, GNorm = 1.4270, lr_0 = 3.1350e-04
Loss = 8.5013e-03, PNorm = 32.5252, GNorm = 0.6533, lr_0 = 3.1080e-04
Loss = 8.0316e-03, PNorm = 32.5487, GNorm = 1.2576, lr_0 = 3.0812e-04
Loss = 8.0716e-03, PNorm = 32.5777, GNorm = 0.5249, lr_0 = 3.0547e-04
Loss = 7.5591e-03, PNorm = 32.5919, GNorm = 0.9351, lr_0 = 3.0283e-04
Loss = 6.9874e-03, PNorm = 32.6018, GNorm = 1.2218, lr_0 = 3.0022e-04
Loss = 7.4906e-03, PNorm = 32.6235, GNorm = 0.8472, lr_0 = 2.9764e-04
Loss = 6.2082e-03, PNorm = 32.6389, GNorm = 0.5032, lr_0 = 2.9507e-04
Loss = 8.1197e-03, PNorm = 32.6330, GNorm = 0.8334, lr_0 = 2.9253e-04
Validation rmse = 1.354335
Epoch 17
Loss = 6.5538e-03, PNorm = 32.6273, GNorm = 0.4718, lr_0 = 2.9001e-04
Loss = 8.0765e-03, PNorm = 32.6505, GNorm = 0.5666, lr_0 = 2.8751e-04
Loss = 6.9517e-03, PNorm = 32.6650, GNorm = 1.4796, lr_0 = 2.8503e-04
Loss = 7.2943e-03, PNorm = 32.6879, GNorm = 0.9496, lr_0 = 2.8257e-04
Loss = 6.7857e-03, PNorm = 32.7096, GNorm = 0.6308, lr_0 = 2.8014e-04
Loss = 7.5297e-03, PNorm = 32.7250, GNorm = 1.1842, lr_0 = 2.7772e-04
Loss = 8.9163e-03, PNorm = 32.7385, GNorm = 0.6590, lr_0 = 2.7533e-04
Loss = 6.6508e-03, PNorm = 32.7521, GNorm = 0.8165, lr_0 = 2.7295e-04
Loss = 8.5118e-03, PNorm = 32.7626, GNorm = 1.2922, lr_0 = 2.7060e-04
Loss = 7.6814e-03, PNorm = 32.7739, GNorm = 0.9275, lr_0 = 2.6827e-04
Validation rmse = 1.353117
Epoch 18
Loss = 7.3382e-03, PNorm = 32.7695, GNorm = 0.6210, lr_0 = 2.6596e-04
Loss = 6.7952e-03, PNorm = 32.7726, GNorm = 0.7224, lr_0 = 2.6367e-04
Loss = 8.7760e-03, PNorm = 32.7792, GNorm = 2.0903, lr_0 = 2.6139e-04
Loss = 7.6792e-03, PNorm = 32.8084, GNorm = 0.4806, lr_0 = 2.5914e-04
Loss = 7.4101e-03, PNorm = 32.8211, GNorm = 0.4401, lr_0 = 2.5691e-04
Loss = 7.5823e-03, PNorm = 32.8376, GNorm = 0.6709, lr_0 = 2.5469e-04
Loss = 8.0504e-03, PNorm = 32.8598, GNorm = 0.9469, lr_0 = 2.5250e-04
Loss = 7.7653e-03, PNorm = 32.8935, GNorm = 1.2432, lr_0 = 2.5032e-04
Loss = 7.4171e-03, PNorm = 32.9198, GNorm = 0.9428, lr_0 = 2.4816e-04
Validation rmse = 1.371092
Epoch 19
Loss = 1.1256e-02, PNorm = 32.9154, GNorm = 2.4926, lr_0 = 2.4602e-04
Loss = 7.2791e-03, PNorm = 32.9080, GNorm = 1.0771, lr_0 = 2.4390e-04
Loss = 6.9979e-03, PNorm = 32.9115, GNorm = 1.0305, lr_0 = 2.4180e-04
Loss = 8.9108e-03, PNorm = 32.9245, GNorm = 2.3968, lr_0 = 2.3972e-04
Loss = 7.1388e-03, PNorm = 32.9419, GNorm = 1.0272, lr_0 = 2.3765e-04
Loss = 6.8909e-03, PNorm = 32.9428, GNorm = 1.3603, lr_0 = 2.3560e-04
Loss = 7.4368e-03, PNorm = 32.9553, GNorm = 1.5932, lr_0 = 2.3357e-04
Loss = 7.5809e-03, PNorm = 32.9669, GNorm = 0.6084, lr_0 = 2.3156e-04
Loss = 7.3580e-03, PNorm = 32.9686, GNorm = 0.4812, lr_0 = 2.2956e-04
Loss = 8.8864e-03, PNorm = 32.9966, GNorm = 0.6663, lr_0 = 2.2758e-04
Validation rmse = 1.360880
Epoch 20
Loss = 8.7171e-03, PNorm = 33.0385, GNorm = 0.9051, lr_0 = 2.2562e-04
Loss = 6.5832e-03, PNorm = 33.0541, GNorm = 0.8382, lr_0 = 2.2368e-04
Loss = 8.2330e-03, PNorm = 33.0547, GNorm = 1.1697, lr_0 = 2.2175e-04
Loss = 9.1211e-03, PNorm = 33.0757, GNorm = 0.7027, lr_0 = 2.1984e-04
Loss = 7.6156e-03, PNorm = 33.0901, GNorm = 1.1627, lr_0 = 2.1794e-04
Loss = 7.4703e-03, PNorm = 33.0987, GNorm = 1.1811, lr_0 = 2.1607e-04
Loss = 6.6649e-03, PNorm = 33.0936, GNorm = 1.2316, lr_0 = 2.1420e-04
Loss = 6.7401e-03, PNorm = 33.0913, GNorm = 0.8973, lr_0 = 2.1236e-04
Loss = 7.5619e-03, PNorm = 33.1070, GNorm = 3.3011, lr_0 = 2.1053e-04
Validation rmse = 1.340004
Epoch 21
Loss = 9.0156e-03, PNorm = 33.1149, GNorm = 1.4693, lr_0 = 2.0871e-04
Loss = 7.0021e-03, PNorm = 33.1261, GNorm = 0.9692, lr_0 = 2.0691e-04
Loss = 7.2164e-03, PNorm = 33.1348, GNorm = 0.7908, lr_0 = 2.0513e-04
Loss = 7.4416e-03, PNorm = 33.1457, GNorm = 0.6405, lr_0 = 2.0336e-04
Loss = 8.6025e-03, PNorm = 33.1562, GNorm = 0.5601, lr_0 = 2.0161e-04
Loss = 8.9176e-03, PNorm = 33.1708, GNorm = 0.8665, lr_0 = 1.9987e-04
Loss = 6.4396e-03, PNorm = 33.1757, GNorm = 0.8032, lr_0 = 1.9815e-04
Loss = 8.4282e-03, PNorm = 33.1788, GNorm = 0.8735, lr_0 = 1.9644e-04
Loss = 7.7180e-03, PNorm = 33.1911, GNorm = 1.3541, lr_0 = 1.9475e-04
Loss = 6.9115e-03, PNorm = 33.2115, GNorm = 1.1179, lr_0 = 1.9307e-04
Validation rmse = 1.346879
Epoch 22
Loss = 6.6695e-03, PNorm = 33.2288, GNorm = 1.0271, lr_0 = 1.9141e-04
Loss = 7.4105e-03, PNorm = 33.2265, GNorm = 0.7989, lr_0 = 1.8976e-04
Loss = 7.2803e-03, PNorm = 33.2402, GNorm = 1.2197, lr_0 = 1.8812e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
      (att_3): Softmax(dim=0)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Loss = 1.9886e-02, PNorm = 26.0327, GNorm = 0.5506, lr_0 = 1.4737e-04
Loss = 2.4765e-02, PNorm = 26.0341, GNorm = 0.2901, lr_0 = 1.9474e-04
Loss = 1.4889e-02, PNorm = 26.0381, GNorm = 0.7050, lr_0 = 2.4211e-04
Loss = 2.1404e-02, PNorm = 26.0471, GNorm = 0.2109, lr_0 = 2.8947e-04
Loss = 1.7394e-02, PNorm = 26.0633, GNorm = 0.2840, lr_0 = 3.3684e-04
Loss = 1.9153e-02, PNorm = 26.0875, GNorm = 0.1932, lr_0 = 3.8421e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Loss = 2.0047e-02, PNorm = 26.1297, GNorm = 1.3374, lr_0 = 1.4737e-04
Loss = 2.4871e-02, PNorm = 26.1296, GNorm = 1.0859, lr_0 = 1.9474e-04
Loss = 1.4702e-02, PNorm = 26.1306, GNorm = 0.9268, lr_0 = 2.4211e-04
Loss = 1.8985e-02, PNorm = 26.1345, GNorm = 2.1692, lr_0 = 2.8947e-04
Loss = 1.4607e-02, PNorm = 26.1415, GNorm = 1.0026, lr_0 = 3.3684e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (act_func): Sigmoid()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Loss = 2.0139e-02, PNorm = 26.0294, GNorm = 1.4651, lr_0 = 1.4737e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (act_func): Sigmoid()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Loss = 1.9625e-02, PNorm = 26.0682, GNorm = 1.1513, lr_0 = 1.4737e-04
Loss = 2.4471e-02, PNorm = 26.0693, GNorm = 0.5469, lr_0 = 1.9474e-04
Loss = 1.4221e-02, PNorm = 26.0720, GNorm = 1.0276, lr_0 = 2.4211e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 7,
 'detailed_results': False,
 'dropout': 0.1,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 100,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 100,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.1, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=100, bias=False)
      (W_h): Linear(in_features=100, out_features=100, bias=False)
      (W_o): Linear(in_features=203, out_features=100, bias=True)
    )
    (attention): Attention(
      (act_func): Sigmoid()
      (att_1): Linear(in_features=200, out_features=200, bias=True)
      (att_2): Linear(in_features=200, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=200, out_features=100, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=100, out_features=1, bias=True)
  )
)
Number of parameters = 102,702
Epoch 0
Loss = 1.9876e-02, PNorm = 26.0841, GNorm = 2.0661, lr_0 = 1.4737e-04
Loss = 2.3375e-02, PNorm = 26.0842, GNorm = 1.1876, lr_0 = 1.9474e-04
Loss = 1.3485e-02, PNorm = 26.0861, GNorm = 2.0802, lr_0 = 2.4211e-04
Loss = 1.8816e-02, PNorm = 26.0907, GNorm = 1.5690, lr_0 = 2.8947e-04
Loss = 1.4182e-02, PNorm = 26.0993, GNorm = 0.8632, lr_0 = 3.3684e-04
Loss = 1.5091e-02, PNorm = 26.1095, GNorm = 0.9522, lr_0 = 3.8421e-04
Loss = 1.2181e-02, PNorm = 26.1218, GNorm = 1.0971, lr_0 = 4.3158e-04
Loss = 1.3930e-02, PNorm = 26.1393, GNorm = 1.0175, lr_0 = 4.7895e-04
Loss = 1.5501e-02, PNorm = 26.1574, GNorm = 1.7526, lr_0 = 5.2632e-04
Validation rmse = 1.768387
Epoch 1
Loss = 1.5039e-02, PNorm = 26.1805, GNorm = 1.1835, lr_0 = 5.7368e-04
Loss = 1.1326e-02, PNorm = 26.2009, GNorm = 1.2829, lr_0 = 6.2105e-04
Loss = 1.4805e-02, PNorm = 26.2240, GNorm = 0.9055, lr_0 = 6.6842e-04
Loss = 1.2694e-02, PNorm = 26.2483, GNorm = 1.3821, lr_0 = 7.1579e-04
Loss = 1.2014e-02, PNorm = 26.2803, GNorm = 0.8462, lr_0 = 7.6316e-04
Loss = 9.9896e-03, PNorm = 26.3091, GNorm = 1.4609, lr_0 = 8.1053e-04
Loss = 9.6456e-03, PNorm = 26.3311, GNorm = 0.9266, lr_0 = 8.5789e-04
Loss = 1.0722e-02, PNorm = 26.3628, GNorm = 1.9143, lr_0 = 9.0526e-04
Loss = 1.1531e-02, PNorm = 26.4035, GNorm = 1.0978, lr_0 = 9.5263e-04
Loss = 1.3248e-02, PNorm = 26.4392, GNorm = 1.4787, lr_0 = 1.0000e-03
Validation rmse = 1.558329
Epoch 2
Loss = 1.2370e-02, PNorm = 26.4781, GNorm = 3.2342, lr_0 = 9.9138e-04
Loss = 1.0266e-02, PNorm = 26.5087, GNorm = 0.7028, lr_0 = 9.8284e-04
Loss = 1.1255e-02, PNorm = 26.5431, GNorm = 1.1730, lr_0 = 9.7437e-04
Loss = 1.0538e-02, PNorm = 26.5774, GNorm = 2.0826, lr_0 = 9.6597e-04
Loss = 9.6908e-03, PNorm = 26.6049, GNorm = 1.7744, lr_0 = 9.5764e-04
Loss = 8.8193e-03, PNorm = 26.6301, GNorm = 0.6979, lr_0 = 9.4939e-04
Loss = 8.8585e-03, PNorm = 26.6593, GNorm = 1.4307, lr_0 = 9.4120e-04
Loss = 9.1854e-03, PNorm = 26.6844, GNorm = 0.9498, lr_0 = 9.3309e-04
Loss = 7.9795e-03, PNorm = 26.7166, GNorm = 0.9990, lr_0 = 9.2505e-04
Validation rmse = 1.427446
Epoch 3
Loss = 9.5457e-03, PNorm = 26.7491, GNorm = 0.5455, lr_0 = 9.1708e-04
Loss = 1.2402e-02, PNorm = 26.7890, GNorm = 0.9491, lr_0 = 9.0917e-04
Loss = 9.9234e-03, PNorm = 26.8256, GNorm = 1.2159, lr_0 = 9.0134e-04
Loss = 7.6438e-03, PNorm = 26.8589, GNorm = 0.9525, lr_0 = 8.9357e-04
Loss = 7.1731e-03, PNorm = 26.8807, GNorm = 1.2100, lr_0 = 8.8587e-04
Loss = 8.4253e-03, PNorm = 26.8982, GNorm = 0.6284, lr_0 = 8.7823e-04
Loss = 9.6680e-03, PNorm = 26.9243, GNorm = 0.8872, lr_0 = 8.7066e-04
Loss = 8.8089e-03, PNorm = 26.9632, GNorm = 0.7249, lr_0 = 8.6316e-04
Loss = 8.0832e-03, PNorm = 26.9897, GNorm = 0.6477, lr_0 = 8.5572e-04
Loss = 9.0516e-03, PNorm = 27.0140, GNorm = 1.3544, lr_0 = 8.4834e-04
Validation rmse = 1.295193
Epoch 4
Loss = 8.3440e-03, PNorm = 27.0417, GNorm = 0.8215, lr_0 = 8.4103e-04
Loss = 6.9758e-03, PNorm = 27.0675, GNorm = 0.6299, lr_0 = 8.3378e-04
Loss = 8.8948e-03, PNorm = 27.0994, GNorm = 1.2852, lr_0 = 8.2660e-04
Loss = 7.8974e-03, PNorm = 27.1373, GNorm = 1.0497, lr_0 = 8.1947e-04
Loss = 7.1753e-03, PNorm = 27.1554, GNorm = 0.8801, lr_0 = 8.1241e-04
Loss = 8.6434e-03, PNorm = 27.1738, GNorm = 1.1911, lr_0 = 8.0541e-04
Loss = 1.0074e-02, PNorm = 27.2099, GNorm = 1.2373, lr_0 = 7.9846e-04
Loss = 7.6260e-03, PNorm = 27.2486, GNorm = 1.2974, lr_0 = 7.9158e-04
Loss = 7.9093e-03, PNorm = 27.2747, GNorm = 0.9755, lr_0 = 7.8476e-04
Validation rmse = 1.262057
Epoch 5
Loss = 7.5514e-03, PNorm = 27.3075, GNorm = 1.1801, lr_0 = 7.7800e-04
Loss = 5.9353e-03, PNorm = 27.3317, GNorm = 1.0546, lr_0 = 7.7129e-04
Loss = 9.5015e-03, PNorm = 27.3535, GNorm = 2.0982, lr_0 = 7.6464e-04
Loss = 8.4017e-03, PNorm = 27.3768, GNorm = 1.2516, lr_0 = 7.5805e-04
Loss = 9.3754e-03, PNorm = 27.3993, GNorm = 1.7855, lr_0 = 7.5152e-04
Loss = 7.2670e-03, PNorm = 27.4155, GNorm = 1.5752, lr_0 = 7.4504e-04
Loss = 7.3663e-03, PNorm = 27.4312, GNorm = 1.4161, lr_0 = 7.3862e-04
Loss = 7.1767e-03, PNorm = 27.4526, GNorm = 0.5269, lr_0 = 7.3225e-04
Loss = 7.8830e-03, PNorm = 27.4780, GNorm = 0.7092, lr_0 = 7.2594e-04
Loss = 7.1989e-03, PNorm = 27.5052, GNorm = 0.5730, lr_0 = 7.1969e-04
Validation rmse = 1.296570
Epoch 6
Loss = 6.9593e-03, PNorm = 27.5227, GNorm = 0.8434, lr_0 = 7.1348e-04
Loss = 7.4489e-03, PNorm = 27.5490, GNorm = 1.0907, lr_0 = 7.0733e-04
Loss = 7.0707e-03, PNorm = 27.5773, GNorm = 1.0670, lr_0 = 7.0124e-04
Loss = 9.1571e-03, PNorm = 27.6055, GNorm = 1.7735, lr_0 = 6.9519e-04
Loss = 9.7242e-03, PNorm = 27.6319, GNorm = 1.2447, lr_0 = 6.8920e-04
Loss = 6.2043e-03, PNorm = 27.6498, GNorm = 1.2326, lr_0 = 6.8326e-04
Loss = 7.8456e-03, PNorm = 27.6716, GNorm = 2.7095, lr_0 = 6.7737e-04
Loss = 7.0614e-03, PNorm = 27.6929, GNorm = 1.2772, lr_0 = 6.7153e-04
Loss = 7.1786e-03, PNorm = 27.7158, GNorm = 1.2830, lr_0 = 6.6575e-04
Validation rmse = 1.269419
Epoch 7
Loss = 7.1011e-03, PNorm = 27.7354, GNorm = 1.8631, lr_0 = 6.6001e-04
Loss = 7.3906e-03, PNorm = 27.7588, GNorm = 1.3101, lr_0 = 6.5432e-04
Loss = 7.2493e-03, PNorm = 27.7769, GNorm = 2.4692, lr_0 = 6.4868e-04
Loss = 8.3064e-03, PNorm = 27.7997, GNorm = 0.5451, lr_0 = 6.4309e-04
Loss = 6.8281e-03, PNorm = 27.8213, GNorm = 0.6032, lr_0 = 6.3755e-04
Loss = 6.9405e-03, PNorm = 27.8380, GNorm = 1.1431, lr_0 = 6.3205e-04
Loss = 8.5189e-03, PNorm = 27.8559, GNorm = 3.4857, lr_0 = 6.2660e-04
Loss = 7.3781e-03, PNorm = 27.8795, GNorm = 0.6857, lr_0 = 6.2120e-04
Loss = 6.7443e-03, PNorm = 27.9063, GNorm = 2.7996, lr_0 = 6.1585e-04
Loss = 5.6319e-03, PNorm = 27.9238, GNorm = 0.7369, lr_0 = 6.1054e-04
Validation rmse = 1.188708
Epoch 8
Loss = 7.0750e-03, PNorm = 27.9439, GNorm = 0.7513, lr_0 = 6.0528e-04
Loss = 6.8900e-03, PNorm = 27.9601, GNorm = 0.8164, lr_0 = 6.0006e-04
Loss = 6.3191e-03, PNorm = 27.9758, GNorm = 1.6336, lr_0 = 5.9489e-04
Loss = 7.5927e-03, PNorm = 27.9935, GNorm = 1.1301, lr_0 = 5.8976e-04
Loss = 7.2042e-03, PNorm = 28.0100, GNorm = 0.5682, lr_0 = 5.8468e-04
Loss = 8.5771e-03, PNorm = 28.0251, GNorm = 2.6589, lr_0 = 5.7964e-04
Loss = 7.1180e-03, PNorm = 28.0405, GNorm = 0.9852, lr_0 = 5.7464e-04
Loss = 8.5851e-03, PNorm = 28.0549, GNorm = 2.3951, lr_0 = 5.6969e-04
Loss = 6.5744e-03, PNorm = 28.0706, GNorm = 1.0456, lr_0 = 5.6478e-04
Validation rmse = 1.145982
Epoch 9
Loss = 7.3858e-03, PNorm = 28.0813, GNorm = 0.6286, lr_0 = 5.5991e-04
Loss = 6.4457e-03, PNorm = 28.0955, GNorm = 0.9377, lr_0 = 5.5509e-04
Loss = 6.7692e-03, PNorm = 28.1110, GNorm = 0.9457, lr_0 = 5.5030e-04
Loss = 6.5773e-03, PNorm = 28.1241, GNorm = 0.7153, lr_0 = 5.4556e-04
Loss = 6.6226e-03, PNorm = 28.1360, GNorm = 1.8524, lr_0 = 5.4086e-04
Loss = 6.3060e-03, PNorm = 28.1464, GNorm = 0.5018, lr_0 = 5.3620e-04
Loss = 6.7315e-03, PNorm = 28.1589, GNorm = 0.5696, lr_0 = 5.3157e-04
Loss = 5.4312e-03, PNorm = 28.1692, GNorm = 0.4918, lr_0 = 5.2699e-04
Loss = 6.7298e-03, PNorm = 28.1802, GNorm = 0.8316, lr_0 = 5.2245e-04
Loss = 7.6237e-03, PNorm = 28.2001, GNorm = 0.8388, lr_0 = 5.1795e-04
Validation rmse = 1.158354
Epoch 10
Loss = 6.5522e-03, PNorm = 28.2199, GNorm = 1.1756, lr_0 = 5.1348e-04
Loss = 7.1821e-03, PNorm = 28.2331, GNorm = 0.6797, lr_0 = 5.0906e-04
Loss = 7.3850e-03, PNorm = 28.2429, GNorm = 1.5605, lr_0 = 5.0467e-04
Loss = 5.7983e-03, PNorm = 28.2462, GNorm = 1.2222, lr_0 = 5.0032e-04
Loss = 6.3517e-03, PNorm = 28.2583, GNorm = 0.9012, lr_0 = 4.9601e-04
Loss = 7.1607e-03, PNorm = 28.2755, GNorm = 1.0102, lr_0 = 4.9173e-04
Loss = 5.7830e-03, PNorm = 28.2932, GNorm = 0.8011, lr_0 = 4.8749e-04
Loss = 7.2223e-03, PNorm = 28.3073, GNorm = 1.7084, lr_0 = 4.8329e-04
Loss = 6.0534e-03, PNorm = 28.3146, GNorm = 0.8960, lr_0 = 4.7913e-04
Validation rmse = 1.150531
Epoch 11
Loss = 6.1988e-03, PNorm = 28.3257, GNorm = 2.0690, lr_0 = 4.7500e-04
Loss = 6.9004e-03, PNorm = 28.3387, GNorm = 0.4632, lr_0 = 4.7090e-04
Loss = 5.4552e-03, PNorm = 28.3516, GNorm = 1.6366, lr_0 = 4.6685e-04
Loss = 5.8387e-03, PNorm = 28.3595, GNorm = 0.9297, lr_0 = 4.6282e-04
Loss = 6.5083e-03, PNorm = 28.3694, GNorm = 0.3705, lr_0 = 4.5883e-04
Loss = 7.4275e-03, PNorm = 28.3831, GNorm = 1.2923, lr_0 = 4.5488e-04
Loss = 6.8468e-03, PNorm = 28.3950, GNorm = 1.1921, lr_0 = 4.5096e-04
Loss = 6.4491e-03, PNorm = 28.4016, GNorm = 1.1856, lr_0 = 4.4707e-04
Loss = 7.5560e-03, PNorm = 28.4171, GNorm = 1.0231, lr_0 = 4.4322e-04
Loss = 6.0621e-03, PNorm = 28.4289, GNorm = 0.5636, lr_0 = 4.3940e-04
Validation rmse = 1.120676
Epoch 12
Loss = 6.2347e-03, PNorm = 28.4391, GNorm = 0.6593, lr_0 = 4.3561e-04
Loss = 6.0501e-03, PNorm = 28.4483, GNorm = 0.7045, lr_0 = 4.3186e-04
Loss = 6.1868e-03, PNorm = 28.4528, GNorm = 0.9485, lr_0 = 4.2813e-04
Loss = 6.7554e-03, PNorm = 28.4550, GNorm = 0.6681, lr_0 = 4.2444e-04
Loss = 6.8484e-03, PNorm = 28.4568, GNorm = 1.1432, lr_0 = 4.2078e-04
Loss = 6.4252e-03, PNorm = 28.4618, GNorm = 0.5910, lr_0 = 4.1716e-04
Loss = 5.7353e-03, PNorm = 28.4684, GNorm = 0.5225, lr_0 = 4.1356e-04
Loss = 5.8029e-03, PNorm = 28.4731, GNorm = 1.2887, lr_0 = 4.1000e-04
Loss = 6.5612e-03, PNorm = 28.4830, GNorm = 0.8223, lr_0 = 4.0646e-04
Validation rmse = 1.118363
Epoch 13
Loss = 6.4522e-03, PNorm = 28.4946, GNorm = 0.9257, lr_0 = 4.0296e-04
Loss = 5.2855e-03, PNorm = 28.5045, GNorm = 0.8090, lr_0 = 3.9949e-04
Loss = 5.7683e-03, PNorm = 28.5100, GNorm = 1.0524, lr_0 = 3.9604e-04
Loss = 6.1855e-03, PNorm = 28.5188, GNorm = 1.3132, lr_0 = 3.9263e-04
Loss = 6.8997e-03, PNorm = 28.5289, GNorm = 1.2477, lr_0 = 3.8925e-04
Loss = 6.7353e-03, PNorm = 28.5364, GNorm = 1.2748, lr_0 = 3.8589e-04
Loss = 5.8537e-03, PNorm = 28.5434, GNorm = 0.5719, lr_0 = 3.8257e-04
Loss = 6.9254e-03, PNorm = 28.5489, GNorm = 0.7383, lr_0 = 3.7927e-04
Loss = 7.6544e-03, PNorm = 28.5588, GNorm = 0.6440, lr_0 = 3.7600e-04
Loss = 6.4328e-03, PNorm = 28.5675, GNorm = 1.5821, lr_0 = 3.7276e-04
Validation rmse = 1.168561
Epoch 14
Loss = 5.6877e-03, PNorm = 28.5761, GNorm = 1.3244, lr_0 = 3.6955e-04
Loss = 7.7385e-03, PNorm = 28.5820, GNorm = 1.0912, lr_0 = 3.6636e-04
Loss = 5.9694e-03, PNorm = 28.5895, GNorm = 0.8442, lr_0 = 3.6320e-04
Loss = 5.4076e-03, PNorm = 28.5929, GNorm = 1.7380, lr_0 = 3.6007e-04
Loss = 5.0302e-03, PNorm = 28.6022, GNorm = 0.7456, lr_0 = 3.5697e-04
Loss = 5.8487e-03, PNorm = 28.6146, GNorm = 1.1452, lr_0 = 3.5389e-04
Loss = 6.3937e-03, PNorm = 28.6199, GNorm = 1.0055, lr_0 = 3.5084e-04
Loss = 6.7871e-03, PNorm = 28.6251, GNorm = 0.8400, lr_0 = 3.4782e-04
Loss = 6.6987e-03, PNorm = 28.6315, GNorm = 1.1230, lr_0 = 3.4482e-04
Validation rmse = 1.115966
Epoch 15
Loss = 7.3853e-03, PNorm = 28.6394, GNorm = 0.6398, lr_0 = 3.4185e-04
Loss = 4.9897e-03, PNorm = 28.6482, GNorm = 0.5349, lr_0 = 3.3890e-04
Loss = 5.4718e-03, PNorm = 28.6553, GNorm = 0.9251, lr_0 = 3.3598e-04
Loss = 6.8652e-03, PNorm = 28.6615, GNorm = 1.2672, lr_0 = 3.3309e-04
Loss = 6.2700e-03, PNorm = 28.6674, GNorm = 1.2731, lr_0 = 3.3022e-04
Loss = 6.8328e-03, PNorm = 28.6732, GNorm = 2.3426, lr_0 = 3.2737e-04
Loss = 5.9683e-03, PNorm = 28.6812, GNorm = 1.4416, lr_0 = 3.2455e-04
Loss = 5.7678e-03, PNorm = 28.6859, GNorm = 0.9383, lr_0 = 3.2175e-04
Loss = 5.6463e-03, PNorm = 28.6922, GNorm = 1.2523, lr_0 = 3.1898e-04
Loss = 5.0153e-03, PNorm = 28.6950, GNorm = 0.5024, lr_0 = 3.1623e-04
Validation rmse = 1.113906
Epoch 16
Loss = 6.8403e-03, PNorm = 28.6993, GNorm = 0.8864, lr_0 = 3.1350e-04
Loss = 6.1606e-03, PNorm = 28.7070, GNorm = 0.8190, lr_0 = 3.1080e-04
Loss = 6.5985e-03, PNorm = 28.7121, GNorm = 0.7213, lr_0 = 3.0812e-04
Loss = 6.9555e-03, PNorm = 28.7189, GNorm = 1.9353, lr_0 = 3.0547e-04
Loss = 5.8548e-03, PNorm = 28.7252, GNorm = 1.1035, lr_0 = 3.0283e-04
Loss = 6.1322e-03, PNorm = 28.7277, GNorm = 1.7807, lr_0 = 3.0022e-04
Loss = 6.1373e-03, PNorm = 28.7317, GNorm = 1.0888, lr_0 = 2.9764e-04
Loss = 5.0368e-03, PNorm = 28.7338, GNorm = 1.0591, lr_0 = 2.9507e-04
Loss = 5.8538e-03, PNorm = 28.7392, GNorm = 1.4821, lr_0 = 2.9253e-04
Validation rmse = 1.134021
Epoch 17
Loss = 6.0222e-03, PNorm = 28.7418, GNorm = 0.6215, lr_0 = 2.9001e-04
Loss = 7.1072e-03, PNorm = 28.7494, GNorm = 1.6757, lr_0 = 2.8751e-04
Loss = 4.6130e-03, PNorm = 28.7565, GNorm = 0.9181, lr_0 = 2.8503e-04
Loss = 5.6334e-03, PNorm = 28.7640, GNorm = 0.7698, lr_0 = 2.8257e-04
Loss = 4.8807e-03, PNorm = 28.7715, GNorm = 1.0338, lr_0 = 2.8014e-04
Loss = 5.6986e-03, PNorm = 28.7774, GNorm = 0.6556, lr_0 = 2.7772e-04
Loss = 8.2396e-03, PNorm = 28.7861, GNorm = 2.1285, lr_0 = 2.7533e-04
Loss = 5.4690e-03, PNorm = 28.7927, GNorm = 1.0101, lr_0 = 2.7295e-04
Loss = 6.5139e-03, PNorm = 28.7954, GNorm = 1.1443, lr_0 = 2.7060e-04
Loss = 6.9940e-03, PNorm = 28.7974, GNorm = 0.6506, lr_0 = 2.6827e-04
Validation rmse = 1.106977
Epoch 18
Loss = 5.4322e-03, PNorm = 28.8024, GNorm = 0.6082, lr_0 = 2.6596e-04
Loss = 4.1878e-03, PNorm = 28.8053, GNorm = 0.8011, lr_0 = 2.6367e-04
Loss = 6.8572e-03, PNorm = 28.8100, GNorm = 1.0276, lr_0 = 2.6139e-04
Loss = 5.0558e-03, PNorm = 28.8166, GNorm = 0.5519, lr_0 = 2.5914e-04
Loss = 6.2337e-03, PNorm = 28.8217, GNorm = 0.5741, lr_0 = 2.5691e-04
Loss = 6.3531e-03, PNorm = 28.8257, GNorm = 0.9034, lr_0 = 2.5469e-04
Loss = 6.2653e-03, PNorm = 28.8292, GNorm = 1.0507, lr_0 = 2.5250e-04
Loss = 6.1883e-03, PNorm = 28.8324, GNorm = 0.5377, lr_0 = 2.5032e-04
Loss = 5.9690e-03, PNorm = 28.8366, GNorm = 1.2583, lr_0 = 2.4816e-04
Validation rmse = 1.137499
Epoch 19
Loss = 7.0033e-03, PNorm = 28.8370, GNorm = 1.2930, lr_0 = 2.4602e-04
Loss = 5.8679e-03, PNorm = 28.8387, GNorm = 0.6021, lr_0 = 2.4390e-04
Loss = 5.4185e-03, PNorm = 28.8411, GNorm = 0.7415, lr_0 = 2.4180e-04
Loss = 6.2370e-03, PNorm = 28.8426, GNorm = 1.3577, lr_0 = 2.3972e-04
Loss = 5.1770e-03, PNorm = 28.8461, GNorm = 0.4410, lr_0 = 2.3765e-04
Loss = 5.8387e-03, PNorm = 28.8490, GNorm = 0.6749, lr_0 = 2.3560e-04
Loss = 6.0228e-03, PNorm = 28.8521, GNorm = 1.8260, lr_0 = 2.3357e-04
Loss = 5.0937e-03, PNorm = 28.8568, GNorm = 1.1229, lr_0 = 2.3156e-04
Loss = 6.1995e-03, PNorm = 28.8580, GNorm = 0.5872, lr_0 = 2.2956e-04
Loss = 7.1302e-03, PNorm = 28.8631, GNorm = 0.9486, lr_0 = 2.2758e-04
Validation rmse = 1.068645
Epoch 20
Loss = 6.4345e-03, PNorm = 28.8675, GNorm = 1.7421, lr_0 = 2.2562e-04
Loss = 5.7763e-03, PNorm = 28.8710, GNorm = 0.8065, lr_0 = 2.2368e-04
Loss = 5.7659e-03, PNorm = 28.8753, GNorm = 0.6598, lr_0 = 2.2175e-04
Loss = 6.4275e-03, PNorm = 28.8812, GNorm = 0.5920, lr_0 = 2.1984e-04
Loss = 5.5051e-03, PNorm = 28.8833, GNorm = 1.1578, lr_0 = 2.1794e-04
Loss = 5.7869e-03, PNorm = 28.8869, GNorm = 0.9736, lr_0 = 2.1607e-04
Loss = 4.9025e-03, PNorm = 28.8900, GNorm = 1.2115, lr_0 = 2.1420e-04
Loss = 4.9590e-03, PNorm = 28.8922, GNorm = 1.0417, lr_0 = 2.1236e-04
Loss = 6.0846e-03, PNorm = 28.8934, GNorm = 0.9691, lr_0 = 2.1053e-04
Validation rmse = 1.063539
Epoch 21
Loss = 5.7544e-03, PNorm = 28.8958, GNorm = 1.0176, lr_0 = 2.0871e-04
Loss = 6.0701e-03, PNorm = 28.8992, GNorm = 0.5314, lr_0 = 2.0691e-04
Loss = 6.1758e-03, PNorm = 28.9021, GNorm = 0.9437, lr_0 = 2.0513e-04
Loss = 5.5306e-03, PNorm = 28.9042, GNorm = 1.2795, lr_0 = 2.0336e-04
Loss = 5.5447e-03, PNorm = 28.9086, GNorm = 0.5954, lr_0 = 2.0161e-04
Loss = 5.9412e-03, PNorm = 28.9121, GNorm = 0.4059, lr_0 = 1.9987e-04
Loss = 5.0099e-03, PNorm = 28.9172, GNorm = 2.2556, lr_0 = 1.9815e-04
Loss = 6.2263e-03, PNorm = 28.9213, GNorm = 0.7753, lr_0 = 1.9644e-04
Loss = 5.9193e-03, PNorm = 28.9269, GNorm = 2.0955, lr_0 = 1.9475e-04
Loss = 5.4119e-03, PNorm = 28.9307, GNorm = 0.5253, lr_0 = 1.9307e-04
Validation rmse = 1.054873
Epoch 22
Loss = 5.8570e-03, PNorm = 28.9326, GNorm = 1.5148, lr_0 = 1.9141e-04
Loss = 5.3093e-03, PNorm = 28.9322, GNorm = 0.6476, lr_0 = 1.8976e-04
Loss = 5.1856e-03, PNorm = 28.9337, GNorm = 1.3922, lr_0 = 1.8812e-04
Loss = 5.1896e-03, PNorm = 28.9369, GNorm = 0.6336, lr_0 = 1.8650e-04
Loss = 5.1279e-03, PNorm = 28.9396, GNorm = 0.6879, lr_0 = 1.8489e-04
Loss = 6.1973e-03, PNorm = 28.9454, GNorm = 0.8798, lr_0 = 1.8330e-04
Loss = 6.9679e-03, PNorm = 28.9497, GNorm = 1.2506, lr_0 = 1.8172e-04
Loss = 5.1963e-03, PNorm = 28.9522, GNorm = 0.6636, lr_0 = 1.8015e-04
Loss = 5.2487e-03, PNorm = 28.9543, GNorm = 0.6980, lr_0 = 1.7860e-04
Validation rmse = 1.064692
Epoch 23
Loss = 6.1850e-03, PNorm = 28.9573, GNorm = 2.0434, lr_0 = 1.7706e-04
Loss = 6.7886e-03, PNorm = 28.9611, GNorm = 1.2048, lr_0 = 1.7553e-04
Loss = 5.8590e-03, PNorm = 28.9622, GNorm = 1.2169, lr_0 = 1.7402e-04
Loss = 5.9803e-03, PNorm = 28.9651, GNorm = 0.9061, lr_0 = 1.7252e-04
Loss = 5.0736e-03, PNorm = 28.9671, GNorm = 0.5168, lr_0 = 1.7103e-04
Loss = 4.6677e-03, PNorm = 28.9693, GNorm = 0.8757, lr_0 = 1.6956e-04
Loss = 6.3092e-03, PNorm = 28.9726, GNorm = 0.4532, lr_0 = 1.6810e-04
Loss = 5.7809e-03, PNorm = 28.9740, GNorm = 0.9381, lr_0 = 1.6665e-04
Loss = 5.1965e-03, PNorm = 28.9766, GNorm = 1.1469, lr_0 = 1.6521e-04
Loss = 5.9029e-03, PNorm = 28.9797, GNorm = 0.6363, lr_0 = 1.6379e-04
Validation rmse = 1.064521
Epoch 24
Loss = 5.2397e-03, PNorm = 28.9812, GNorm = 1.8300, lr_0 = 1.6238e-04
Loss = 5.1985e-03, PNorm = 28.9834, GNorm = 0.7181, lr_0 = 1.6098e-04
Loss = 4.9103e-03, PNorm = 28.9846, GNorm = 1.1435, lr_0 = 1.5959e-04
Loss = 5.9900e-03, PNorm = 28.9862, GNorm = 0.8038, lr_0 = 1.5822e-04
Loss = 5.7609e-03, PNorm = 28.9891, GNorm = 0.6806, lr_0 = 1.5685e-04
Loss = 6.1567e-03, PNorm = 28.9925, GNorm = 0.5906, lr_0 = 1.5550e-04
Loss = 6.0591e-03, PNorm = 28.9947, GNorm = 0.9212, lr_0 = 1.5416e-04
Loss = 5.6386e-03, PNorm = 28.9961, GNorm = 0.7818, lr_0 = 1.5283e-04
Loss = 5.4177e-03, PNorm = 28.9983, GNorm = 0.4952, lr_0 = 1.5151e-04
Validation rmse = 1.072230
Epoch 25
Loss = 5.1352e-03, PNorm = 28.9997, GNorm = 1.9648, lr_0 = 1.5021e-04
Loss = 5.2603e-03, PNorm = 29.0004, GNorm = 0.5447, lr_0 = 1.4891e-04
Loss = 6.9514e-03, PNorm = 29.0036, GNorm = 1.1551, lr_0 = 1.4763e-04
Loss = 4.6204e-03, PNorm = 29.0061, GNorm = 0.9850, lr_0 = 1.4636e-04
Loss = 5.8345e-03, PNorm = 29.0075, GNorm = 0.8339, lr_0 = 1.4510e-04
Loss = 5.4374e-03, PNorm = 29.0092, GNorm = 0.5862, lr_0 = 1.4384e-04
Loss = 5.0668e-03, PNorm = 29.0096, GNorm = 0.7720, lr_0 = 1.4261e-04
Loss = 5.1907e-03, PNorm = 29.0114, GNorm = 0.5736, lr_0 = 1.4138e-04
Loss = 5.9645e-03, PNorm = 29.0143, GNorm = 1.2989, lr_0 = 1.4016e-04
Loss = 5.9838e-03, PNorm = 29.0155, GNorm = 0.8152, lr_0 = 1.3895e-04
Validation rmse = 1.074634
Epoch 26
Loss = 4.7321e-03, PNorm = 29.0158, GNorm = 0.8277, lr_0 = 1.3775e-04
Loss = 5.6243e-03, PNorm = 29.0162, GNorm = 0.6772, lr_0 = 1.3656e-04
Loss = 6.3789e-03, PNorm = 29.0180, GNorm = 1.1987, lr_0 = 1.3539e-04
Loss = 6.2152e-03, PNorm = 29.0226, GNorm = 1.3358, lr_0 = 1.3422e-04
Loss = 6.4003e-03, PNorm = 29.0244, GNorm = 1.0493, lr_0 = 1.3306e-04
Loss = 5.2468e-03, PNorm = 29.0257, GNorm = 1.0377, lr_0 = 1.3192e-04
Loss = 5.7488e-03, PNorm = 29.0255, GNorm = 0.5701, lr_0 = 1.3078e-04
Loss = 4.8337e-03, PNorm = 29.0276, GNorm = 0.6614, lr_0 = 1.2965e-04
Loss = 5.9604e-03, PNorm = 29.0306, GNorm = 0.7281, lr_0 = 1.2854e-04
Validation rmse = 1.063707
Epoch 27
Loss = 7.3041e-03, PNorm = 29.0338, GNorm = 0.6511, lr_0 = 1.2743e-04
Loss = 5.7596e-03, PNorm = 29.0359, GNorm = 0.7519, lr_0 = 1.2633e-04
Loss = 5.9811e-03, PNorm = 29.0368, GNorm = 0.7505, lr_0 = 1.2524e-04
Loss = 5.8352e-03, PNorm = 29.0382, GNorm = 0.7187, lr_0 = 1.2416e-04
Loss = 4.3854e-03, PNorm = 29.0394, GNorm = 0.7378, lr_0 = 1.2309e-04
Loss = 5.2983e-03, PNorm = 29.0400, GNorm = 1.1353, lr_0 = 1.2203e-04
Loss = 5.0909e-03, PNorm = 29.0416, GNorm = 1.7149, lr_0 = 1.2098e-04
Loss = 6.2197e-03, PNorm = 29.0434, GNorm = 1.0902, lr_0 = 1.1994e-04
Loss = 5.3148e-03, PNorm = 29.0449, GNorm = 0.8764, lr_0 = 1.1890e-04
Loss = 6.1916e-03, PNorm = 29.0473, GNorm = 1.7096, lr_0 = 1.1788e-04
Validation rmse = 1.041131
Epoch 28
Loss = 5.0192e-03, PNorm = 29.0512, GNorm = 0.6114, lr_0 = 1.1686e-04
Loss = 5.3373e-03, PNorm = 29.0534, GNorm = 1.0395, lr_0 = 1.1585e-04
Loss = 5.8299e-03, PNorm = 29.0557, GNorm = 0.6777, lr_0 = 1.1486e-04
Loss = 5.9547e-03, PNorm = 29.0581, GNorm = 0.8946, lr_0 = 1.1387e-04
Loss = 4.7107e-03, PNorm = 29.0601, GNorm = 1.2074, lr_0 = 1.1288e-04
Loss = 5.1208e-03, PNorm = 29.0623, GNorm = 0.8379, lr_0 = 1.1191e-04
Loss = 5.0940e-03, PNorm = 29.0649, GNorm = 1.4508, lr_0 = 1.1095e-04
Loss = 4.2431e-03, PNorm = 29.0675, GNorm = 1.1243, lr_0 = 1.0999e-04
Loss = 6.5829e-03, PNorm = 29.0707, GNorm = 0.8444, lr_0 = 1.0904e-04
Validation rmse = 1.039838
Epoch 29
Loss = 5.0237e-03, PNorm = 29.0737, GNorm = 1.2257, lr_0 = 1.0810e-04
Loss = 5.0631e-03, PNorm = 29.0753, GNorm = 1.0677, lr_0 = 1.0717e-04
Loss = 5.0356e-03, PNorm = 29.0761, GNorm = 0.8068, lr_0 = 1.0625e-04
Loss = 5.5840e-03, PNorm = 29.0771, GNorm = 1.0096, lr_0 = 1.0533e-04
Loss = 4.8920e-03, PNorm = 29.0780, GNorm = 0.9064, lr_0 = 1.0442e-04
Loss = 6.4030e-03, PNorm = 29.0773, GNorm = 0.8009, lr_0 = 1.0352e-04
Loss = 5.6035e-03, PNorm = 29.0781, GNorm = 0.8508, lr_0 = 1.0263e-04
Loss = 5.7744e-03, PNorm = 29.0807, GNorm = 0.9973, lr_0 = 1.0175e-04
Loss = 6.7144e-03, PNorm = 29.0830, GNorm = 1.4193, lr_0 = 1.0087e-04
Loss = 4.2215e-03, PNorm = 29.0855, GNorm = 0.5993, lr_0 = 1.0000e-04
Validation rmse = 1.095686
Model 0 best validation rmse = 1.039838 on epoch 28
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.236421
Ensemble test rmse = 1.236421
1-fold cross validation
Seed 0 ==> test rmse = 1.236421
Overall test rmse = 1.236421 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): Sigmoid()
      (att_1): Linear(in_features=20, out_features=30, bias=True)
      (att_2): Linear(in_features=30, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,292
Epoch 0
Validation rmse = 4.605894
Epoch 1
Loss = 4.8260e-01, PNorm = 9.3576, GNorm = 2.0227, lr_0 = 6.0000e-04
Validation rmse = 4.615083
Epoch 2
Loss = 2.4413e-01, PNorm = 9.3598, GNorm = 0.0341, lr_0 = 9.8189e-04
Validation rmse = 4.617380
Epoch 3
Loss = 7.9020e-01, PNorm = 9.3631, GNorm = 2.5172, lr_0 = 8.9615e-04
Validation rmse = 4.621809
Epoch 4
Loss = 1.1010e+00, PNorm = 9.3659, GNorm = 0.0379, lr_0 = 8.1790e-04
Validation rmse = 4.637455
Epoch 5
Loss = 1.2146e+00, PNorm = 9.3688, GNorm = 4.3324, lr_0 = 7.4648e-04
Validation rmse = 4.644871
Epoch 6
Loss = 1.3680e+00, PNorm = 9.3724, GNorm = 2.2066, lr_0 = 6.8129e-04
Validation rmse = 4.651435
Epoch 7
Loss = 1.1579e+00, PNorm = 9.3758, GNorm = 0.3791, lr_0 = 6.2180e-04
Validation rmse = 4.653719
Epoch 8
Loss = 1.0816e+00, PNorm = 9.3785, GNorm = 4.2274, lr_0 = 5.6750e-04
Validation rmse = 4.655754
Epoch 9
Loss = 9.5872e-01, PNorm = 9.3818, GNorm = 4.8518, lr_0 = 5.1795e-04
Validation rmse = 4.653968
Epoch 10
Validation rmse = 4.653474
Epoch 11
Loss = 2.1991e+00, PNorm = 9.3840, GNorm = 4.8526, lr_0 = 4.7272e-04
Validation rmse = 4.650970
Epoch 12
Loss = 1.5229e+00, PNorm = 9.3865, GNorm = 4.4667, lr_0 = 4.3144e-04
Validation rmse = 4.646471
Epoch 13
Loss = 5.2835e-01, PNorm = 9.3890, GNorm = 4.0564, lr_0 = 3.9377e-04
Validation rmse = 4.645058
Epoch 14
Loss = 1.2779e+00, PNorm = 9.3914, GNorm = 4.1746, lr_0 = 3.5938e-04
Validation rmse = 4.643008
Epoch 15
Loss = 1.2986e+00, PNorm = 9.3938, GNorm = 4.8905, lr_0 = 3.2800e-04
Validation rmse = 4.640828
Epoch 16
Loss = 8.8306e-01, PNorm = 9.3953, GNorm = 4.8928, lr_0 = 2.9936e-04
Validation rmse = 4.639818
Epoch 17
Loss = 9.0277e-01, PNorm = 9.3971, GNorm = 4.8974, lr_0 = 2.7322e-04
Validation rmse = 4.638423
Epoch 18
Loss = 8.5494e-01, PNorm = 9.3988, GNorm = 0.1130, lr_0 = 2.4936e-04
Validation rmse = 4.637875
Epoch 19
Loss = 9.3032e-01, PNorm = 9.4004, GNorm = 0.2835, lr_0 = 2.2758e-04
Validation rmse = 4.636337
Epoch 20
Validation rmse = 4.634272
Epoch 21
Loss = 5.8719e-01, PNorm = 9.4019, GNorm = 2.1462, lr_0 = 2.0771e-04
Validation rmse = 4.632480
Epoch 22
Loss = 1.8649e+00, PNorm = 9.4033, GNorm = 4.0152, lr_0 = 1.8957e-04
Validation rmse = 4.633112
Epoch 23
Loss = 3.8491e-01, PNorm = 9.4046, GNorm = 2.1760, lr_0 = 1.7302e-04
Validation rmse = 4.632490
Epoch 24
Loss = 1.1147e+00, PNorm = 9.4058, GNorm = 4.0151, lr_0 = 1.5791e-04
Validation rmse = 4.631384
Epoch 25
Loss = 1.3277e+00, PNorm = 9.4071, GNorm = 4.9243, lr_0 = 1.4412e-04
Validation rmse = 4.630282
Epoch 26
Loss = 1.2015e+00, PNorm = 9.4081, GNorm = 2.1412, lr_0 = 1.3154e-04
Validation rmse = 4.629895
Epoch 27
Loss = 1.0997e+00, PNorm = 9.4089, GNorm = 4.4274, lr_0 = 1.2005e-04
Validation rmse = 4.629070
Epoch 28
Loss = 8.4345e-01, PNorm = 9.4096, GNorm = 4.1013, lr_0 = 1.0957e-04
Validation rmse = 4.628538
Epoch 29
Loss = 9.1810e-01, PNorm = 9.4104, GNorm = 0.2228, lr_0 = 1.0000e-04
Validation rmse = 4.628285
Model 0 best validation rmse = 4.605894 on epoch 0
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 2.161894
Ensemble test rmse = 2.161894
1-fold cross validation
Seed 0 ==> test rmse = 2.161894
Overall test rmse = 2.161894 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=30, bias=True)
      (att_2): Linear(in_features=30, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,292
Epoch 0
Validation rmse = 4.783583
Epoch 1
Loss = 5.1367e-01, PNorm = 9.5081, GNorm = 1.8227, lr_0 = 6.0000e-04
Validation rmse = 4.788314
Epoch 2
Loss = 2.5757e-01, PNorm = 9.5074, GNorm = 0.2277, lr_0 = 9.8189e-04
Validation rmse = 4.793020
Epoch 3
Loss = 7.1060e-01, PNorm = 9.5071, GNorm = 2.0188, lr_0 = 8.9615e-04
Validation rmse = 4.797885
Epoch 4
Loss = 1.1783e+00, PNorm = 9.5070, GNorm = 0.1866, lr_0 = 8.1790e-04
Validation rmse = 4.810546
Epoch 5
Loss = 1.2309e+00, PNorm = 9.5077, GNorm = 3.1105, lr_0 = 7.4648e-04
Validation rmse = 4.817352
Epoch 6
Loss = 1.3303e+00, PNorm = 9.5095, GNorm = 2.1131, lr_0 = 6.8129e-04
Validation rmse = 4.826707
Epoch 7
Loss = 1.1147e+00, PNorm = 9.5117, GNorm = 0.4723, lr_0 = 6.2180e-04
Validation rmse = 4.832517
Epoch 8
Loss = 1.0476e+00, PNorm = 9.5145, GNorm = 3.2281, lr_0 = 5.6750e-04
Validation rmse = 4.839272
Epoch 9
Loss = 9.2035e-01, PNorm = 9.5169, GNorm = 4.5094, lr_0 = 5.1795e-04
Validation rmse = 4.844806
Epoch 10
Validation rmse = 4.859226
Epoch 11
Loss = 1.9374e+00, PNorm = 9.5206, GNorm = 4.5693, lr_0 = 4.7272e-04
Validation rmse = 4.868906
Epoch 12
Loss = 1.4498e+00, PNorm = 9.5238, GNorm = 4.0966, lr_0 = 4.3144e-04
Validation rmse = 4.872701
Epoch 13
Loss = 4.9295e-01, PNorm = 9.5267, GNorm = 2.5920, lr_0 = 3.9377e-04
Validation rmse = 4.880032
Epoch 14
Loss = 1.2308e+00, PNorm = 9.5299, GNorm = 3.3964, lr_0 = 3.5938e-04
Validation rmse = 4.886151
Epoch 15
Loss = 1.1743e+00, PNorm = 9.5328, GNorm = 4.7153, lr_0 = 3.2800e-04
Validation rmse = 4.889209
Epoch 16
Loss = 7.9089e-01, PNorm = 9.5355, GNorm = 4.7410, lr_0 = 2.9936e-04
Validation rmse = 4.897441
Epoch 17
Loss = 7.8521e-01, PNorm = 9.5379, GNorm = 4.7636, lr_0 = 2.7322e-04
Validation rmse = 4.902795
Epoch 18
Loss = 7.7109e-01, PNorm = 9.5405, GNorm = 0.8778, lr_0 = 2.4936e-04
Validation rmse = 4.909182
Epoch 19
Loss = 8.4194e-01, PNorm = 9.5427, GNorm = 0.1061, lr_0 = 2.2758e-04
Validation rmse = 4.913101
Epoch 20
Validation rmse = 4.914373
Epoch 21
Loss = 7.3717e-01, PNorm = 9.5444, GNorm = 2.2753, lr_0 = 2.0771e-04
Validation rmse = 4.915869
Epoch 22
Loss = 1.5054e+00, PNorm = 9.5460, GNorm = 2.6133, lr_0 = 1.8957e-04
Validation rmse = 4.921759
Epoch 23
Loss = 3.7771e-01, PNorm = 9.5476, GNorm = 1.7477, lr_0 = 1.7302e-04
Validation rmse = 4.923939
Epoch 24
Loss = 1.0294e+00, PNorm = 9.5491, GNorm = 2.6160, lr_0 = 1.5791e-04
Validation rmse = 4.927194
Epoch 25
Loss = 1.1187e+00, PNorm = 9.5504, GNorm = 4.8495, lr_0 = 1.4412e-04
Validation rmse = 4.929338
Epoch 26
Loss = 1.0519e+00, PNorm = 9.5518, GNorm = 2.2992, lr_0 = 1.3154e-04
Validation rmse = 4.932533
Epoch 27
Loss = 9.2983e-01, PNorm = 9.5528, GNorm = 4.3053, lr_0 = 1.2005e-04
Validation rmse = 4.933487
Epoch 28
Loss = 7.3225e-01, PNorm = 9.5537, GNorm = 4.3090, lr_0 = 1.0957e-04
Validation rmse = 4.934547
Epoch 29
Loss = 8.1007e-01, PNorm = 9.5547, GNorm = 0.3589, lr_0 = 1.0000e-04
Validation rmse = 4.936747
Model 0 best validation rmse = 4.783583 on epoch 0
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.943657
Ensemble test rmse = 1.943657
1-fold cross validation
Seed 0 ==> test rmse = 1.943657
Overall test rmse = 1.943657 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=30, bias=True)
      (att_2): Linear(in_features=30, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,292
Epoch 0
Validation rmse = 4.736229
Epoch 1
Loss = 5.1972e-01, PNorm = 9.5192, GNorm = 1.9265, lr_0 = 6.0000e-04
Validation rmse = 4.746363
Epoch 2
Loss = 2.6704e-01, PNorm = 9.5198, GNorm = 0.3384, lr_0 = 9.8189e-04
Validation rmse = 4.749789
Epoch 3
Loss = 7.1217e-01, PNorm = 9.5212, GNorm = 1.9960, lr_0 = 8.9615e-04
Validation rmse = 4.745447
Epoch 4
Loss = 1.2350e+00, PNorm = 9.5189, GNorm = 0.2802, lr_0 = 8.1790e-04
Validation rmse = 4.744351
Epoch 5
Loss = 1.2951e+00, PNorm = 9.5178, GNorm = 2.3769, lr_0 = 7.4648e-04
Validation rmse = 4.748649
Epoch 6
Loss = 1.3946e+00, PNorm = 9.5177, GNorm = 1.9819, lr_0 = 6.8129e-04
Validation rmse = 4.753771
Epoch 7
Loss = 1.1859e+00, PNorm = 9.5179, GNorm = 0.6181, lr_0 = 6.2180e-04
Validation rmse = 4.753283
Epoch 8
Loss = 1.1145e+00, PNorm = 9.5183, GNorm = 2.3767, lr_0 = 5.6750e-04
Validation rmse = 4.754875
Epoch 9
Loss = 9.9424e-01, PNorm = 9.5187, GNorm = 4.2287, lr_0 = 5.1795e-04
Validation rmse = 4.755650
Epoch 10
Validation rmse = 4.763014
Epoch 11
Loss = 2.4857e+00, PNorm = 9.5194, GNorm = 3.7402, lr_0 = 4.7272e-04
Validation rmse = 4.761776
Epoch 12
Loss = 1.5834e+00, PNorm = 9.5200, GNorm = 3.4984, lr_0 = 4.3144e-04
Validation rmse = 4.758812
Epoch 13
Loss = 5.0297e-01, PNorm = 9.5210, GNorm = 2.4119, lr_0 = 3.9377e-04
Validation rmse = 4.759671
Epoch 14
Loss = 1.2968e+00, PNorm = 9.5219, GNorm = 2.4153, lr_0 = 3.5938e-04
Validation rmse = 4.758490
Epoch 15
Loss = 1.3026e+00, PNorm = 9.5226, GNorm = 4.2249, lr_0 = 3.2800e-04
Validation rmse = 4.758073
Epoch 16
Loss = 9.9481e-01, PNorm = 9.5235, GNorm = 3.7683, lr_0 = 2.9936e-04
Validation rmse = 4.758685
Epoch 17
Loss = 9.8368e-01, PNorm = 9.5242, GNorm = 3.7763, lr_0 = 2.7322e-04
Validation rmse = 4.758698
Epoch 18
Loss = 9.2543e-01, PNorm = 9.5250, GNorm = 0.1860, lr_0 = 2.4936e-04
Validation rmse = 4.760070
Epoch 19
Loss = 9.8289e-01, PNorm = 9.5258, GNorm = 0.5697, lr_0 = 2.2758e-04
Validation rmse = 4.759185
Epoch 20
Validation rmse = 4.758222
Epoch 21
Loss = 6.6048e-01, PNorm = 9.5265, GNorm = 2.2588, lr_0 = 2.0771e-04
Validation rmse = 4.757774
Epoch 22
Loss = 1.9501e+00, PNorm = 9.5271, GNorm = 2.4114, lr_0 = 1.8957e-04
Validation rmse = 4.761116
Epoch 23
Loss = 4.0233e-01, PNorm = 9.5277, GNorm = 1.7932, lr_0 = 1.7302e-04
Validation rmse = 4.760008
Epoch 24
Loss = 1.1461e+00, PNorm = 9.5282, GNorm = 2.4116, lr_0 = 1.5791e-04
Validation rmse = 4.761211
Epoch 25
Loss = 1.4032e+00, PNorm = 9.5287, GNorm = 3.8137, lr_0 = 1.4412e-04
Validation rmse = 4.760169
Epoch 26
Loss = 1.2773e+00, PNorm = 9.5292, GNorm = 2.2561, lr_0 = 1.3154e-04
Validation rmse = 4.760717
Epoch 27
Loss = 1.1597e+00, PNorm = 9.5296, GNorm = 3.5212, lr_0 = 1.2005e-04
Validation rmse = 4.760562
Epoch 28
Loss = 9.1430e-01, PNorm = 9.5301, GNorm = 3.5270, lr_0 = 1.0957e-04
Validation rmse = 4.760375
Epoch 29
Loss = 9.7322e-01, PNorm = 9.5304, GNorm = 0.5502, lr_0 = 1.0000e-04
Validation rmse = 4.761775
Model 0 best validation rmse = 4.736229 on epoch 0
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.935646
Ensemble test rmse = 1.935646
1-fold cross validation
Seed 0 ==> test rmse = 1.935646
Overall test rmse = 1.935646 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=30, bias=True)
      (att_2): Linear(in_features=30, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,292
Epoch 0
Validation rmse = 4.763410
Epoch 1
Loss = 5.1897e-01, PNorm = 9.2863, GNorm = 1.4701, lr_0 = 6.0000e-04
Validation rmse = 4.762777
Epoch 2
Loss = 2.6272e-01, PNorm = 9.2864, GNorm = 0.1788, lr_0 = 9.8189e-04
Validation rmse = 4.760840
Epoch 3
Loss = 7.1056e-01, PNorm = 9.2869, GNorm = 1.7787, lr_0 = 8.9615e-04
Validation rmse = 4.758785
Epoch 4
Loss = 1.2195e+00, PNorm = 9.2879, GNorm = 0.1781, lr_0 = 8.1790e-04
Validation rmse = 4.767167
Epoch 5
Loss = 1.2821e+00, PNorm = 9.2895, GNorm = 2.6743, lr_0 = 7.4648e-04
Validation rmse = 4.768242
Epoch 6
Loss = 1.3842e+00, PNorm = 9.2931, GNorm = 1.7861, lr_0 = 6.8129e-04
Validation rmse = 4.765366
Epoch 7
Loss = 1.1773e+00, PNorm = 9.2965, GNorm = 0.4849, lr_0 = 6.2180e-04
Validation rmse = 4.768224
Epoch 8
Loss = 1.1062e+00, PNorm = 9.2997, GNorm = 2.6941, lr_0 = 5.6750e-04
Validation rmse = 4.768334
Epoch 9
Loss = 9.8685e-01, PNorm = 9.3039, GNorm = 3.5599, lr_0 = 5.1795e-04
Validation rmse = 4.764998
Epoch 10
Validation rmse = 4.769240
Epoch 11
Loss = 2.4883e+00, PNorm = 9.3073, GNorm = 3.3258, lr_0 = 4.7272e-04
Validation rmse = 4.768699
Epoch 12
Loss = 1.5799e+00, PNorm = 9.3115, GNorm = 2.9038, lr_0 = 4.3144e-04
Validation rmse = 4.767650
Epoch 13
Loss = 4.7443e-01, PNorm = 9.3153, GNorm = 2.9918, lr_0 = 3.9377e-04
Validation rmse = 4.767014
Epoch 14
Loss = 1.2643e+00, PNorm = 9.3193, GNorm = 2.8197, lr_0 = 3.5938e-04
Validation rmse = 4.766446
Epoch 15
Loss = 1.2761e+00, PNorm = 9.3226, GNorm = 3.6460, lr_0 = 3.2800e-04
Validation rmse = 4.763840
Epoch 16
Loss = 9.9915e-01, PNorm = 9.3252, GNorm = 3.3254, lr_0 = 2.9936e-04
Validation rmse = 4.763873
Epoch 17
Loss = 9.7271e-01, PNorm = 9.3285, GNorm = 3.3325, lr_0 = 2.7322e-04
Validation rmse = 4.764911
Epoch 18
Loss = 9.2392e-01, PNorm = 9.3313, GNorm = 0.1814, lr_0 = 2.4936e-04
Validation rmse = 4.765169
Epoch 19
Loss = 9.6457e-01, PNorm = 9.3344, GNorm = 0.5015, lr_0 = 2.2758e-04
Validation rmse = 4.763892
Epoch 20
Validation rmse = 4.762542
Epoch 21
Loss = 6.4952e-01, PNorm = 9.3371, GNorm = 1.8594, lr_0 = 2.0771e-04
Validation rmse = 4.761657
Epoch 22
Loss = 1.8800e+00, PNorm = 9.3398, GNorm = 3.3142, lr_0 = 1.8957e-04
Validation rmse = 4.759997
Epoch 23
Loss = 4.0599e-01, PNorm = 9.3421, GNorm = 1.5534, lr_0 = 1.7302e-04
Validation rmse = 4.760049
Epoch 24
Loss = 1.0898e+00, PNorm = 9.3445, GNorm = 3.2615, lr_0 = 1.5791e-04
Validation rmse = 4.760497
Epoch 25
Loss = 1.3662e+00, PNorm = 9.3469, GNorm = 3.3516, lr_0 = 1.4412e-04
Validation rmse = 4.760472
Epoch 26
Loss = 1.2427e+00, PNorm = 9.3489, GNorm = 1.8869, lr_0 = 1.3154e-04
Validation rmse = 4.760242
Epoch 27
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=30, bias=True)
      (att_2): Linear(in_features=30, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,292
Epoch 0
Validation rmse = 4.842452
Epoch 1
Loss = 5.4298e-01, PNorm = 9.3841, GNorm = 2.2382, lr_0 = 6.0000e-04
Validation rmse = 4.838166
Epoch 2
Loss = 2.7776e-01, PNorm = 9.3836, GNorm = 0.1681, lr_0 = 9.8189e-04
Validation rmse = 4.829973
Epoch 3
Loss = 6.7423e-01, PNorm = 9.3842, GNorm = 2.5351, lr_0 = 8.9615e-04
Validation rmse = 4.828034
Epoch 4
Loss = 1.1505e+00, PNorm = 9.3857, GNorm = 0.1699, lr_0 = 8.1790e-04
Validation rmse = 4.849718
Epoch 5
Loss = 1.2268e+00, PNorm = 9.3876, GNorm = 5.6183, lr_0 = 7.4648e-04
Validation rmse = 4.850584
Epoch 6
Loss = 1.3196e+00, PNorm = 9.3898, GNorm = 2.4996, lr_0 = 6.8129e-04
Validation rmse = 4.853613
Epoch 7
Loss = 1.1072e+00, PNorm = 9.3927, GNorm = 0.5494, lr_0 = 6.2180e-04
Validation rmse = 4.856079
Epoch 8
Loss = 1.0301e+00, PNorm = 9.3958, GNorm = 5.1278, lr_0 = 5.6750e-04
Validation rmse = 4.866266
Epoch 9
Loss = 9.1321e-01, PNorm = 9.3984, GNorm = 5.6000, lr_0 = 5.1795e-04
Validation rmse = 4.866173
Epoch 10
Validation rmse = 4.886687
Epoch 11
Loss = 2.3060e+00, PNorm = 9.4016, GNorm = 5.2843, lr_0 = 4.7272e-04
Validation rmse = 4.899834
Epoch 12
Loss = 1.4212e+00, PNorm = 9.4050, GNorm = 4.5032, lr_0 = 4.3144e-04
Validation rmse = 4.898942
Epoch 13
Loss = 3.8744e-01, PNorm = 9.4085, GNorm = 3.8726, lr_0 = 3.9377e-04
Validation rmse = 4.907659
Epoch 14
Loss = 1.0891e+00, PNorm = 9.4118, GNorm = 5.0142, lr_0 = 3.5938e-04
Validation rmse = 4.912347
Epoch 15
Loss = 1.1545e+00, PNorm = 9.4148, GNorm = 5.8144, lr_0 = 3.2800e-04
Validation rmse = 4.914960
Epoch 16
Loss = 9.2338e-01, PNorm = 9.4172, GNorm = 5.4262, lr_0 = 2.9936e-04
Validation rmse = 4.927196
Epoch 17
Loss = 8.7958e-01, PNorm = 9.4199, GNorm = 5.4440, lr_0 = 2.7322e-04
Validation rmse = 4.933478
Epoch 18
Loss = 8.3005e-01, PNorm = 9.4223, GNorm = 0.0827, lr_0 = 2.4936e-04
Validation rmse = 4.938733
Epoch 19
Loss = 8.5211e-01, PNorm = 9.4246, GNorm = 0.4219, lr_0 = 2.2758e-04
Validation rmse = 4.941707
Epoch 20
Validation rmse = 4.942634
Epoch 21
Loss = 5.4607e-01, PNorm = 9.4267, GNorm = 2.2297, lr_0 = 2.0771e-04
Validation rmse = 4.944093
Epoch 22
Loss = 1.6235e+00, PNorm = 9.4287, GNorm = 3.9637, lr_0 = 1.8957e-04
Validation rmse = 4.953332
Epoch 23
Loss = 4.2601e-01, PNorm = 9.4304, GNorm = 2.6760, lr_0 = 1.7302e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=30, bias=True)
      (att_2): Linear(in_features=30, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,292
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=30, bias=True)
      (att_2): Linear(in_features=30, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,292
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Validation rmse = 4.754383
Epoch 1
Loss = 5.2879e-01, PNorm = 9.2201, GNorm = 1.5362, lr_0 = 6.0000e-04
Validation rmse = 4.754773
Epoch 2
Loss = 2.7038e-01, PNorm = 9.2213, GNorm = 0.3811, lr_0 = 9.8189e-04
Validation rmse = 4.750507
Epoch 3
Loss = 6.6759e-01, PNorm = 9.2226, GNorm = 1.6992, lr_0 = 8.9615e-04
Validation rmse = 4.744992
Epoch 4
Loss = 1.2384e+00, PNorm = 9.2241, GNorm = 0.3739, lr_0 = 8.1790e-04
Validation rmse = 4.745316
Epoch 5
Loss = 1.2778e+00, PNorm = 9.2257, GNorm = 4.0336, lr_0 = 7.4648e-04
Validation rmse = 4.743768
Epoch 6
Loss = 1.3668e+00, PNorm = 9.2273, GNorm = 1.6930, lr_0 = 6.8129e-04
Validation rmse = 4.743844
Epoch 7
Loss = 1.1685e+00, PNorm = 9.2289, GNorm = 0.8313, lr_0 = 6.2180e-04
Validation rmse = 4.741777
Epoch 8
Loss = 1.0913e+00, PNorm = 9.2304, GNorm = 4.0316, lr_0 = 5.6750e-04
Validation rmse = 4.739648
Epoch 9
Loss = 9.7637e-01, PNorm = 9.2320, GNorm = 5.2998, lr_0 = 5.1795e-04
Validation rmse = 4.738594
Epoch 10
Validation rmse = 4.739130
Epoch 11
Loss = 2.5179e+00, PNorm = 9.2333, GNorm = 3.2728, lr_0 = 4.7272e-04
Validation rmse = 4.738291
Epoch 12
Loss = 1.5618e+00, PNorm = 9.2347, GNorm = 4.6638, lr_0 = 4.3144e-04
Validation rmse = 4.735814
Epoch 13
Loss = 4.5987e-01, PNorm = 9.2362, GNorm = 4.0761, lr_0 = 3.9377e-04
Validation rmse = 4.735719
Epoch 14
Loss = 1.2418e+00, PNorm = 9.2376, GNorm = 4.0443, lr_0 = 3.5938e-04
Validation rmse = 4.734959
Epoch 15
Loss = 1.2544e+00, PNorm = 9.2389, GNorm = 5.3136, lr_0 = 3.2800e-04
Validation rmse = 4.734677
Epoch 16
Loss = 1.0061e+00, PNorm = 9.2401, GNorm = 3.2750, lr_0 = 2.9936e-04
Validation rmse = 4.734992
Epoch 17
Loss = 9.7496e-01, PNorm = 9.2413, GNorm = 3.2749, lr_0 = 2.7322e-04
Validation rmse = 4.735018
Epoch 18
Loss = 9.2100e-01, PNorm = 9.2422, GNorm = 0.3395, lr_0 = 2.4936e-04
Validation rmse = 4.735568
Epoch 19
Loss = 9.6067e-01, PNorm = 9.2433, GNorm = 0.8220, lr_0 = 2.2758e-04
Validation rmse = 4.735290
Epoch 20
Validation rmse = 4.734722
Epoch 21
Loss = 6.1479e-01, PNorm = 9.2442, GNorm = 1.6609, lr_0 = 2.0771e-04
Validation rmse = 4.734276
Epoch 22
Loss = 1.8978e+00, PNorm = 9.2450, GNorm = 4.0833, lr_0 = 1.8957e-04
Validation rmse = 4.735078
Epoch 23
Loss = 4.0247e-01, PNorm = 9.2458, GNorm = 1.5584, lr_0 = 1.7302e-04
Validation rmse = 4.734775
Epoch 24
Loss = 1.0819e+00, PNorm = 9.2465, GNorm = 3.9746, lr_0 = 1.5791e-04
Validation rmse = 4.734967
Epoch 25
Loss = 1.3670e+00, PNorm = 9.2471, GNorm = 3.2595, lr_0 = 1.4412e-04
Validation rmse = 4.734727
Epoch 26
Loss = 1.2403e+00, PNorm = 9.2478, GNorm = 1.6687, lr_0 = 1.3154e-04
Validation rmse = 4.734935
Epoch 27
Loss = 1.1367e+00, PNorm = 9.2482, GNorm = 4.7241, lr_0 = 1.2005e-04
Validation rmse = 4.734747
Epoch 28
Loss = 9.1367e-01, PNorm = 9.2487, GNorm = 4.7225, lr_0 = 1.0957e-04
Validation rmse = 4.734479
Epoch 29
Loss = 9.5208e-01, PNorm = 9.2492, GNorm = 0.8039, lr_0 = 1.0000e-04
Validation rmse = 4.734748
Model 0 best validation rmse = 4.734276 on epoch 21
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 1.913334
Ensemble test rmse = 1.913334
1-fold cross validation
Seed 0 ==> test rmse = 1.913334
Overall test rmse = 1.913334 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Validation rmse = 5.037161
Epoch 1
Loss = 4.2785e-01, PNorm = 9.2288, GNorm = 1.8272, lr_0 = 6.0000e-04
Validation rmse = 5.024985
Epoch 2
Loss = 2.1673e-01, PNorm = 9.2283, GNorm = 0.0679, lr_0 = 9.8189e-04
Validation rmse = 5.001761
Epoch 3
Loss = 8.7323e-01, PNorm = 9.2278, GNorm = 2.7507, lr_0 = 8.9615e-04
Validation rmse = 4.989134
Epoch 4
Loss = 1.2097e+00, PNorm = 9.2277, GNorm = 0.0771, lr_0 = 8.1790e-04
Validation rmse = 4.990904
Epoch 5
Loss = 1.3096e+00, PNorm = 9.2276, GNorm = 3.3416, lr_0 = 7.4648e-04
Validation rmse = 4.984938
Epoch 6
Loss = 1.4367e+00, PNorm = 9.2275, GNorm = 2.6704, lr_0 = 6.8129e-04
Validation rmse = 4.981753
Epoch 7
Loss = 1.2156e+00, PNorm = 9.2276, GNorm = 0.5363, lr_0 = 6.2180e-04
Validation rmse = 4.975676
Epoch 8
Loss = 1.1603e+00, PNorm = 9.2277, GNorm = 3.3209, lr_0 = 5.6750e-04
Validation rmse = 4.970330
Epoch 9
Loss = 1.0332e+00, PNorm = 9.2278, GNorm = 3.7703, lr_0 = 5.1795e-04
Validation rmse = 4.963792
Epoch 10
Validation rmse = 4.964556
Epoch 11
Loss = 2.3837e+00, PNorm = 9.2280, GNorm = 3.7667, lr_0 = 4.7272e-04
Validation rmse = 4.962598
Epoch 12
Loss = 1.6209e+00, PNorm = 9.2282, GNorm = 3.3706, lr_0 = 4.3144e-04
Validation rmse = 4.955751
Epoch 13
Loss = 5.4568e-01, PNorm = 9.2284, GNorm = 3.5655, lr_0 = 3.9377e-04
Validation rmse = 4.954537
Epoch 14
Loss = 1.3735e+00, PNorm = 9.2287, GNorm = 3.3052, lr_0 = 3.5938e-04
Validation rmse = 4.950753
Epoch 15
Loss = 1.3506e+00, PNorm = 9.2289, GNorm = 3.7739, lr_0 = 3.2800e-04
Validation rmse = 4.946146
Epoch 16
Loss = 1.0056e+00, PNorm = 9.2292, GNorm = 3.7729, lr_0 = 2.9936e-04
Validation rmse = 4.946613
Epoch 17
Loss = 9.9673e-01, PNorm = 9.2294, GNorm = 3.7727, lr_0 = 2.7322e-04
Validation rmse = 4.945685
Epoch 18
Loss = 9.4664e-01, PNorm = 9.2296, GNorm = 0.0908, lr_0 = 2.4936e-04
Validation rmse = 4.944938
Epoch 19
Loss = 1.0139e+00, PNorm = 9.2299, GNorm = 0.5392, lr_0 = 2.2758e-04
Validation rmse = 4.942635
Epoch 20
Validation rmse = 4.939553
Epoch 21
Loss = 7.6564e-01, PNorm = 9.2300, GNorm = 2.4926, lr_0 = 2.0771e-04
Validation rmse = 4.937410
Epoch 22
Loss = 1.9729e+00, PNorm = 9.2302, GNorm = 3.5029, lr_0 = 1.8957e-04
Validation rmse = 4.938053
Epoch 23
Loss = 4.2622e-01, PNorm = 9.2304, GNorm = 1.6692, lr_0 = 1.7302e-04
Validation rmse = 4.936884
Epoch 24
Loss = 1.1914e+00, PNorm = 9.2306, GNorm = 3.4959, lr_0 = 1.5791e-04
Validation rmse = 4.936034
Epoch 25
Loss = 1.4328e+00, PNorm = 9.2307, GNorm = 3.7752, lr_0 = 1.4412e-04
Validation rmse = 4.934695
Epoch 26
Loss = 1.3187e+00, PNorm = 9.2309, GNorm = 2.4691, lr_0 = 1.3154e-04
Validation rmse = 4.934428
Epoch 27
Loss = 1.1874e+00, PNorm = 9.2310, GNorm = 3.3772, lr_0 = 1.2005e-04
Validation rmse = 4.933428
Epoch 28
Loss = 9.4117e-01, PNorm = 9.2311, GNorm = 3.3774, lr_0 = 1.0957e-04
Validation rmse = 4.932544
Epoch 29
Loss = 1.0065e+00, PNorm = 9.2312, GNorm = 0.5403, lr_0 = 1.0000e-04
Validation rmse = 4.932369
Model 0 best validation rmse = 4.932369 on epoch 29
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 2.022507
Ensemble test rmse = 2.022507
1-fold cross validation
Seed 0 ==> test rmse = 2.022507
Overall test rmse = 2.022507 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged_tester.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 12 | train size = 9 | val size = 1 | test size = 2
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (sigm): Sigmoid()
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Validation rmse = 4.633006
Epoch 1
Loss = 5.7082e-01, PNorm = 9.0208, GNorm = 2.5498, lr_0 = 6.0000e-04
Validation rmse = 4.640349
Epoch 2
Loss = 2.9549e-01, PNorm = 9.0201, GNorm = 0.5488, lr_0 = 9.8189e-04
Validation rmse = 4.640683
Epoch 3
Loss = 6.1049e-01, PNorm = 9.0197, GNorm = 2.6201, lr_0 = 8.9615e-04
Validation rmse = 4.636048
Epoch 4
Loss = 1.3507e+00, PNorm = 9.0204, GNorm = 0.5176, lr_0 = 8.1790e-04
Validation rmse = 4.659607
Epoch 5
Loss = 1.3342e+00, PNorm = 9.0215, GNorm = 4.1519, lr_0 = 7.4648e-04
Validation rmse = 4.665284
Epoch 6
Loss = 1.3971e+00, PNorm = 9.0229, GNorm = 2.6571, lr_0 = 6.8129e-04
Validation rmse = 4.672477
Epoch 7
Loss = 1.1893e+00, PNorm = 9.0243, GNorm = 0.9744, lr_0 = 6.2180e-04
Validation rmse = 4.671465
Epoch 8
Loss = 1.1025e+00, PNorm = 9.0259, GNorm = 4.1548, lr_0 = 5.6750e-04
Validation rmse = 4.672944
Epoch 9
Loss = 9.8625e-01, PNorm = 9.0275, GNorm = 5.9674, lr_0 = 5.1795e-04
Validation rmse = 4.668512
Epoch 10
Validation rmse = 4.681450
Epoch 11
Loss = 2.8384e+00, PNorm = 9.0295, GNorm = 5.9264, lr_0 = 4.7272e-04
Validation rmse = 4.685155
Epoch 12
Loss = 1.5246e+00, PNorm = 9.0310, GNorm = 4.7269, lr_0 = 4.3144e-04
Validation rmse = 4.678803
Epoch 13
Loss = 4.3116e-01, PNorm = 9.0324, GNorm = 4.2656, lr_0 = 3.9377e-04
Validation rmse = 4.681070
Epoch 14
Loss = 1.1778e+00, PNorm = 9.0339, GNorm = 4.1747, lr_0 = 3.5938e-04
Validation rmse = 4.679431
Epoch 15
Loss = 1.2671e+00, PNorm = 9.0353, GNorm = 5.9403, lr_0 = 3.2800e-04
Validation rmse = 4.676417
Epoch 16
Loss = 1.0543e+00, PNorm = 9.0366, GNorm = 5.9258, lr_0 = 2.9936e-04
Validation rmse = 4.682832
Epoch 17
Loss = 1.0144e+00, PNorm = 9.0378, GNorm = 5.9195, lr_0 = 2.7322e-04
Validation rmse = 4.685157
Epoch 18
Loss = 9.3999e-01, PNorm = 9.0389, GNorm = 0.4129, lr_0 = 2.4936e-04
Validation rmse = 4.689047
Epoch 19
Loss = 9.6660e-01, PNorm = 9.0400, GNorm = 0.9037, lr_0 = 2.2758e-04
Validation rmse = 4.688804
Epoch 20
Validation rmse = 4.686382
Epoch 21
Loss = 5.6960e-01, PNorm = 9.0409, GNorm = 2.6738, lr_0 = 2.0771e-04
Validation rmse = 4.684005
Epoch 22
Loss = 1.9997e+00, PNorm = 9.0416, GNorm = 4.2784, lr_0 = 1.8957e-04
Validation rmse = 4.688294
Epoch 23
Loss = 4.0200e-01, PNorm = 9.0424, GNorm = 2.6044, lr_0 = 1.7302e-04
Validation rmse = 4.688957
Epoch 24
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 1,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 10,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 10,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=10, bias=False)
      (W_h): Linear(in_features=10, out_features=10, bias=False)
      (W_o): Linear(in_features=113, out_features=10, bias=True)
    )
    (attention): Attention(
      (sigm): Sigmoid()
      (act_func): ReLU()
      (att_1): Linear(in_features=20, out_features=20, bias=True)
      (att_2): Linear(in_features=20, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=20, out_features=10, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Number of parameters = 3,072
Epoch 0
Loss = 1.0921e+00, PNorm = 9.1850, GNorm = 7.6981, lr_0 = 1.0095e-04
Loss = 9.4413e-01, PNorm = 9.1851, GNorm = 3.5651, lr_0 = 1.0189e-04
Loss = 6.3821e-01, PNorm = 9.1852, GNorm = 1.8979, lr_0 = 1.0284e-04
Loss = 1.5555e-01, PNorm = 9.1852, GNorm = 0.8288, lr_0 = 1.0379e-04
Loss = 9.2723e-01, PNorm = 9.1855, GNorm = 0.5087, lr_0 = 1.0473e-04
Loss = 4.1649e+00, PNorm = 9.1857, GNorm = 0.7600, lr_0 = 1.0568e-04
Loss = 1.9777e+00, PNorm = 9.1858, GNorm = 1.1552, lr_0 = 1.0663e-04
Loss = 2.0019e-01, PNorm = 9.1858, GNorm = 0.8840, lr_0 = 1.0758e-04
Loss = 8.5362e-01, PNorm = 9.1857, GNorm = 0.7635, lr_0 = 1.0852e-04
Loss = 9.1911e-01, PNorm = 9.1858, GNorm = 0.8910, lr_0 = 1.0947e-04
Loss = 3.9910e-01, PNorm = 9.1858, GNorm = 2.7817, lr_0 = 1.1042e-04
Loss = 6.1522e-01, PNorm = 9.1859, GNorm = 2.9359, lr_0 = 1.1136e-04
Loss = 3.6936e-01, PNorm = 9.1860, GNorm = 0.0234, lr_0 = 1.1231e-04
Loss = 8.8028e-01, PNorm = 9.1860, GNorm = 0.0727, lr_0 = 1.1326e-04
Loss = 7.1451e-01, PNorm = 9.1861, GNorm = 4.1886, lr_0 = 1.1420e-04
Loss = 1.0237e+00, PNorm = 9.1863, GNorm = 0.0251, lr_0 = 1.1515e-04
Loss = 1.5596e+00, PNorm = 9.1865, GNorm = 3.7132, lr_0 = 1.1610e-04
Loss = 1.7320e+00, PNorm = 9.1865, GNorm = 1.6122, lr_0 = 1.1705e-04
Loss = 7.8671e-01, PNorm = 9.1861, GNorm = 2.6316, lr_0 = 1.1799e-04
Loss = 7.5698e-01, PNorm = 9.1859, GNorm = 1.0420, lr_0 = 1.1894e-04
Loss = 5.2749e-01, PNorm = 9.1858, GNorm = 4.0313, lr_0 = 1.1989e-04
Loss = 1.2195e+00, PNorm = 9.1855, GNorm = 3.2653, lr_0 = 1.2083e-04
Loss = 4.0233e-01, PNorm = 9.1855, GNorm = 0.0459, lr_0 = 1.2178e-04
Loss = 3.6423e-01, PNorm = 9.1855, GNorm = 0.1045, lr_0 = 1.2273e-04
Loss = 1.5532e+00, PNorm = 9.1855, GNorm = 1.1487, lr_0 = 1.2367e-04
Loss = 3.0030e-01, PNorm = 9.1854, GNorm = 1.2446, lr_0 = 1.2462e-04
Loss = 6.3351e-01, PNorm = 9.1853, GNorm = 4.8605, lr_0 = 1.2557e-04
Loss = 4.5464e-01, PNorm = 9.1853, GNorm = 0.9355, lr_0 = 1.2652e-04
Loss = 5.7885e-01, PNorm = 9.1854, GNorm = 0.2397, lr_0 = 1.2746e-04
Loss = 7.0016e-01, PNorm = 9.1856, GNorm = 4.2349, lr_0 = 1.2841e-04
Loss = 1.3328e-01, PNorm = 9.1857, GNorm = 1.6087, lr_0 = 1.2936e-04
Loss = 7.9149e-01, PNorm = 9.1857, GNorm = 2.3072, lr_0 = 1.3030e-04
Loss = 7.7311e-01, PNorm = 9.1858, GNorm = 0.9285, lr_0 = 1.3125e-04
Loss = 3.5395e-01, PNorm = 9.1859, GNorm = 0.7422, lr_0 = 1.3220e-04
Loss = 1.1850e+00, PNorm = 9.1859, GNorm = 3.5140, lr_0 = 1.3314e-04
Loss = 1.3856e+00, PNorm = 9.1858, GNorm = 0.1759, lr_0 = 1.3409e-04
Loss = 2.4053e+00, PNorm = 9.1857, GNorm = 1.0758, lr_0 = 1.3504e-04
Loss = 2.0733e-01, PNorm = 9.1858, GNorm = 0.3118, lr_0 = 1.3598e-04
Loss = 9.5751e-01, PNorm = 9.1861, GNorm = 0.2188, lr_0 = 1.3693e-04
Loss = 5.9932e-01, PNorm = 9.1863, GNorm = 1.1012, lr_0 = 1.3788e-04
Loss = 8.9409e-01, PNorm = 9.1866, GNorm = 0.7413, lr_0 = 1.3883e-04
Loss = 6.1593e-01, PNorm = 9.1868, GNorm = 1.7463, lr_0 = 1.3977e-04
Loss = 2.6384e+00, PNorm = 9.1870, GNorm = 3.4243, lr_0 = 1.4072e-04
Loss = 7.8165e-01, PNorm = 9.1874, GNorm = 3.5495, lr_0 = 1.4167e-04
Loss = 8.6582e-01, PNorm = 9.1876, GNorm = 1.0637, lr_0 = 1.4261e-04
Loss = 8.5079e-01, PNorm = 9.1875, GNorm = 0.2121, lr_0 = 1.4356e-04
Loss = 4.7311e+00, PNorm = 9.1872, GNorm = 14.4074, lr_0 = 1.4451e-04
Loss = 6.8815e-01, PNorm = 9.1869, GNorm = 1.5423, lr_0 = 1.4545e-04
Loss = 1.2984e+00, PNorm = 9.1867, GNorm = 5.6943, lr_0 = 1.4640e-04
Loss = 7.4017e-01, PNorm = 9.1866, GNorm = 0.9317, lr_0 = 1.4735e-04
Loss = 2.6015e+00, PNorm = 9.1869, GNorm = 5.6439, lr_0 = 1.4830e-04
Loss = 3.6665e-01, PNorm = 9.1873, GNorm = 0.5299, lr_0 = 1.4924e-04
Loss = 1.1431e+00, PNorm = 9.1872, GNorm = 2.9637, lr_0 = 1.5019e-04
Loss = 5.2930e-01, PNorm = 9.1869, GNorm = 0.1653, lr_0 = 1.5114e-04
Loss = 3.6204e+00, PNorm = 9.1869, GNorm = 10.6145, lr_0 = 1.5208e-04
Loss = 1.6922e+00, PNorm = 9.1869, GNorm = 0.1815, lr_0 = 1.5303e-04
Loss = 4.7479e+00, PNorm = 9.1870, GNorm = 9.3829, lr_0 = 1.5398e-04
Loss = 1.1264e+00, PNorm = 9.1870, GNorm = 2.2376, lr_0 = 1.5492e-04
Loss = 9.4153e-01, PNorm = 9.1869, GNorm = 0.2529, lr_0 = 1.5587e-04
Loss = 6.5962e-01, PNorm = 9.1868, GNorm = 1.0709, lr_0 = 1.5682e-04
Loss = 1.2752e+00, PNorm = 9.1868, GNorm = 0.7789, lr_0 = 1.5777e-04
Loss = 8.1235e-01, PNorm = 9.1870, GNorm = 1.3248, lr_0 = 1.5871e-04
Loss = 7.1282e-01, PNorm = 9.1868, GNorm = 2.3568, lr_0 = 1.5966e-04
Loss = 2.2151e+00, PNorm = 9.1870, GNorm = 1.0416, lr_0 = 1.6061e-04
Loss = 1.5919e+00, PNorm = 9.1872, GNorm = 0.4677, lr_0 = 1.6155e-04
Loss = 2.3924e+00, PNorm = 9.1875, GNorm = 1.9994, lr_0 = 1.6250e-04
Loss = 3.3961e+00, PNorm = 9.1876, GNorm = 2.2745, lr_0 = 1.6345e-04
Loss = 1.4292e+00, PNorm = 9.1878, GNorm = 2.7378, lr_0 = 1.6439e-04
Loss = 5.2859e-01, PNorm = 9.1880, GNorm = 1.5057, lr_0 = 1.6534e-04
Loss = 2.9335e+00, PNorm = 9.1882, GNorm = 0.3686, lr_0 = 1.6629e-04
Loss = 1.2660e+00, PNorm = 9.1882, GNorm = 0.8087, lr_0 = 1.6723e-04
Loss = 9.3132e-01, PNorm = 9.1881, GNorm = 0.5881, lr_0 = 1.6818e-04
Loss = 6.6889e-01, PNorm = 9.1881, GNorm = 1.1365, lr_0 = 1.6913e-04
Loss = 2.2009e+00, PNorm = 9.1881, GNorm = 1.7624, lr_0 = 1.7008e-04
Loss = 4.3727e-01, PNorm = 9.1880, GNorm = 0.5667, lr_0 = 1.7102e-04
Loss = 8.5220e-01, PNorm = 9.1879, GNorm = 2.0266, lr_0 = 1.7197e-04
Loss = 5.2592e-01, PNorm = 9.1880, GNorm = 1.1110, lr_0 = 1.7292e-04
Loss = 7.2470e-01, PNorm = 9.1882, GNorm = 0.1790, lr_0 = 1.7386e-04
Loss = 1.0240e+00, PNorm = 9.1884, GNorm = 2.5377, lr_0 = 1.7481e-04
Loss = 3.0700e-01, PNorm = 9.1883, GNorm = 0.9757, lr_0 = 1.7576e-04
Loss = 1.1371e+00, PNorm = 9.1882, GNorm = 3.2058, lr_0 = 1.7670e-04
Loss = 8.4139e-01, PNorm = 9.1883, GNorm = 1.3201, lr_0 = 1.7765e-04
Loss = 6.2295e-01, PNorm = 9.1883, GNorm = 0.4235, lr_0 = 1.7860e-04
Loss = 2.4895e+00, PNorm = 9.1884, GNorm = 0.4830, lr_0 = 1.7955e-04
Loss = 2.5810e-01, PNorm = 9.1884, GNorm = 0.5216, lr_0 = 1.8049e-04
Loss = 5.1889e-01, PNorm = 9.1885, GNorm = 1.5723, lr_0 = 1.8144e-04
Loss = 4.0019e-01, PNorm = 9.1882, GNorm = 1.0643, lr_0 = 1.8239e-04
Loss = 8.8421e-01, PNorm = 9.1883, GNorm = 2.2319, lr_0 = 1.8333e-04
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (sigm): Sigmoid()
      (act_func): ReLU()
      (att_1): Linear(in_features=400, out_features=400, bias=True)
      (att_2): Linear(in_features=400, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=400, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 365,402
Epoch 0
Loss = 1.9787e-02, PNorm = 35.4585, GNorm = 1.7675, lr_0 = 1.4737e-04
Loss = 2.2658e-02, PNorm = 35.4627, GNorm = 1.8954, lr_0 = 1.9474e-04
Loss = 1.1865e-02, PNorm = 35.4735, GNorm = 1.5942, lr_0 = 2.4211e-04
Loss = 1.5146e-02, PNorm = 35.4880, GNorm = 1.5094, lr_0 = 2.8947e-04
Loss = 1.1665e-02, PNorm = 35.5044, GNorm = 1.4940, lr_0 = 3.3684e-04
Loss = 1.1739e-02, PNorm = 35.5220, GNorm = 2.5652, lr_0 = 3.8421e-04
Loss = 8.7564e-03, PNorm = 35.5404, GNorm = 2.0958, lr_0 = 4.3158e-04
Loss = 9.2871e-03, PNorm = 35.5637, GNorm = 2.2077, lr_0 = 4.7895e-04
Loss = 1.0705e-02, PNorm = 35.5826, GNorm = 3.5898, lr_0 = 5.2632e-04
Validation rmse = 1.469079
Epoch 1
Loss = 9.6593e-03, PNorm = 35.6064, GNorm = 3.7569, lr_0 = 5.7368e-04
Loss = 7.4888e-03, PNorm = 35.6264, GNorm = 3.5688, lr_0 = 6.2105e-04
Loss = 9.4373e-03, PNorm = 35.6489, GNorm = 4.2005, lr_0 = 6.6842e-04
Loss = 8.1409e-03, PNorm = 35.6713, GNorm = 2.3332, lr_0 = 7.1579e-04
Loss = 7.2558e-03, PNorm = 35.6910, GNorm = 2.0677, lr_0 = 7.6316e-04
Loss = 5.6599e-03, PNorm = 35.7093, GNorm = 1.6905, lr_0 = 8.1053e-04
Loss = 5.1796e-03, PNorm = 35.7320, GNorm = 0.7035, lr_0 = 8.5789e-04
Loss = 5.5004e-03, PNorm = 35.7636, GNorm = 1.0102, lr_0 = 9.0526e-04
Loss = 6.8447e-03, PNorm = 35.7900, GNorm = 3.3871, lr_0 = 9.5263e-04
Loss = 8.6439e-03, PNorm = 35.8110, GNorm = 3.9810, lr_0 = 1.0000e-03
Validation rmse = 1.626203
Epoch 2
Loss = 8.4306e-03, PNorm = 35.8398, GNorm = 9.0288, lr_0 = 9.9138e-04
Loss = 7.1937e-03, PNorm = 35.8705, GNorm = 0.8227, lr_0 = 9.8284e-04
Loss = 7.3144e-03, PNorm = 35.8969, GNorm = 2.3863, lr_0 = 9.7437e-04
Loss = 5.4609e-03, PNorm = 35.9227, GNorm = 0.7984, lr_0 = 9.6597e-04
Loss = 5.5107e-03, PNorm = 35.9464, GNorm = 4.7547, lr_0 = 9.5764e-04
Loss = 4.2887e-03, PNorm = 35.9695, GNorm = 1.0686, lr_0 = 9.4939e-04
Loss = 4.6985e-03, PNorm = 35.9884, GNorm = 1.8590, lr_0 = 9.4120e-04
Loss = 5.2592e-03, PNorm = 36.0083, GNorm = 3.5114, lr_0 = 9.3309e-04
Loss = 4.6647e-03, PNorm = 36.0319, GNorm = 2.9055, lr_0 = 9.2505e-04
Validation rmse = 1.214201
Epoch 3
Loss = 4.8304e-03, PNorm = 36.0546, GNorm = 1.9954, lr_0 = 9.1708e-04
Loss = 6.2896e-03, PNorm = 36.0831, GNorm = 5.1831, lr_0 = 9.0917e-04
Loss = 5.4577e-03, PNorm = 36.1102, GNorm = 2.5928, lr_0 = 9.0134e-04
Loss = 3.7856e-03, PNorm = 36.1367, GNorm = 1.0580, lr_0 = 8.9357e-04
Loss = 3.9308e-03, PNorm = 36.1550, GNorm = 1.1267, lr_0 = 8.8587e-04
Loss = 4.2371e-03, PNorm = 36.1739, GNorm = 1.0006, lr_0 = 8.7823e-04
Loss = 4.8141e-03, PNorm = 36.1900, GNorm = 3.4079, lr_0 = 8.7066e-04
Loss = 5.4529e-03, PNorm = 36.2117, GNorm = 3.0030, lr_0 = 8.6316e-04
Loss = 3.6097e-03, PNorm = 36.2342, GNorm = 2.8383, lr_0 = 8.5572e-04
Loss = 4.6643e-03, PNorm = 36.2525, GNorm = 2.4102, lr_0 = 8.4834e-04
Validation rmse = 1.092956
Epoch 4
Loss = 4.0351e-03, PNorm = 36.2698, GNorm = 2.1069, lr_0 = 8.4103e-04
Loss = 3.2686e-03, PNorm = 36.2891, GNorm = 1.7107, lr_0 = 8.3378e-04
Loss = 4.2534e-03, PNorm = 36.3088, GNorm = 1.1579, lr_0 = 8.2660e-04
Loss = 3.7957e-03, PNorm = 36.3313, GNorm = 0.8665, lr_0 = 8.1947e-04
Loss = 3.2419e-03, PNorm = 36.3491, GNorm = 2.2593, lr_0 = 8.1241e-04
Loss = 3.5584e-03, PNorm = 36.3663, GNorm = 1.5547, lr_0 = 8.0541e-04
Loss = 4.6654e-03, PNorm = 36.3854, GNorm = 1.0129, lr_0 = 7.9846e-04
Loss = 3.7660e-03, PNorm = 36.4077, GNorm = 1.6213, lr_0 = 7.9158e-04
Loss = 4.2526e-03, PNorm = 36.4253, GNorm = 0.9683, lr_0 = 7.8476e-04
Validation rmse = 1.033863
Epoch 5
Loss = 3.0013e-03, PNorm = 36.4462, GNorm = 0.7270, lr_0 = 7.7800e-04
Loss = 2.5423e-03, PNorm = 36.4627, GNorm = 1.5393, lr_0 = 7.7129e-04
Loss = 3.9018e-03, PNorm = 36.4787, GNorm = 1.6933, lr_0 = 7.6464e-04
Loss = 3.6527e-03, PNorm = 36.5023, GNorm = 1.0033, lr_0 = 7.5805e-04
Loss = 4.6321e-03, PNorm = 36.5243, GNorm = 2.2581, lr_0 = 7.5152e-04
Loss = 3.6437e-03, PNorm = 36.5377, GNorm = 0.7401, lr_0 = 7.4504e-04
Loss = 3.1210e-03, PNorm = 36.5518, GNorm = 1.4326, lr_0 = 7.3862e-04
Loss = 3.0928e-03, PNorm = 36.5733, GNorm = 0.6281, lr_0 = 7.3225e-04
Loss = 3.8014e-03, PNorm = 36.5941, GNorm = 1.4629, lr_0 = 7.2594e-04
Loss = 3.0679e-03, PNorm = 36.6149, GNorm = 2.6339, lr_0 = 7.1969e-04
Validation rmse = 0.969379
Epoch 6
Loss = 2.9213e-03, PNorm = 36.6314, GNorm = 0.9528, lr_0 = 7.1348e-04
Loss = 3.0275e-03, PNorm = 36.6499, GNorm = 0.5556, lr_0 = 7.0733e-04
Loss = 3.4269e-03, PNorm = 36.6656, GNorm = 2.4499, lr_0 = 7.0124e-04
Loss = 3.4571e-03, PNorm = 36.6818, GNorm = 0.8367, lr_0 = 6.9519e-04
Loss = 3.9824e-03, PNorm = 36.7024, GNorm = 1.3333, lr_0 = 6.8920e-04
Loss = 2.7152e-03, PNorm = 36.7185, GNorm = 1.4824, lr_0 = 6.8326e-04
Loss = 3.1114e-03, PNorm = 36.7361, GNorm = 2.5894, lr_0 = 6.7737e-04
Loss = 2.7939e-03, PNorm = 36.7560, GNorm = 1.8261, lr_0 = 6.7153e-04
Loss = 3.1617e-03, PNorm = 36.7729, GNorm = 1.1851, lr_0 = 6.6575e-04
Validation rmse = 0.953114
Epoch 7
Loss = 2.4928e-03, PNorm = 36.7868, GNorm = 1.3189, lr_0 = 6.6001e-04
Loss = 2.8442e-03, PNorm = 36.8003, GNorm = 1.7740, lr_0 = 6.5432e-04
Loss = 3.0954e-03, PNorm = 36.8186, GNorm = 1.9004, lr_0 = 6.4868e-04
Loss = 3.4158e-03, PNorm = 36.8348, GNorm = 4.4098, lr_0 = 6.4309e-04
Loss = 2.9103e-03, PNorm = 36.8498, GNorm = 3.1056, lr_0 = 6.3755e-04
Loss = 2.8267e-03, PNorm = 36.8653, GNorm = 3.7077, lr_0 = 6.3205e-04
Loss = 4.1067e-03, PNorm = 36.8813, GNorm = 3.5869, lr_0 = 6.2660e-04
Loss = 2.8471e-03, PNorm = 36.8990, GNorm = 1.7511, lr_0 = 6.2120e-04
Loss = 3.0544e-03, PNorm = 36.9193, GNorm = 3.7042, lr_0 = 6.1585e-04
Loss = 2.4335e-03, PNorm = 36.9346, GNorm = 2.1691, lr_0 = 6.1054e-04
Validation rmse = 0.940144
Epoch 8
Loss = 2.1504e-03, PNorm = 36.9504, GNorm = 0.8125, lr_0 = 6.0528e-04
Loss = 2.4810e-03, PNorm = 36.9640, GNorm = 1.6821, lr_0 = 6.0006e-04
Loss = 2.5927e-03, PNorm = 36.9754, GNorm = 1.0769, lr_0 = 5.9489e-04
Loss = 2.5645e-03, PNorm = 36.9864, GNorm = 1.6406, lr_0 = 5.8976e-04
Loss = 2.7921e-03, PNorm = 37.0009, GNorm = 2.4037, lr_0 = 5.8468e-04
Loss = 2.5360e-03, PNorm = 37.0160, GNorm = 0.8073, lr_0 = 5.7964e-04
Loss = 2.7080e-03, PNorm = 37.0297, GNorm = 0.9419, lr_0 = 5.7464e-04
Loss = 2.8549e-03, PNorm = 37.0439, GNorm = 1.5782, lr_0 = 5.6969e-04
Loss = 2.2778e-03, PNorm = 37.0591, GNorm = 0.5162, lr_0 = 5.6478e-04
Validation rmse = 0.877377
Epoch 9
Loss = 2.0753e-03, PNorm = 37.0709, GNorm = 1.1410, lr_0 = 5.5991e-04
Loss = 2.6608e-03, PNorm = 37.0829, GNorm = 0.9039, lr_0 = 5.5509e-04
Loss = 2.2898e-03, PNorm = 37.0963, GNorm = 0.5985, lr_0 = 5.5030e-04
Loss = 2.6292e-03, PNorm = 37.1080, GNorm = 1.3567, lr_0 = 5.4556e-04
Loss = 2.3399e-03, PNorm = 37.1213, GNorm = 0.8434, lr_0 = 5.4086e-04
Loss = 2.8155e-03, PNorm = 37.1355, GNorm = 1.1128, lr_0 = 5.3620e-04
Loss = 2.7703e-03, PNorm = 37.1483, GNorm = 0.6080, lr_0 = 5.3157e-04
Loss = 1.9440e-03, PNorm = 37.1608, GNorm = 1.3233, lr_0 = 5.2699e-04
Loss = 2.1435e-03, PNorm = 37.1696, GNorm = 1.0757, lr_0 = 5.2245e-04
Loss = 2.5445e-03, PNorm = 37.1840, GNorm = 0.3735, lr_0 = 5.1795e-04
Validation rmse = 0.892619
Epoch 10
Loss = 2.5005e-03, PNorm = 37.1985, GNorm = 1.9677, lr_0 = 5.1348e-04
Loss = 2.9890e-03, PNorm = 37.2121, GNorm = 0.6434, lr_0 = 5.0906e-04
Loss = 2.4755e-03, PNorm = 37.2260, GNorm = 1.1952, lr_0 = 5.0467e-04
Loss = 1.8990e-03, PNorm = 37.2355, GNorm = 0.7834, lr_0 = 5.0032e-04
Loss = 1.8808e-03, PNorm = 37.2463, GNorm = 0.4246, lr_0 = 4.9601e-04
Loss = 2.3756e-03, PNorm = 37.2593, GNorm = 1.6464, lr_0 = 4.9173e-04
Loss = 2.2842e-03, PNorm = 37.2691, GNorm = 2.7959, lr_0 = 4.8749e-04
Loss = 2.2025e-03, PNorm = 37.2803, GNorm = 1.2909, lr_0 = 4.8329e-04
Loss = 2.0802e-03, PNorm = 37.2886, GNorm = 1.7120, lr_0 = 4.7913e-04
Validation rmse = 0.861301
Epoch 11
Loss = 2.0447e-03, PNorm = 37.2965, GNorm = 3.6491, lr_0 = 4.7500e-04
Loss = 2.2211e-03, PNorm = 37.3064, GNorm = 1.1125, lr_0 = 4.7090e-04
Loss = 1.8794e-03, PNorm = 37.3163, GNorm = 1.2557, lr_0 = 4.6685e-04
Loss = 1.9245e-03, PNorm = 37.3245, GNorm = 1.5949, lr_0 = 4.6282e-04
Loss = 1.9456e-03, PNorm = 37.3335, GNorm = 1.3594, lr_0 = 4.5883e-04
Loss = 2.8251e-03, PNorm = 37.3441, GNorm = 1.3756, lr_0 = 4.5488e-04
Loss = 1.9525e-03, PNorm = 37.3558, GNorm = 0.5034, lr_0 = 4.5096e-04
Loss = 1.7651e-03, PNorm = 37.3657, GNorm = 2.1466, lr_0 = 4.4707e-04
Loss = 2.4658e-03, PNorm = 37.3754, GNorm = 1.3723, lr_0 = 4.4322e-04
Loss = 2.4599e-03, PNorm = 37.3855, GNorm = 1.8831, lr_0 = 4.3940e-04
Validation rmse = 0.851250
Epoch 12
Loss = 1.9205e-03, PNorm = 37.3959, GNorm = 0.5740, lr_0 = 4.3561e-04
Loss = 1.8232e-03, PNorm = 37.4076, GNorm = 0.4212, lr_0 = 4.3186e-04
Loss = 2.4239e-03, PNorm = 37.4136, GNorm = 1.0270, lr_0 = 4.2813e-04
Loss = 2.2614e-03, PNorm = 37.4226, GNorm = 0.9926, lr_0 = 4.2444e-04
Loss = 2.2570e-03, PNorm = 37.4307, GNorm = 0.5386, lr_0 = 4.2078e-04
Loss = 2.1530e-03, PNorm = 37.4386, GNorm = 0.8683, lr_0 = 4.1716e-04
Loss = 2.4418e-03, PNorm = 37.4475, GNorm = 0.7448, lr_0 = 4.1356e-04
Loss = 1.7888e-03, PNorm = 37.4562, GNorm = 0.8379, lr_0 = 4.1000e-04
Loss = 1.8483e-03, PNorm = 37.4645, GNorm = 0.5595, lr_0 = 4.0646e-04
Validation rmse = 0.894909
Epoch 13
Loss = 3.0912e-03, PNorm = 37.4754, GNorm = 0.7620, lr_0 = 4.0296e-04
Loss = 1.4017e-03, PNorm = 37.4845, GNorm = 0.7294, lr_0 = 3.9949e-04
Loss = 1.8990e-03, PNorm = 37.4928, GNorm = 0.7939, lr_0 = 3.9604e-04
Loss = 2.1804e-03, PNorm = 37.4982, GNorm = 0.6750, lr_0 = 3.9263e-04
Loss = 2.5811e-03, PNorm = 37.5063, GNorm = 1.1431, lr_0 = 3.8925e-04
Loss = 1.8884e-03, PNorm = 37.5158, GNorm = 0.8053, lr_0 = 3.8589e-04
Loss = 1.6981e-03, PNorm = 37.5219, GNorm = 1.1860, lr_0 = 3.8257e-04
Loss = 2.2948e-03, PNorm = 37.5295, GNorm = 1.5821, lr_0 = 3.7927e-04
Loss = 2.1494e-03, PNorm = 37.5391, GNorm = 1.2507, lr_0 = 3.7600e-04
Loss = 1.8100e-03, PNorm = 37.5469, GNorm = 0.8025, lr_0 = 3.7276e-04
Validation rmse = 0.846264
Epoch 14
Loss = 1.8050e-03, PNorm = 37.5565, GNorm = 0.7134, lr_0 = 3.6955e-04
Loss = 2.3751e-03, PNorm = 37.5646, GNorm = 0.9841, lr_0 = 3.6636e-04
Loss = 1.9934e-03, PNorm = 37.5738, GNorm = 1.1579, lr_0 = 3.6320e-04
Loss = 1.9211e-03, PNorm = 37.5805, GNorm = 0.5475, lr_0 = 3.6007e-04
Loss = 1.3579e-03, PNorm = 37.5871, GNorm = 1.1030, lr_0 = 3.5697e-04
Loss = 1.6488e-03, PNorm = 37.5952, GNorm = 2.2636, lr_0 = 3.5389e-04
Loss = 2.2542e-03, PNorm = 37.6028, GNorm = 1.3393, lr_0 = 3.5084e-04
Loss = 1.9728e-03, PNorm = 37.6092, GNorm = 0.7248, lr_0 = 3.4782e-04
Loss = 2.1176e-03, PNorm = 37.6155, GNorm = 1.0945, lr_0 = 3.4482e-04
Validation rmse = 0.812134
Epoch 15
Loss = 2.4932e-03, PNorm = 37.6215, GNorm = 1.1082, lr_0 = 3.4185e-04
Loss = 1.6536e-03, PNorm = 37.6265, GNorm = 1.2413, lr_0 = 3.3890e-04
Loss = 1.6471e-03, PNorm = 37.6309, GNorm = 0.9307, lr_0 = 3.3598e-04
Loss = 2.0698e-03, PNorm = 37.6379, GNorm = 1.9286, lr_0 = 3.3309e-04
Loss = 2.1890e-03, PNorm = 37.6444, GNorm = 1.0509, lr_0 = 3.3022e-04
Loss = 2.1417e-03, PNorm = 37.6521, GNorm = 3.2009, lr_0 = 3.2737e-04
Loss = 1.6589e-03, PNorm = 37.6601, GNorm = 1.0814, lr_0 = 3.2455e-04
Loss = 1.9993e-03, PNorm = 37.6663, GNorm = 1.6230, lr_0 = 3.2175e-04
Loss = 1.6489e-03, PNorm = 37.6730, GNorm = 0.5140, lr_0 = 3.1898e-04
Loss = 1.5772e-03, PNorm = 37.6807, GNorm = 0.4704, lr_0 = 3.1623e-04
Validation rmse = 0.785536
Epoch 16
Loss = 2.1394e-03, PNorm = 37.6893, GNorm = 1.2627, lr_0 = 3.1350e-04
Loss = 1.3134e-03, PNorm = 37.6965, GNorm = 0.3708, lr_0 = 3.1080e-04
Loss = 1.9358e-03, PNorm = 37.7034, GNorm = 0.8881, lr_0 = 3.0812e-04
Loss = 1.9965e-03, PNorm = 37.7082, GNorm = 1.2483, lr_0 = 3.0547e-04
Loss = 2.1707e-03, PNorm = 37.7133, GNorm = 0.8189, lr_0 = 3.0283e-04
Loss = 1.8918e-03, PNorm = 37.7189, GNorm = 2.0206, lr_0 = 3.0022e-04
Loss = 2.2232e-03, PNorm = 37.7263, GNorm = 0.5384, lr_0 = 2.9764e-04
Loss = 1.2944e-03, PNorm = 37.7330, GNorm = 0.8122, lr_0 = 2.9507e-04
Loss = 1.7401e-03, PNorm = 37.7393, GNorm = 0.6439, lr_0 = 2.9253e-04
Validation rmse = 0.795841
Epoch 17
Loss = 1.3837e-03, PNorm = 37.7444, GNorm = 0.7350, lr_0 = 2.9001e-04
Loss = 1.9767e-03, PNorm = 37.7495, GNorm = 0.5799, lr_0 = 2.8751e-04
Loss = 1.4278e-03, PNorm = 37.7541, GNorm = 2.6510, lr_0 = 2.8503e-04
Loss = 1.8040e-03, PNorm = 37.7599, GNorm = 1.1634, lr_0 = 2.8257e-04
Loss = 1.5939e-03, PNorm = 37.7645, GNorm = 1.0274, lr_0 = 2.8014e-04
Loss = 1.8410e-03, PNorm = 37.7689, GNorm = 1.0513, lr_0 = 2.7772e-04
Loss = 2.0255e-03, PNorm = 37.7765, GNorm = 1.5620, lr_0 = 2.7533e-04
Loss = 1.4196e-03, PNorm = 37.7815, GNorm = 1.0006, lr_0 = 2.7295e-04
Loss = 1.8489e-03, PNorm = 37.7868, GNorm = 0.6263, lr_0 = 2.7060e-04
Loss = 2.0020e-03, PNorm = 37.7933, GNorm = 0.5254, lr_0 = 2.6827e-04
Validation rmse = 0.795642
Epoch 18
Loss = 1.3716e-03, PNorm = 37.7966, GNorm = 0.3484, lr_0 = 2.6596e-04
Loss = 1.3092e-03, PNorm = 37.8002, GNorm = 1.2222, lr_0 = 2.6367e-04
Loss = 2.0331e-03, PNorm = 37.8045, GNorm = 0.8852, lr_0 = 2.6139e-04
Loss = 1.5855e-03, PNorm = 37.8096, GNorm = 0.9902, lr_0 = 2.5914e-04
Loss = 1.4092e-03, PNorm = 37.8153, GNorm = 0.4121, lr_0 = 2.5691e-04
Loss = 1.6570e-03, PNorm = 37.8210, GNorm = 0.5689, lr_0 = 2.5469e-04
Loss = 1.8405e-03, PNorm = 37.8259, GNorm = 1.3676, lr_0 = 2.5250e-04
Loss = 2.1656e-03, PNorm = 37.8322, GNorm = 0.6494, lr_0 = 2.5032e-04
Loss = 1.8748e-03, PNorm = 37.8377, GNorm = 1.6230, lr_0 = 2.4816e-04
Validation rmse = 0.784649
Epoch 19
Loss = 1.5838e-03, PNorm = 37.8417, GNorm = 0.8912, lr_0 = 2.4602e-04
Loss = 1.8705e-03, PNorm = 37.8465, GNorm = 1.1799, lr_0 = 2.4390e-04
Loss = 1.8681e-03, PNorm = 37.8501, GNorm = 0.5020, lr_0 = 2.4180e-04
Loss = 1.3914e-03, PNorm = 37.8536, GNorm = 1.1200, lr_0 = 2.3972e-04
Loss = 1.6613e-03, PNorm = 37.8578, GNorm = 0.9288, lr_0 = 2.3765e-04
Loss = 1.2923e-03, PNorm = 37.8621, GNorm = 1.3694, lr_0 = 2.3560e-04
Loss = 1.6832e-03, PNorm = 37.8659, GNorm = 1.8514, lr_0 = 2.3357e-04
Loss = 1.4659e-03, PNorm = 37.8705, GNorm = 0.8302, lr_0 = 2.3156e-04
Loss = 1.5272e-03, PNorm = 37.8755, GNorm = 0.7300, lr_0 = 2.2956e-04
Loss = 2.3445e-03, PNorm = 37.8814, GNorm = 0.9815, lr_0 = 2.2758e-04
Validation rmse = 0.773711
Epoch 20
Loss = 1.6852e-03, PNorm = 37.8870, GNorm = 1.1441, lr_0 = 2.2562e-04
Loss = 1.8313e-03, PNorm = 37.8929, GNorm = 0.8658, lr_0 = 2.2368e-04
Loss = 1.5217e-03, PNorm = 37.8959, GNorm = 0.9872, lr_0 = 2.2175e-04
Loss = 2.2216e-03, PNorm = 37.9001, GNorm = 1.0237, lr_0 = 2.1984e-04
Loss = 1.7994e-03, PNorm = 37.9044, GNorm = 1.1096, lr_0 = 2.1794e-04
Loss = 1.4335e-03, PNorm = 37.9091, GNorm = 0.6537, lr_0 = 2.1607e-04
Loss = 1.3378e-03, PNorm = 37.9121, GNorm = 1.0182, lr_0 = 2.1420e-04
Loss = 1.5709e-03, PNorm = 37.9164, GNorm = 1.7973, lr_0 = 2.1236e-04
Loss = 1.5046e-03, PNorm = 37.9199, GNorm = 0.4672, lr_0 = 2.1053e-04
Validation rmse = 0.777180
Epoch 21
Loss = 1.5846e-03, PNorm = 37.9252, GNorm = 0.7846, lr_0 = 2.0871e-04
Loss = 1.7613e-03, PNorm = 37.9292, GNorm = 1.5379, lr_0 = 2.0691e-04
Loss = 1.7893e-03, PNorm = 37.9329, GNorm = 0.8524, lr_0 = 2.0513e-04
Loss = 1.6576e-03, PNorm = 37.9356, GNorm = 0.4735, lr_0 = 2.0336e-04
Loss = 1.4876e-03, PNorm = 37.9392, GNorm = 1.1470, lr_0 = 2.0161e-04
Loss = 1.4429e-03, PNorm = 37.9420, GNorm = 0.6932, lr_0 = 1.9987e-04
Loss = 1.2731e-03, PNorm = 37.9459, GNorm = 1.6396, lr_0 = 1.9815e-04
Loss = 1.9376e-03, PNorm = 37.9497, GNorm = 0.9729, lr_0 = 1.9644e-04
Loss = 1.7046e-03, PNorm = 37.9548, GNorm = 1.0115, lr_0 = 1.9475e-04
Loss = 1.5057e-03, PNorm = 37.9590, GNorm = 0.5732, lr_0 = 1.9307e-04
Validation rmse = 0.769731
Epoch 22
Loss = 1.6768e-03, PNorm = 37.9631, GNorm = 0.8240, lr_0 = 1.9141e-04
Loss = 1.3110e-03, PNorm = 37.9674, GNorm = 0.5016, lr_0 = 1.8976e-04
Loss = 1.4764e-03, PNorm = 37.9695, GNorm = 0.6011, lr_0 = 1.8812e-04
Loss = 1.2686e-03, PNorm = 37.9725, GNorm = 0.6725, lr_0 = 1.8650e-04
Loss = 1.4634e-03, PNorm = 37.9771, GNorm = 0.7925, lr_0 = 1.8489e-04
Loss = 1.8435e-03, PNorm = 37.9817, GNorm = 0.8560, lr_0 = 1.8330e-04
Loss = 1.9697e-03, PNorm = 37.9856, GNorm = 0.4893, lr_0 = 1.8172e-04
Loss = 1.4640e-03, PNorm = 37.9883, GNorm = 0.5876, lr_0 = 1.8015e-04
Loss = 1.3371e-03, PNorm = 37.9912, GNorm = 0.7603, lr_0 = 1.7860e-04
Validation rmse = 0.756142
Epoch 23
Loss = 1.9552e-03, PNorm = 37.9943, GNorm = 1.9045, lr_0 = 1.7706e-04
Loss = 1.9332e-03, PNorm = 37.9979, GNorm = 0.7654, lr_0 = 1.7553e-04
Loss = 1.3707e-03, PNorm = 38.0015, GNorm = 1.8017, lr_0 = 1.7402e-04
Loss = 1.8594e-03, PNorm = 38.0034, GNorm = 1.1127, lr_0 = 1.7252e-04
Loss = 1.2680e-03, PNorm = 38.0046, GNorm = 1.1207, lr_0 = 1.7103e-04
Loss = 1.1030e-03, PNorm = 38.0070, GNorm = 0.7445, lr_0 = 1.6956e-04
Loss = 1.4267e-03, PNorm = 38.0102, GNorm = 0.6670, lr_0 = 1.6810e-04
Loss = 1.7511e-03, PNorm = 38.0137, GNorm = 1.1016, lr_0 = 1.6665e-04
Loss = 1.1527e-03, PNorm = 38.0172, GNorm = 0.4836, lr_0 = 1.6521e-04
Loss = 1.7219e-03, PNorm = 38.0216, GNorm = 0.7952, lr_0 = 1.6379e-04
Validation rmse = 0.767225
Epoch 24
Loss = 1.2950e-03, PNorm = 38.0256, GNorm = 1.8650, lr_0 = 1.6238e-04
Loss = 1.4120e-03, PNorm = 38.0289, GNorm = 0.7233, lr_0 = 1.6098e-04
Loss = 1.3593e-03, PNorm = 38.0315, GNorm = 0.8068, lr_0 = 1.5959e-04
Loss = 1.7918e-03, PNorm = 38.0332, GNorm = 0.7257, lr_0 = 1.5822e-04
Loss = 1.7943e-03, PNorm = 38.0366, GNorm = 0.6460, lr_0 = 1.5685e-04
Loss = 1.4804e-03, PNorm = 38.0392, GNorm = 1.0139, lr_0 = 1.5550e-04
Loss = 1.3689e-03, PNorm = 38.0426, GNorm = 0.5448, lr_0 = 1.5416e-04
Loss = 1.6153e-03, PNorm = 38.0463, GNorm = 1.0646, lr_0 = 1.5283e-04
Loss = 1.3909e-03, PNorm = 38.0492, GNorm = 0.7346, lr_0 = 1.5151e-04
Validation rmse = 0.742591
Epoch 25
Loss = 1.5548e-03, PNorm = 38.0518, GNorm = 1.9348, lr_0 = 1.5021e-04
Loss = 1.1745e-03, PNorm = 38.0535, GNorm = 0.8979, lr_0 = 1.4891e-04
Loss = 1.9523e-03, PNorm = 38.0564, GNorm = 2.0017, lr_0 = 1.4763e-04
Loss = 1.2739e-03, PNorm = 38.0604, GNorm = 0.4560, lr_0 = 1.4636e-04
Loss = 1.8196e-03, PNorm = 38.0634, GNorm = 0.6909, lr_0 = 1.4510e-04
Loss = 1.5189e-03, PNorm = 38.0654, GNorm = 0.6218, lr_0 = 1.4384e-04
Loss = 1.5501e-03, PNorm = 38.0672, GNorm = 0.4527, lr_0 = 1.4261e-04
Loss = 1.3764e-03, PNorm = 38.0690, GNorm = 0.4177, lr_0 = 1.4138e-04
Loss = 1.3850e-03, PNorm = 38.0715, GNorm = 0.5756, lr_0 = 1.4016e-04
Loss = 1.5438e-03, PNorm = 38.0740, GNorm = 0.8415, lr_0 = 1.3895e-04
Validation rmse = 0.750029
Epoch 26
Loss = 1.2191e-03, PNorm = 38.0769, GNorm = 0.4596, lr_0 = 1.3775e-04
Loss = 1.4607e-03, PNorm = 38.0792, GNorm = 1.1447, lr_0 = 1.3656e-04
Loss = 2.2098e-03, PNorm = 38.0815, GNorm = 0.7089, lr_0 = 1.3539e-04
Loss = 1.5629e-03, PNorm = 38.0855, GNorm = 0.6864, lr_0 = 1.3422e-04
Loss = 1.7166e-03, PNorm = 38.0881, GNorm = 0.3979, lr_0 = 1.3306e-04
Loss = 1.1869e-03, PNorm = 38.0901, GNorm = 1.1550, lr_0 = 1.3192e-04
Loss = 1.2126e-03, PNorm = 38.0913, GNorm = 0.4957, lr_0 = 1.3078e-04
Loss = 1.0597e-03, PNorm = 38.0928, GNorm = 0.5121, lr_0 = 1.2965e-04
Loss = 1.3032e-03, PNorm = 38.0945, GNorm = 1.0045, lr_0 = 1.2854e-04
Validation rmse = 0.751247
Epoch 27
Loss = 2.2495e-03, PNorm = 38.0967, GNorm = 0.6056, lr_0 = 1.2743e-04
Loss = 1.7460e-03, PNorm = 38.0992, GNorm = 1.4818, lr_0 = 1.2633e-04
Loss = 1.3138e-03, PNorm = 38.1014, GNorm = 2.1977, lr_0 = 1.2524e-04
Loss = 1.2719e-03, PNorm = 38.1039, GNorm = 0.6170, lr_0 = 1.2416e-04
Loss = 1.2157e-03, PNorm = 38.1063, GNorm = 0.4684, lr_0 = 1.2309e-04
Loss = 1.3961e-03, PNorm = 38.1090, GNorm = 2.0465, lr_0 = 1.2203e-04
Loss = 1.4291e-03, PNorm = 38.1118, GNorm = 1.2200, lr_0 = 1.2098e-04
Loss = 1.3458e-03, PNorm = 38.1148, GNorm = 1.1858, lr_0 = 1.1994e-04
Loss = 1.3812e-03, PNorm = 38.1178, GNorm = 0.8611, lr_0 = 1.1890e-04
Loss = 1.8114e-03, PNorm = 38.1193, GNorm = 1.5092, lr_0 = 1.1788e-04
Validation rmse = 0.754164
Epoch 28
Loss = 1.4030e-03, PNorm = 38.1216, GNorm = 0.5813, lr_0 = 1.1686e-04
Loss = 1.2484e-03, PNorm = 38.1237, GNorm = 0.9214, lr_0 = 1.1585e-04
Loss = 1.4268e-03, PNorm = 38.1262, GNorm = 0.9217, lr_0 = 1.1486e-04
Loss = 1.4699e-03, PNorm = 38.1284, GNorm = 1.4630, lr_0 = 1.1387e-04
Loss = 1.0241e-03, PNorm = 38.1302, GNorm = 0.7768, lr_0 = 1.1288e-04
Loss = 1.4259e-03, PNorm = 38.1322, GNorm = 1.1727, lr_0 = 1.1191e-04
Loss = 1.8433e-03, PNorm = 38.1334, GNorm = 1.4884, lr_0 = 1.1095e-04
Loss = 1.2105e-03, PNorm = 38.1354, GNorm = 0.8232, lr_0 = 1.0999e-04
Loss = 2.0346e-03, PNorm = 38.1372, GNorm = 0.5657, lr_0 = 1.0904e-04
Validation rmse = 0.735592
Epoch 29
Loss = 1.0696e-03, PNorm = 38.1386, GNorm = 1.6241, lr_0 = 1.0810e-04
Loss = 1.4008e-03, PNorm = 38.1397, GNorm = 0.7781, lr_0 = 1.0717e-04
Loss = 1.5346e-03, PNorm = 38.1416, GNorm = 1.6453, lr_0 = 1.0625e-04
Loss = 1.5056e-03, PNorm = 38.1436, GNorm = 0.4554, lr_0 = 1.0533e-04
Loss = 1.2560e-03, PNorm = 38.1450, GNorm = 0.7179, lr_0 = 1.0442e-04
Loss = 1.5357e-03, PNorm = 38.1472, GNorm = 0.8980, lr_0 = 1.0352e-04
Loss = 1.6124e-03, PNorm = 38.1496, GNorm = 2.0938, lr_0 = 1.0263e-04
Loss = 1.4799e-03, PNorm = 38.1523, GNorm = 2.2539, lr_0 = 1.0175e-04
Loss = 1.6322e-03, PNorm = 38.1549, GNorm = 1.7067, lr_0 = 1.0087e-04
Loss = 1.2114e-03, PNorm = 38.1568, GNorm = 0.3968, lr_0 = 1.0000e-04
Validation rmse = 0.727217
Model 0 best validation rmse = 0.727217 on epoch 29
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 0.842880
Ensemble test rmse = 0.842880
1-fold cross validation
Seed 0 ==> test rmse = 0.842880
Overall test rmse = 0.842880 +/- 0.000000
Fold 0
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': False,
 'data_path': 'data/solvation/MNSol_compsol_cleaned_averaged.csv',
 'dataset_type': 'regression',
 'depth': 5,
 'detailed_results': False,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'explicit_solvent_number': 2,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 200,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 200,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': False,
 'save_dir': 'examples/solvation_attention/fold_0',
 'save_inchi_split': False,
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'solvation': True,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'random',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 1
Splitting data with seed 0
Total size = 5,940 | train size = 4,752 | val size = 594 | test size = 594
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN_solvation_attention(
    (encoder): MPNEncoder_attention(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=117, out_features=200, bias=False)
      (W_h): Linear(in_features=200, out_features=200, bias=False)
      (W_o): Linear(in_features=303, out_features=200, bias=True)
    )
    (attention): Attention(
      (sigm): Sigmoid()
      (act_func): ReLU()
      (att_1): Linear(in_features=400, out_features=400, bias=True)
      (att_2): Linear(in_features=400, out_features=1, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=800, out_features=200, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=200, out_features=1, bias=True)
  )
)
Number of parameters = 445,402
Epoch 0
Loss = 1.6920e-02, PNorm = 36.1975, GNorm = 5.9726, lr_0 = 1.4737e-04
Loss = 1.5916e-02, PNorm = 36.2044, GNorm = 1.8171, lr_0 = 1.9474e-04
Loss = 8.7019e-03, PNorm = 36.2111, GNorm = 5.3693, lr_0 = 2.4211e-04
Loss = 1.1694e-02, PNorm = 36.2207, GNorm = 3.5391, lr_0 = 2.8947e-04
Loss = 8.6433e-03, PNorm = 36.2318, GNorm = 3.0563, lr_0 = 3.3684e-04
Loss = 8.3215e-03, PNorm = 36.2476, GNorm = 3.0282, lr_0 = 3.8421e-04
Loss = 6.4396e-03, PNorm = 36.2640, GNorm = 2.8318, lr_0 = 4.3158e-04
Loss = 5.5477e-03, PNorm = 36.2890, GNorm = 3.7032, lr_0 = 4.7895e-04
Loss = 6.7049e-03, PNorm = 36.3108, GNorm = 3.6757, lr_0 = 5.2632e-04
Validation rmse = 1.552231
Epoch 1
Loss = 6.6223e-03, PNorm = 36.3384, GNorm = 5.5202, lr_0 = 5.7368e-04
Loss = 4.2407e-03, PNorm = 36.3697, GNorm = 2.7163, lr_0 = 6.2105e-04
Loss = 4.3222e-03, PNorm = 36.4019, GNorm = 1.0659, lr_0 = 6.6842e-04
Loss = 4.5380e-03, PNorm = 36.4285, GNorm = 6.8025, lr_0 = 7.1579e-04
Loss = 4.5319e-03, PNorm = 36.4657, GNorm = 2.7262, lr_0 = 7.6316e-04
Loss = 3.4118e-03, PNorm = 36.4967, GNorm = 3.5329, lr_0 = 8.1053e-04
Loss = 2.7664e-03, PNorm = 36.5315, GNorm = 1.4857, lr_0 = 8.5789e-04
Loss = 2.8799e-03, PNorm = 36.5686, GNorm = 4.2952, lr_0 = 9.0526e-04
Loss = 3.3100e-03, PNorm = 36.5996, GNorm = 1.0608, lr_0 = 9.5263e-04
Loss = 4.4904e-03, PNorm = 36.6512, GNorm = 3.2510, lr_0 = 1.0000e-03
Validation rmse = 1.090626
Epoch 2
Loss = 3.6465e-03, PNorm = 36.7189, GNorm = 5.5754, lr_0 = 9.9138e-04
Loss = 2.5711e-03, PNorm = 36.7575, GNorm = 3.4782, lr_0 = 9.8284e-04
Loss = 3.1172e-03, PNorm = 36.8065, GNorm = 2.9209, lr_0 = 9.7437e-04
Loss = 2.2665e-03, PNorm = 36.8414, GNorm = 1.9685, lr_0 = 9.6597e-04
Loss = 2.6300e-03, PNorm = 36.8669, GNorm = 3.6885, lr_0 = 9.5764e-04
Loss = 1.5849e-03, PNorm = 36.8992, GNorm = 2.8256, lr_0 = 9.4939e-04
Loss = 2.0321e-03, PNorm = 36.9264, GNorm = 1.9458, lr_0 = 9.4120e-04
Loss = 2.1301e-03, PNorm = 36.9425, GNorm = 1.0350, lr_0 = 9.3309e-04
Loss = 1.8006e-03, PNorm = 36.9638, GNorm = 1.2368, lr_0 = 9.2505e-04
Validation rmse = 0.763797
Epoch 3
Loss = 1.5726e-03, PNorm = 36.9984, GNorm = 1.1919, lr_0 = 9.1708e-04
Loss = 1.1240e-03, PNorm = 37.0288, GNorm = 0.8890, lr_0 = 9.0917e-04
Loss = 1.7347e-03, PNorm = 37.0501, GNorm = 1.3209, lr_0 = 9.0134e-04
Loss = 1.3648e-03, PNorm = 37.0609, GNorm = 1.6425, lr_0 = 8.9357e-04
Loss = 1.1995e-03, PNorm = 37.0790, GNorm = 0.7177, lr_0 = 8.8587e-04
Loss = 2.0342e-03, PNorm = 37.0941, GNorm = 1.4774, lr_0 = 8.7823e-04
Loss = 2.1168e-03, PNorm = 37.1144, GNorm = 3.0425, lr_0 = 8.7066e-04
Loss = 2.1710e-03, PNorm = 37.1521, GNorm = 1.1091, lr_0 = 8.6316e-04
Loss = 2.0503e-03, PNorm = 37.1720, GNorm = 1.9964, lr_0 = 8.5572e-04
Loss = 1.5705e-03, PNorm = 37.1966, GNorm = 0.8901, lr_0 = 8.4834e-04
Validation rmse = 0.685985
Epoch 4
Loss = 1.5936e-03, PNorm = 37.2172, GNorm = 1.8241, lr_0 = 8.4103e-04
Loss = 1.1359e-03, PNorm = 37.2415, GNorm = 0.5209, lr_0 = 8.3378e-04
Loss = 1.2908e-03, PNorm = 37.2631, GNorm = 0.7008, lr_0 = 8.2660e-04
Loss = 1.3172e-03, PNorm = 37.2862, GNorm = 0.5990, lr_0 = 8.1947e-04
Loss = 1.0284e-03, PNorm = 37.3059, GNorm = 0.6936, lr_0 = 8.1241e-04
Loss = 1.1772e-03, PNorm = 37.3204, GNorm = 1.5525, lr_0 = 8.0541e-04
Loss = 1.1081e-03, PNorm = 37.3432, GNorm = 0.8046, lr_0 = 7.9846e-04
Loss = 1.1619e-03, PNorm = 37.3657, GNorm = 1.4681, lr_0 = 7.9158e-04
Loss = 1.6413e-03, PNorm = 37.3828, GNorm = 1.9900, lr_0 = 7.8476e-04
Validation rmse = 0.725855
Epoch 5
Loss = 1.1197e-03, PNorm = 37.3999, GNorm = 1.2409, lr_0 = 7.7800e-04
Loss = 8.2692e-04, PNorm = 37.4179, GNorm = 0.8635, lr_0 = 7.7129e-04
Loss = 1.0499e-03, PNorm = 37.4380, GNorm = 0.7425, lr_0 = 7.6464e-04
Loss = 1.2182e-03, PNorm = 37.4671, GNorm = 2.2924, lr_0 = 7.5805e-04
Loss = 1.3953e-03, PNorm = 37.4825, GNorm = 2.5294, lr_0 = 7.5152e-04
Loss = 1.1471e-03, PNorm = 37.5016, GNorm = 0.4492, lr_0 = 7.4504e-04
Loss = 7.3129e-04, PNorm = 37.5178, GNorm = 0.7290, lr_0 = 7.3862e-04
Loss = 1.0531e-03, PNorm = 37.5286, GNorm = 1.5707, lr_0 = 7.3225e-04
Loss = 1.4460e-03, PNorm = 37.5421, GNorm = 1.3478, lr_0 = 7.2594e-04
Loss = 8.2227e-04, PNorm = 37.5595, GNorm = 0.7903, lr_0 = 7.1969e-04
Validation rmse = 0.561857
Epoch 6
Loss = 9.0067e-04, PNorm = 37.5765, GNorm = 1.0081, lr_0 = 7.1348e-04
Loss = 6.4053e-04, PNorm = 37.5898, GNorm = 0.7478, lr_0 = 7.0733e-04
Loss = 7.5610e-04, PNorm = 37.6050, GNorm = 1.1736, lr_0 = 7.0124e-04
Loss = 9.4920e-04, PNorm = 37.6158, GNorm = 1.1631, lr_0 = 6.9519e-04
Loss = 9.5771e-04, PNorm = 37.6337, GNorm = 1.4030, lr_0 = 6.8920e-04
Loss = 7.3424e-04, PNorm = 37.6437, GNorm = 1.6158, lr_0 = 6.8326e-04
Loss = 9.8587e-04, PNorm = 37.6561, GNorm = 2.1405, lr_0 = 6.7737e-04
Loss = 6.2855e-04, PNorm = 37.6700, GNorm = 1.0215, lr_0 = 6.7153e-04
Loss = 1.0860e-03, PNorm = 37.6832, GNorm = 0.7300, lr_0 = 6.6575e-04
Validation rmse = 0.546302
Epoch 7
Loss = 6.7691e-04, PNorm = 37.6916, GNorm = 0.8418, lr_0 = 6.6001e-04
Loss = 9.1090e-04, PNorm = 37.7019, GNorm = 0.9087, lr_0 = 6.5432e-04
Loss = 5.8647e-04, PNorm = 37.7156, GNorm = 0.8583, lr_0 = 6.4868e-04
Loss = 9.3679e-04, PNorm = 37.7300, GNorm = 2.3440, lr_0 = 6.4309e-04
Loss = 7.7722e-04, PNorm = 37.7430, GNorm = 0.6618, lr_0 = 6.3755e-04
Loss = 9.2213e-04, PNorm = 37.7566, GNorm = 0.6983, lr_0 = 6.3205e-04
Loss = 9.8290e-04, PNorm = 37.7756, GNorm = 1.7140, lr_0 = 6.2660e-04
Loss = 7.2196e-04, PNorm = 37.7891, GNorm = 0.5611, lr_0 = 6.2120e-04
Loss = 6.8052e-04, PNorm = 37.8004, GNorm = 1.5923, lr_0 = 6.1585e-04
Loss = 7.0577e-04, PNorm = 37.8137, GNorm = 0.8906, lr_0 = 6.1054e-04
Validation rmse = 0.533784
Epoch 8
Loss = 4.7204e-04, PNorm = 37.8260, GNorm = 0.3727, lr_0 = 6.0528e-04
Loss = 4.7086e-04, PNorm = 37.8318, GNorm = 0.8141, lr_0 = 6.0006e-04
Loss = 8.5336e-04, PNorm = 37.8370, GNorm = 0.7469, lr_0 = 5.9489e-04
Loss = 6.2260e-04, PNorm = 37.8460, GNorm = 1.2247, lr_0 = 5.8976e-04
Loss = 5.4010e-04, PNorm = 37.8620, GNorm = 0.6418, lr_0 = 5.8468e-04
Loss = 6.9352e-04, PNorm = 37.8733, GNorm = 0.7881, lr_0 = 5.7964e-04
Loss = 8.9980e-04, PNorm = 37.8869, GNorm = 0.8537, lr_0 = 5.7464e-04
Loss = 7.7368e-04, PNorm = 37.9031, GNorm = 1.3981, lr_0 = 5.6969e-04
Loss = 7.6986e-04, PNorm = 37.9152, GNorm = 0.7694, lr_0 = 5.6478e-04
Validation rmse = 0.617080
Epoch 9
Loss = 6.2008e-04, PNorm = 37.9251, GNorm = 1.1143, lr_0 = 5.5991e-04
Loss = 5.4531e-04, PNorm = 37.9360, GNorm = 0.7951, lr_0 = 5.5509e-04
Loss = 7.1751e-04, PNorm = 37.9502, GNorm = 0.8772, lr_0 = 5.5030e-04
Loss = 7.9800e-04, PNorm = 37.9605, GNorm = 0.6087, lr_0 = 5.4556e-04
Loss = 7.4945e-04, PNorm = 37.9732, GNorm = 1.1093, lr_0 = 5.4086e-04
Loss = 9.4650e-04, PNorm = 37.9791, GNorm = 2.1783, lr_0 = 5.3620e-04
Loss = 1.2570e-03, PNorm = 37.9907, GNorm = 1.8994, lr_0 = 5.3157e-04
Loss = 5.9964e-04, PNorm = 38.0081, GNorm = 1.4271, lr_0 = 5.2699e-04
Loss = 5.2471e-04, PNorm = 38.0197, GNorm = 1.3862, lr_0 = 5.2245e-04
Loss = 5.1967e-04, PNorm = 38.0360, GNorm = 0.7116, lr_0 = 5.1795e-04
Validation rmse = 0.493004
Epoch 10
Loss = 3.9998e-04, PNorm = 38.0461, GNorm = 0.6303, lr_0 = 5.1348e-04
Loss = 5.9283e-04, PNorm = 38.0564, GNorm = 0.5859, lr_0 = 5.0906e-04
Loss = 5.6028e-04, PNorm = 38.0659, GNorm = 1.0202, lr_0 = 5.0467e-04
Loss = 6.5529e-04, PNorm = 38.0738, GNorm = 0.7663, lr_0 = 5.0032e-04
Loss = 5.0121e-04, PNorm = 38.0773, GNorm = 0.2928, lr_0 = 4.9601e-04
Loss = 5.9314e-04, PNorm = 38.0870, GNorm = 1.8689, lr_0 = 4.9173e-04
Loss = 5.6451e-04, PNorm = 38.1002, GNorm = 1.6172, lr_0 = 4.8749e-04
Loss = 7.8122e-04, PNorm = 38.1105, GNorm = 1.4507, lr_0 = 4.8329e-04
Loss = 5.4879e-04, PNorm = 38.1158, GNorm = 1.1431, lr_0 = 4.7913e-04
Validation rmse = 0.534404
Epoch 11
Loss = 6.0903e-04, PNorm = 38.1223, GNorm = 1.8258, lr_0 = 4.7500e-04
Loss = 5.7195e-04, PNorm = 38.1335, GNorm = 0.8636, lr_0 = 4.7090e-04
Loss = 5.1516e-04, PNorm = 38.1426, GNorm = 0.9304, lr_0 = 4.6685e-04
Loss = 4.8144e-04, PNorm = 38.1506, GNorm = 0.7774, lr_0 = 4.6282e-04
Loss = 3.6145e-04, PNorm = 38.1598, GNorm = 0.2491, lr_0 = 4.5883e-04
Loss = 3.9348e-04, PNorm = 38.1699, GNorm = 0.2604, lr_0 = 4.5488e-04
Loss = 4.8386e-04, PNorm = 38.1776, GNorm = 0.6290, lr_0 = 4.5096e-04
Loss = 5.5470e-04, PNorm = 38.1823, GNorm = 0.8869, lr_0 = 4.4707e-04
Loss = 8.0042e-04, PNorm = 38.1896, GNorm = 0.8385, lr_0 = 4.4322e-04
Loss = 4.7674e-04, PNorm = 38.1982, GNorm = 0.4285, lr_0 = 4.3940e-04
Validation rmse = 0.506863
Epoch 12
Loss = 3.8277e-04, PNorm = 38.2014, GNorm = 0.3451, lr_0 = 4.3561e-04
Loss = 4.4641e-04, PNorm = 38.2116, GNorm = 0.5757, lr_0 = 4.3186e-04
Loss = 8.9730e-04, PNorm = 38.2210, GNorm = 0.8645, lr_0 = 4.2813e-04
Loss = 6.9628e-04, PNorm = 38.2272, GNorm = 0.4256, lr_0 = 4.2444e-04
Loss = 5.0860e-04, PNorm = 38.2363, GNorm = 0.6072, lr_0 = 4.2078e-04
Loss = 4.0702e-04, PNorm = 38.2477, GNorm = 0.7618, lr_0 = 4.1716e-04
Loss = 5.6179e-04, PNorm = 38.2539, GNorm = 0.5170, lr_0 = 4.1356e-04
Loss = 4.0432e-04, PNorm = 38.2573, GNorm = 1.3382, lr_0 = 4.1000e-04
Loss = 3.7476e-04, PNorm = 38.2665, GNorm = 0.7454, lr_0 = 4.0646e-04
Validation rmse = 0.517827
Epoch 13
Loss = 9.8865e-04, PNorm = 38.2763, GNorm = 0.4678, lr_0 = 4.0296e-04
Loss = 2.3090e-04, PNorm = 38.2820, GNorm = 0.2651, lr_0 = 3.9949e-04
Loss = 3.1838e-04, PNorm = 38.2868, GNorm = 1.1683, lr_0 = 3.9604e-04
Loss = 4.1039e-04, PNorm = 38.2927, GNorm = 0.6599, lr_0 = 3.9263e-04
Loss = 5.1885e-04, PNorm = 38.3038, GNorm = 0.7004, lr_0 = 3.8925e-04
Loss = 4.4992e-04, PNorm = 38.3125, GNorm = 0.4443, lr_0 = 3.8589e-04
Loss = 3.2373e-04, PNorm = 38.3196, GNorm = 0.2599, lr_0 = 3.8257e-04
Loss = 3.6745e-04, PNorm = 38.3286, GNorm = 1.0803, lr_0 = 3.7927e-04
Loss = 5.5231e-04, PNorm = 38.3340, GNorm = 0.5853, lr_0 = 3.7600e-04
Loss = 4.0148e-04, PNorm = 38.3385, GNorm = 0.6400, lr_0 = 3.7276e-04
Validation rmse = 0.490732
Epoch 14
Loss = 3.2922e-04, PNorm = 38.3453, GNorm = 0.8377, lr_0 = 3.6955e-04
Loss = 4.6288e-04, PNorm = 38.3535, GNorm = 0.4704, lr_0 = 3.6636e-04
Loss = 3.3216e-04, PNorm = 38.3566, GNorm = 0.8589, lr_0 = 3.6320e-04
Loss = 3.3322e-04, PNorm = 38.3592, GNorm = 0.4479, lr_0 = 3.6007e-04
Loss = 3.1137e-04, PNorm = 38.3641, GNorm = 0.6151, lr_0 = 3.5697e-04
Loss = 2.4201e-04, PNorm = 38.3687, GNorm = 0.3838, lr_0 = 3.5389e-04
Loss = 5.7953e-04, PNorm = 38.3749, GNorm = 0.5720, lr_0 = 3.5084e-04
Loss = 4.2208e-04, PNorm = 38.3828, GNorm = 0.9359, lr_0 = 3.4782e-04
Loss = 3.9031e-04, PNorm = 38.3917, GNorm = 0.6081, lr_0 = 3.4482e-04
Validation rmse = 0.487120
Epoch 15
Loss = 7.3480e-04, PNorm = 38.3985, GNorm = 1.8104, lr_0 = 3.4185e-04
Loss = 2.7123e-04, PNorm = 38.4031, GNorm = 0.3832, lr_0 = 3.3890e-04
Loss = 2.8666e-04, PNorm = 38.4102, GNorm = 0.2394, lr_0 = 3.3598e-04
Loss = 5.1175e-04, PNorm = 38.4171, GNorm = 0.4895, lr_0 = 3.3309e-04
Loss = 3.3678e-04, PNorm = 38.4211, GNorm = 0.3936, lr_0 = 3.3022e-04
Loss = 3.7810e-04, PNorm = 38.4295, GNorm = 1.7129, lr_0 = 3.2737e-04
Loss = 3.7888e-04, PNorm = 38.4378, GNorm = 1.0737, lr_0 = 3.2455e-04
Loss = 4.6374e-04, PNorm = 38.4478, GNorm = 0.9357, lr_0 = 3.2175e-04
Loss = 3.2610e-04, PNorm = 38.4531, GNorm = 0.6612, lr_0 = 3.1898e-04
Loss = 3.4550e-04, PNorm = 38.4581, GNorm = 0.3544, lr_0 = 3.1623e-04
Validation rmse = 0.520426
Epoch 16
Loss = 3.5464e-04, PNorm = 38.4649, GNorm = 1.1194, lr_0 = 3.1350e-04
Loss = 2.5475e-04, PNorm = 38.4698, GNorm = 0.3758, lr_0 = 3.1080e-04
Loss = 3.2451e-04, PNorm = 38.4752, GNorm = 0.6231, lr_0 = 3.0812e-04
Loss = 5.4280e-04, PNorm = 38.4800, GNorm = 0.5607, lr_0 = 3.0547e-04
Loss = 3.9189e-04, PNorm = 38.4832, GNorm = 0.6209, lr_0 = 3.0283e-04
Loss = 3.1372e-04, PNorm = 38.4888, GNorm = 0.4221, lr_0 = 3.0022e-04
Loss = 6.1705e-04, PNorm = 38.4952, GNorm = 0.8138, lr_0 = 2.9764e-04
Loss = 3.1450e-04, PNorm = 38.5003, GNorm = 0.7141, lr_0 = 2.9507e-04
Loss = 2.8529e-04, PNorm = 38.5057, GNorm = 0.2069, lr_0 = 2.9253e-04
Validation rmse = 0.488015
Epoch 17
Loss = 3.8475e-04, PNorm = 38.5109, GNorm = 0.5035, lr_0 = 2.9001e-04
Loss = 3.1431e-04, PNorm = 38.5162, GNorm = 0.6526, lr_0 = 2.8751e-04
Loss = 2.3739e-04, PNorm = 38.5217, GNorm = 0.4695, lr_0 = 2.8503e-04
Loss = 3.3614e-04, PNorm = 38.5283, GNorm = 0.2993, lr_0 = 2.8257e-04
Loss = 3.7900e-04, PNorm = 38.5336, GNorm = 1.6574, lr_0 = 2.8014e-04
Loss = 2.9866e-04, PNorm = 38.5373, GNorm = 0.8088, lr_0 = 2.7772e-04
Loss = 3.6990e-04, PNorm = 38.5432, GNorm = 0.4841, lr_0 = 2.7533e-04
Loss = 2.8166e-04, PNorm = 38.5478, GNorm = 0.2772, lr_0 = 2.7295e-04
Loss = 4.3613e-04, PNorm = 38.5512, GNorm = 0.4882, lr_0 = 2.7060e-04
Loss = 2.4079e-04, PNorm = 38.5550, GNorm = 0.1706, lr_0 = 2.6827e-04
Validation rmse = 0.466481
Epoch 18
Loss = 2.2902e-04, PNorm = 38.5587, GNorm = 0.2198, lr_0 = 2.6596e-04
Loss = 2.4919e-04, PNorm = 38.5616, GNorm = 0.8447, lr_0 = 2.6367e-04
Loss = 3.2572e-04, PNorm = 38.5663, GNorm = 0.9803, lr_0 = 2.6139e-04
Loss = 2.2789e-04, PNorm = 38.5729, GNorm = 0.4207, lr_0 = 2.5914e-04
Loss = 2.0853e-04, PNorm = 38.5773, GNorm = 0.2656, lr_0 = 2.5691e-04
Loss = 2.6405e-04, PNorm = 38.5807, GNorm = 0.2674, lr_0 = 2.5469e-04
Loss = 4.3833e-04, PNorm = 38.5843, GNorm = 0.4814, lr_0 = 2.5250e-04
Loss = 4.0645e-04, PNorm = 38.5910, GNorm = 0.2875, lr_0 = 2.5032e-04
Loss = 4.3485e-04, PNorm = 38.5973, GNorm = 0.5326, lr_0 = 2.4816e-04
Validation rmse = 0.490979
Epoch 19
Loss = 3.0181e-04, PNorm = 38.6017, GNorm = 0.2674, lr_0 = 2.4602e-04
Loss = 2.7528e-04, PNorm = 38.6046, GNorm = 0.6303, lr_0 = 2.4390e-04
Loss = 4.4371e-04, PNorm = 38.6088, GNorm = 0.4940, lr_0 = 2.4180e-04
Loss = 3.0792e-04, PNorm = 38.6144, GNorm = 1.1167, lr_0 = 2.3972e-04
Loss = 2.5918e-04, PNorm = 38.6193, GNorm = 0.5139, lr_0 = 2.3765e-04
Loss = 2.2875e-04, PNorm = 38.6219, GNorm = 0.3388, lr_0 = 2.3560e-04
Loss = 3.8114e-04, PNorm = 38.6254, GNorm = 0.6217, lr_0 = 2.3357e-04
Loss = 2.0255e-04, PNorm = 38.6300, GNorm = 0.7976, lr_0 = 2.3156e-04
Loss = 2.6732e-04, PNorm = 38.6342, GNorm = 0.4411, lr_0 = 2.2956e-04
Loss = 3.7358e-04, PNorm = 38.6384, GNorm = 0.5332, lr_0 = 2.2758e-04
Validation rmse = 0.463390
Epoch 20
Loss = 4.6643e-04, PNorm = 38.6420, GNorm = 1.2255, lr_0 = 2.2562e-04
Loss = 3.8995e-04, PNorm = 38.6469, GNorm = 0.3369, lr_0 = 2.2368e-04
Loss = 2.8501e-04, PNorm = 38.6512, GNorm = 0.3391, lr_0 = 2.2175e-04
Loss = 3.1822e-04, PNorm = 38.6560, GNorm = 0.5362, lr_0 = 2.1984e-04
Loss = 3.0485e-04, PNorm = 38.6622, GNorm = 0.4679, lr_0 = 2.1794e-04
Loss = 2.3443e-04, PNorm = 38.6653, GNorm = 0.4127, lr_0 = 2.1607e-04
Loss = 2.5665e-04, PNorm = 38.6689, GNorm = 0.3709, lr_0 = 2.1420e-04
Loss = 2.6714e-04, PNorm = 38.6729, GNorm = 0.2395, lr_0 = 2.1236e-04
Loss = 1.9560e-04, PNorm = 38.6751, GNorm = 0.2605, lr_0 = 2.1053e-04
Validation rmse = 0.461835
Epoch 21
Loss = 1.4798e-04, PNorm = 38.6767, GNorm = 0.1983, lr_0 = 2.0871e-04
Loss = 3.7057e-04, PNorm = 38.6805, GNorm = 0.7619, lr_0 = 2.0691e-04
Loss = 2.9119e-04, PNorm = 38.6824, GNorm = 0.9162, lr_0 = 2.0513e-04
Loss = 2.2305e-04, PNorm = 38.6862, GNorm = 0.5308, lr_0 = 2.0336e-04
Loss = 2.4996e-04, PNorm = 38.6916, GNorm = 0.2843, lr_0 = 2.0161e-04
Loss = 3.7259e-04, PNorm = 38.6939, GNorm = 0.7101, lr_0 = 1.9987e-04
Loss = 2.6939e-04, PNorm = 38.6952, GNorm = 0.5252, lr_0 = 1.9815e-04
Loss = 2.7926e-04, PNorm = 38.6991, GNorm = 0.3511, lr_0 = 1.9644e-04
Loss = 2.3603e-04, PNorm = 38.7042, GNorm = 0.4921, lr_0 = 1.9475e-04
Loss = 1.9938e-04, PNorm = 38.7091, GNorm = 0.2682, lr_0 = 1.9307e-04
Validation rmse = 0.467445
Epoch 22
Loss = 2.2781e-04, PNorm = 38.7130, GNorm = 0.3821, lr_0 = 1.9141e-04
Loss = 1.8212e-04, PNorm = 38.7160, GNorm = 0.3232, lr_0 = 1.8976e-04
Loss = 1.8905e-04, PNorm = 38.7195, GNorm = 0.2081, lr_0 = 1.8812e-04
Loss = 1.8132e-04, PNorm = 38.7233, GNorm = 0.2874, lr_0 = 1.8650e-04
Loss = 1.7593e-04, PNorm = 38.7261, GNorm = 0.4164, lr_0 = 1.8489e-04
Loss = 2.4241e-04, PNorm = 38.7276, GNorm = 0.3600, lr_0 = 1.8330e-04
Loss = 1.9221e-04, PNorm = 38.7302, GNorm = 0.4375, lr_0 = 1.8172e-04
Loss = 3.8574e-04, PNorm = 38.7360, GNorm = 0.3886, lr_0 = 1.8015e-04
Loss = 2.3803e-04, PNorm = 38.7403, GNorm = 0.6309, lr_0 = 1.7860e-04
Validation rmse = 0.456166
Epoch 23
Loss = 2.2752e-04, PNorm = 38.7431, GNorm = 0.5302, lr_0 = 1.7706e-04
Loss = 3.1937e-04, PNorm = 38.7437, GNorm = 0.1700, lr_0 = 1.7553e-04
Loss = 2.0848e-04, PNorm = 38.7479, GNorm = 0.2996, lr_0 = 1.7402e-04
Loss = 3.9687e-04, PNorm = 38.7516, GNorm = 0.4597, lr_0 = 1.7252e-04
Loss = 2.8576e-04, PNorm = 38.7539, GNorm = 1.1180, lr_0 = 1.7103e-04
Loss = 2.0113e-04, PNorm = 38.7577, GNorm = 0.4286, lr_0 = 1.6956e-04
Loss = 4.3448e-04, PNorm = 38.7586, GNorm = 0.2780, lr_0 = 1.6810e-04
Loss = 2.1154e-04, PNorm = 38.7606, GNorm = 0.3299, lr_0 = 1.6665e-04
Loss = 1.6660e-04, PNorm = 38.7630, GNorm = 0.3375, lr_0 = 1.6521e-04
Loss = 1.9799e-04, PNorm = 38.7649, GNorm = 0.3404, lr_0 = 1.6379e-04
Validation rmse = 0.453181
Epoch 24
Loss = 1.9554e-04, PNorm = 38.7680, GNorm = 0.3444, lr_0 = 1.6238e-04
Loss = 2.1457e-04, PNorm = 38.7707, GNorm = 0.2171, lr_0 = 1.6098e-04
Loss = 1.9862e-04, PNorm = 38.7741, GNorm = 0.5293, lr_0 = 1.5959e-04
Loss = 2.3240e-04, PNorm = 38.7762, GNorm = 0.3944, lr_0 = 1.5822e-04
Loss = 3.5672e-04, PNorm = 38.7785, GNorm = 0.4050, lr_0 = 1.5685e-04
Loss = 1.9097e-04, PNorm = 38.7800, GNorm = 0.3881, lr_0 = 1.5550e-04
Loss = 1.5225e-04, PNorm = 38.7834, GNorm = 0.1648, lr_0 = 1.5416e-04
Loss = 1.7129e-04, PNorm = 38.7865, GNorm = 0.2775, lr_0 = 1.5283e-04
Loss = 1.9456e-04, PNorm = 38.7896, GNorm = 0.2988, lr_0 = 1.5151e-04
Validation rmse = 0.478809
Epoch 25
Loss = 4.1500e-04, PNorm = 38.7923, GNorm = 2.0908, lr_0 = 1.5021e-04
Loss = 3.7975e-04, PNorm = 38.7936, GNorm = 0.7872, lr_0 = 1.4891e-04
Loss = 2.4537e-04, PNorm = 38.7960, GNorm = 0.5574, lr_0 = 1.4763e-04
Loss = 1.6783e-04, PNorm = 38.7996, GNorm = 0.1894, lr_0 = 1.4636e-04
Loss = 3.6095e-04, PNorm = 38.8025, GNorm = 0.6661, lr_0 = 1.4510e-04
Loss = 2.3190e-04, PNorm = 38.8050, GNorm = 0.5056, lr_0 = 1.4384e-04
Loss = 2.5952e-04, PNorm = 38.8082, GNorm = 0.6855, lr_0 = 1.4261e-04
Loss = 1.8805e-04, PNorm = 38.8121, GNorm = 0.5139, lr_0 = 1.4138e-04
Loss = 2.0421e-04, PNorm = 38.8131, GNorm = 0.4058, lr_0 = 1.4016e-04
Loss = 1.6231e-04, PNorm = 38.8148, GNorm = 0.2899, lr_0 = 1.3895e-04
Validation rmse = 0.475922
Epoch 26
Loss = 1.4990e-04, PNorm = 38.8171, GNorm = 0.2680, lr_0 = 1.3775e-04
Loss = 2.2487e-04, PNorm = 38.8199, GNorm = 0.2497, lr_0 = 1.3656e-04
Loss = 1.7331e-04, PNorm = 38.8236, GNorm = 0.4110, lr_0 = 1.3539e-04
Loss = 1.7784e-04, PNorm = 38.8271, GNorm = 0.7663, lr_0 = 1.3422e-04
Loss = 6.1516e-04, PNorm = 38.8293, GNorm = 0.4684, lr_0 = 1.3306e-04
Loss = 2.4052e-04, PNorm = 38.8293, GNorm = 0.3945, lr_0 = 1.3192e-04
Loss = 1.6057e-04, PNorm = 38.8311, GNorm = 0.4209, lr_0 = 1.3078e-04
Loss = 1.5063e-04, PNorm = 38.8338, GNorm = 0.3379, lr_0 = 1.2965e-04
Loss = 1.6916e-04, PNorm = 38.8356, GNorm = 0.3375, lr_0 = 1.2854e-04
Validation rmse = 0.462207
Epoch 27
Loss = 2.0357e-04, PNorm = 38.8385, GNorm = 0.4751, lr_0 = 1.2743e-04
Loss = 3.8419e-04, PNorm = 38.8412, GNorm = 1.7313, lr_0 = 1.2633e-04
Loss = 1.9717e-04, PNorm = 38.8431, GNorm = 0.3760, lr_0 = 1.2524e-04
Loss = 1.8997e-04, PNorm = 38.8452, GNorm = 0.3127, lr_0 = 1.2416e-04
Loss = 2.1344e-04, PNorm = 38.8477, GNorm = 0.1680, lr_0 = 1.2309e-04
Loss = 2.0214e-04, PNorm = 38.8498, GNorm = 0.5162, lr_0 = 1.2203e-04
Loss = 1.5687e-04, PNorm = 38.8513, GNorm = 0.4385, lr_0 = 1.2098e-04
Loss = 1.6596e-04, PNorm = 38.8528, GNorm = 0.2925, lr_0 = 1.1994e-04
Loss = 2.2923e-04, PNorm = 38.8554, GNorm = 0.1377, lr_0 = 1.1890e-04
Loss = 1.9167e-04, PNorm = 38.8588, GNorm = 0.7348, lr_0 = 1.1788e-04
Validation rmse = 0.469076
Epoch 28
Loss = 3.2391e-04, PNorm = 38.8616, GNorm = 0.1660, lr_0 = 1.1686e-04
Loss = 1.6628e-04, PNorm = 38.8630, GNorm = 0.3426, lr_0 = 1.1585e-04
Loss = 1.5823e-04, PNorm = 38.8648, GNorm = 0.1578, lr_0 = 1.1486e-04
Loss = 1.3604e-04, PNorm = 38.8664, GNorm = 0.2007, lr_0 = 1.1387e-04
Loss = 1.1888e-04, PNorm = 38.8681, GNorm = 0.2235, lr_0 = 1.1288e-04
Loss = 1.5926e-04, PNorm = 38.8699, GNorm = 0.3584, lr_0 = 1.1191e-04
Loss = 4.0392e-04, PNorm = 38.8722, GNorm = 1.7290, lr_0 = 1.1095e-04
Loss = 1.7034e-04, PNorm = 38.8746, GNorm = 0.6045, lr_0 = 1.0999e-04
Loss = 2.3208e-04, PNorm = 38.8767, GNorm = 0.9643, lr_0 = 1.0904e-04
Validation rmse = 0.464252
Epoch 29
Loss = 1.8089e-04, PNorm = 38.8794, GNorm = 0.2909, lr_0 = 1.0810e-04
Loss = 1.4076e-04, PNorm = 38.8806, GNorm = 0.2657, lr_0 = 1.0717e-04
Loss = 1.9373e-04, PNorm = 38.8830, GNorm = 0.2895, lr_0 = 1.0625e-04
Loss = 1.7892e-04, PNorm = 38.8851, GNorm = 0.2947, lr_0 = 1.0533e-04
Loss = 2.6964e-04, PNorm = 38.8865, GNorm = 0.1773, lr_0 = 1.0442e-04
Loss = 1.6046e-04, PNorm = 38.8878, GNorm = 0.4184, lr_0 = 1.0352e-04
Loss = 1.7502e-04, PNorm = 38.8895, GNorm = 0.8886, lr_0 = 1.0263e-04
Loss = 1.2998e-04, PNorm = 38.8914, GNorm = 0.2094, lr_0 = 1.0175e-04
Loss = 3.3624e-04, PNorm = 38.8939, GNorm = 0.4970, lr_0 = 1.0087e-04
Loss = 1.9273e-04, PNorm = 38.8944, GNorm = 0.3232, lr_0 = 1.0000e-04
Validation rmse = 0.462599
Model 0 best validation rmse = 0.453181 on epoch 23
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "encoder.attention.cached_zero_vector".
Loading pretrained parameter "encoder.attention.att_1.weight".
Loading pretrained parameter "encoder.attention.att_1.bias".
Loading pretrained parameter "encoder.attention.att_2.weight".
Loading pretrained parameter "encoder.attention.att_2.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Model 0 test rmse = 0.590256
Ensemble test rmse = 0.590256
1-fold cross validation
Seed 0 ==> test rmse = 0.590256
Overall test rmse = 0.590256 +/- 0.000000
